{"meta":{"title":"黑暗森林","subtitle":"勇气是唯一的武器，坚韧是唯一的铠甲","description":"思辨","author":"BlackForest1990","url":"https://blackforest1990.github.io","root":"/"},"pages":[{"title":"tags","date":"2023-03-14T08:49:03.000Z","updated":"2023-03-14T08:59:35.881Z","comments":true,"path":"tags/index.html","permalink":"https://blackforest1990.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-03-14T09:15:12.000Z","updated":"2023-03-14T09:15:42.088Z","comments":true,"path":"categories/index.html","permalink":"https://blackforest1990.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"历史中的魅力发言","slug":"历史中的魅力发言","date":"2024-07-18T02:02:39.000Z","updated":"2024-07-18T02:40:24.413Z","comments":true,"path":"2024/07/18/历史中的魅力发言/","link":"","permalink":"https://blackforest1990.github.io/2024/07/18/%E5%8E%86%E5%8F%B2%E4%B8%AD%E7%9A%84%E9%AD%85%E5%8A%9B%E5%8F%91%E8%A8%80/","excerpt":"","text":"秦 国家军国主义，丈量全国土地，登记造册全国资源，对于人民压榨太过，没有很好地消化六国残余势力 项羽 军神级别，勇略为第一将，而无战略，以自己的喜怒而支配政治（烧咸阳宫，杀秦降将，失关中之心） 刘邦 49岁斩白蛇，取关中，军霸上，完成蜕变（成大事要有第一等的克制），用合适的人，合理的分糕，对于制度的探索要摸着石头过河，黑猫白猫抓住老鼠就是好猫。 “吾以布衣提三尺剑取天下，此非天命乎？命乃在天，虽扁鹊何益！” 韩信 长于军事战略，能够快速判断局势，趁关中立足未稳而定关中，10个月平魏，定赵，灭齐，降燕，充分收集情报，实实虚虚，以比之长攻彼之短，但是在身居高位的时候没有三分天下，终究是国士无双，而没有意识到政治斗争的残酷性。 吕后 用黄老之道治国，维持小政府，在没有大动乱（战争和大灾），能够很好地与民休息，不与军功集团争相权，保持政治平衡，对于权力克制不够，对于政敌迫害过大，导致后面反弹太大。 汉文帝刘恒 宽厚而福报深厚，在政治和权力面前永远是谨慎的，推恩令逐渐蚕食诸侯的权力，继续用黄老之道治国。 武帝 为千古一帝，权术的顶峰，文化上独尊儒术，儒学也彻底倒向统治者，创造了很多压榨人民的手段，官方统一货币，国有企业出现（盐，铁），人民被无止境的盘剥，允许买官，通过压榨天下，拥有了庞大的经费，发出了犯强汉者虽远必诛的强音，然后在晚年通过权术，让太子自杀，发布罪己诏，让政策软着陆，定下了昭宣的与民休息政策，选择了接班人霍光，让自己的政策能够执行。 宣帝 民间长大，了解民间疾苦，不搞政治清算，对于霍光的政策能够延续，“汉家自有制度，本以霸王道杂之。奈何纯任德教，用周政乎！”是对于汉朝制度最好的注解，然而儒家没有被关进笼子里，太学制度让儒家成为了大祸。 王莽 乱天下者，儒家理想主义者，糟糕的经济政策和豪强政策，加上糟糕的天气，一世而终。 光武 安定天下，豪族投票出的代理人，重新确立儒学的地位，以夷制夷的优秀策略。 曹操 浪漫的诗人和强大的军事家，优秀的政治家，年轻时候想成为大汉的一名优秀官吏，后来讨伐董卓后认清现实，涿郡老家的武人集团起兵，后来得到颍川荀彧的文人集团加盟，在于袁绍的官渡之战中，最后亲自带领骑兵攻打乌巢。 平定北方，统一了全国80%人口的地方，但是屠戮徐州，迁徙百姓，对于曾经得王佐之才，最后诛杀了荀彧。 李世民 7世纪地表最强，少年英雄，布局深远，军略上能够洞悉战场的局势，以最小的成本取得最大的战果，政治上以自我的克制造成了清明的政治环境，对于北疆的策略非常高明，作为天可汗，能够集合草原部落去对付不听话的野心家，种种布局，以最小的代价确定了北疆的和平，唯一比较诟病的是对于继承人的策略。 半瓶子水时最晃荡，一瓶子水时都没声音，之所以“惟大英雄能本色”，是因为像炫耀、忐忑、不自信等等的关卡他都已经过去了，他该啥样就啥样，他从不需要来证明自己什么。 1、永远亲临现场调研，具体问题具体分析。 2、永远注意士气，永远跟同志们讲明白革命因何成功的理论依据。 3、永远直面困难，永远解决问题，永远亲自带队攻坚最难的任务。 4、战争是政治的延续，打仗永远算账，不是单纯地为了打仗而打仗，永远全局一盘棋。 武则天 唐朝衰弱由武氏开始，任用酷吏，破坏政治清明，好大喜功，军事上昏招频出，从而使府兵制衰弱下去，为藩镇割据打好基础。","categories":[{"name":"历史","slug":"历史","permalink":"https://blackforest1990.github.io/categories/%E5%8E%86%E5%8F%B2/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://blackforest1990.github.io/tags/%E6%9D%82%E8%B0%88/"}]},{"title":"汇编语言","slug":"汇编语言","date":"2024-04-29T06:43:46.000Z","updated":"2024-07-24T03:37:55.628Z","comments":true,"path":"2024/04/29/汇编语言/","link":"","permalink":"https://blackforest1990.github.io/2024/04/29/%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/","excerpt":"","text":"汇编语言 汇编语言是基于机器语言的一种抽象，具体来说，计算机艺术，是从门电路-&gt;CMOS电路-&gt;assemble-&gt;C的层层抽象。 本文具体阐述的是关于汇编的抽象，不讨论具体的电路设计。 汇编语言发展至今，有以下3类指令组成。 汇编指令：机器码的助记符，有对应的机器码。 伪指令：没有对应的机器码，由编译器执行，计算机并不执行。 其他符号：如+、-、*、/等，由编译器识别，没有对应的机器码。 寄存器 简单的来说，在CPU中 运算器进行信息处理 寄存器进行信息存储 控制器控制各种器件进行工作 内部总线连接各种器件，在他们之间进行数据的传送。 通用寄存器 8086CPU的所有寄存器都是16位的，可以存放两个字节。AX、BX、CX、DX这4个寄存器存放一般性数据，被称为通用寄存器。 8086的上一代CPU中的寄存器都是8位的，为了兼容性，这4个寄存器都可以分为两个可独立使用的8位寄存器来用。 AX可分为AH和AL BX可分为BH和BL CX可分为CH和CL DX可分为DH和DL 字在寄存器中的存储 字节：记为byte,一个字节由8个bit组成，可以存在8位寄存器中。 字：记为word,一个字由两个字节组成，这两个字节分别称为这个字的高位字节和低位字节 物理地址 我们知道，CPU访问内存单元时，要给出内存单元的地址。所有的内存单元构成的存储空间是一个一维的线性空间，每一个内存单元在这个空间中都有唯一的地址，我们将这个唯一的地址称为物理地址。CPU通过地址总线送入存储器的，必须是一个内存单元的物理地址。在CPU向地址总线上发出物理地址之前，必须要在内部先形成这个物理地址。不同的CPU可以有不同的形成物理地址的方式。我们现在讨论8086CPU是如何在内部形成内存单元的物理地址的。 8086是16位的CPU： ● 运算器一次最多可以处理16位的数据； ● 寄存器的最大宽度为16位； ● 寄存器和运算器之间的通路为16位。 在8086内部，能够一次性处理、传输、暂时存储的信息的最大长度是16位的。内存单元的地址在送上地址总线之前，必须在CPU中处理、传输、暂时存放，对于16位CPU,能一次性处理、传输、暂时存储16位的地址。 8086CPU有20位地址总线，可以传送20位地址，达到1MB寻址能力。8086CPU又是16位结构，在内部一次性处理、传输、暂时存储的地址为16位。从8086CPU的内部结构来看，如果将地址从内部简单地发出，那么它只能送出16位的地址，表现出的寻址能力只有64KB。 8086CPU采用一种在内部用两个16位地址合成的方法形成一个20位的物理地址。 CPU中的相关部件提供两个16位的地址，一个称为段地址，另一个称为偏移地址。 段地址和偏移地址通过内部总线送入一个称为地址加法器的部件。 地址加法器将两个16位的地址合成位一个20位的物理地址，通过内部总线送入输入输出控制电路，输入输出控制电路将20位物理地址送上地址总线。 20位地址总线被传送到存储器。 物理地址 = 段地址 * 16 + 偏移地址 段寄存器 段地址在段寄存器中存放。段寄存器：CS、DS、SS、ES。 CS和IP是8086CPU中两个最关键的寄存器，它们指示了CPU当前要读取指令的地址。CS为代码段寄存器，IP为指令指针寄存器。 CPU将CS：IP指向的内存单元中的内容当成指令。 (1)从CS:IP指向的内存单元读取指令，读取的指令进入指令缓冲器； (2)IP=IP+所读取指令的长度，从而指向下一条指令； (3)执行指令。转到步骤(1),重复这个过程。 在8086CPU加电启动或复位后(即CPU刚开始工作时)CS和IP被设置为CS=FFFFH,IP=0000H,即在8086PC机刚启动时，CPU从内存FFFFOH单元中读取指令执行，FFFFOH单元中的指令是8086PC机开机后执行的第一条指令。 若想同时修改CS、IP的内容，可用形如“jmp段地址：偏移地址”的指令完成 “jmp段地址：偏移地址”指令的功能为：用指令中给出的段地址修改CS,偏移地址修改","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"汇编语言","slug":"汇编语言","permalink":"https://blackforest1990.github.io/tags/%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/"}]},{"title":"Initialization","slug":"Initialization","date":"2024-04-19T01:43:12.000Z","updated":"2024-04-19T08:22:15.363Z","comments":true,"path":"2024/04/19/Initialization/","link":"","permalink":"https://blackforest1990.github.io/2024/04/19/Initialization/","excerpt":"","text":"内核初始化流程 了解到整个内核初始化的完整周期，从内核解压之后的第一步到内核自身运行的第一个进程 内核解压之后的首要步骤 - 描述内核中的首要步骤。 早期的中断和异常控制 - 描述了早期的中断初始化和早期的缺页处理函数。 在到达内核入口之前最后的准备 - 描述了在调用 start_kernel 之前最后的准备工作。 内核入口 - start_kernel - 描述了内核通用代码中初始化的第一步。 体系架构初始化 - 描述了特定架构的初始化。 进一步初始化指定体系架构 - 描述了再一次的指定架构初始化流程。 最后对指定体系架构初始化 - 描述了指定架构初始化流程的结尾。 调度器初始化 - 描述了调度初始化之前的准备工作，以及调度初始化。 RCU 初始化 - 描述了 RCU 的初始化。 初始化结束 - Linux内核初始化的最后部分。 内核解压之后的首要步骤 在在Linux Booting的中，我们跟踪到了 arch/x86/boot/compressed/head_64.S文件中的 jmp 指令： 1jmp *%rax 此时 rax 寄存器中保存的就是 Linux 内核入口点，通过调用 decompress_kernel （arch/x86/boot/compressed/misc.c） 函数后获得。由此可见，内核引导程序的最后一行代码是一句指向内核入口点的跳转指令。既然已经知道了内核入口点定义在哪，我们就可以继续探究 Linux 内核在引导结束后做了些什么。 内核执行的第一步 OK，在调用了 decompress_kernel 函数后，rax 寄存器中保存了解压缩后的内核镜像的地址，并且跳转了过去。解压缩后的内核镜像的入口点定义在 arch/x86/kernel/head_64.S，这个文件的开头几行如下： 1234567 __HEAD .code64 .globl startup_64startup_64: ... ... ... 我们可以看到 startup_64 过程定义在了 __HEAD 区段下。 __HEAD 只是一个宏，它将展开为可执行的 .head.text 区段： 1#define __HEAD .section &quot;.head.text&quot;,&quot;ax&quot; 我们可以在 arch/x86/kernel/vmlinux.lds.S 链接器脚本文件中看到这个区段的定义： 123456.text : AT(ADDR(.text) - LOAD_OFFSET) &#123; _text = .; ... ... ...&#125; :text = 0x9090 除了对 .text 区段的定义，我们还能从这个脚本文件中得知内核的默认物理地址与虚拟地址。_text 是一个地址计数器，对于 x86_64来说，它定义为： 1. = __START_KERNEL; __START_KERNEL 宏的定义在 arch/x86/include/asm/page_types.h头文件中，它由内核映射的虚拟基址与基物理起始点相加得到： 123#define _START_KERNEL (__START_KERNEL_map + __PHYSICAL_START)#define __PHYSICAL_START ALIGN(CONFIG_PHYSICAL_START, CONFIG_PHYSICAL_ALIGN) 换句话说： Linux 内核的物理基址 - 0x1000000; Linux 内核的虚拟基址 - 0xffffffff81000000. 现在我们知道了 startup_64 过程的默认物理地址与虚拟地址，但是真正的地址必须要通过下面的代码计算得到： 12 leaq _text(%rip), %rbpsubq $_text - __START_KERNEL_map, %rbp 没错，虽然定义为 0x1000000，但是仍然有可能变化，例如启用 kASLR 的时候。所以我们当前的目标是计算 0x1000000 与实际加载地址的差。这里我们首先将RIP相对地址（rip-relative）放入 rbp 寄存器，并且从中减去 $_text - __START_KERNEL_map 。我们已经知道， _text 在编译后的默认虚拟地址为 0xffffffff81000000， 物理地址为 0x1000000。__START_KERNEL_map 宏将展开为 0xffffffff80000000，因此对于对于第二行汇编代码，我们将得到如下的表达式： 1rbp = 0x1000000 - (0xffffffff81000000 - 0xffffffff80000000) 在计算过后，rbp 的值将为 0，代表了实际加载地址与编译后的默认地址之间的差值。在我们这个例子中，0 代表了 Linux 内核被加载到了默认地址，并且没有启用 kASLR 。 在得到了 startup_64 的地址后，我们需要检查这个地址是否已经正确对齐。下面的代码将进行这项工作： 12 testl $~PMD_PAGE_MASK, %ebpjnz bad_address 在这里我们将 rbp 寄存器的低32位与 PMD_PAGE_MASK 进行比较。PMD_PAGE_MASK 代表中层页目录（Page middle directory）屏蔽位，它的定义如下： 1234#define PMD_PAGE_MASK (~(PMD_PAGE_SIZE-1))#define PMD_PAGE_SIZE (_AC(1, UL) &lt;&lt; PMD_SHIFT)#define PMD_SHIFT 21 可以很容易得出 PMD_PAGE_SIZE 为 2MB 。在这里我们使用标准公式来检查对齐问题，如果 text 的地址没有对齐到 2MB，则跳转到 bad_address。 在此之后，我们通过检查高 18 位来防止这个地址过大： 123 leaq _text(%rip), %raxshrq $MAX_PHYSMEM_BITS, %raxjnz bad_address 这个地址必须不超过 46 个比特，即小于2的46次方： 1#define MAX_PHYSMEM_BITS 46 OK，至此我们完成了一些初步的检查，可以继续进行后续的工作了。 修正页表基地址 在开始设置 Identity 分页之前，我们需要首先修正下面的地址： 1234addq %rbp, early_level4_pgt + (L4_START_KERNEL*8)(%rip)addq %rbp, level3_kernel_pgt + (510*8)(%rip)addq %rbp, level3_kernel_pgt + (511*8)(%rip)addq %rbp, level2_fixmap_pgt + (506*8)(%rip) 如果 startup_64 的值不为默认的 0x1000000 的话， 则包括 early_level4_pgt、level3_kernel_pgt 在内的很多地址都会不正确。rbp寄存器中包含的是相对地址，因此我们把它与 early_level4_pgt、level3_kernel_pgt 以及 level2_fixmap_pgt 中特定的项相加。首先我们来看一下它们的定义： 1234567891011121314151617181920NEXT_PAGE(early_level4_pgt) .fill 511,8,0 .quad level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLENEXT_PAGE(level3_kernel_pgt) .fill L3_START_KERNEL,8,0 .quad level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE .quad level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLENEXT_PAGE(level2_kernel_pgt) PMDS(0, __PAGE_KERNEL_LARGE_EXEC, KERNEL_IMAGE_SIZE/PMD_SIZE)NEXT_PAGE(level2_fixmap_pgt) .fill 506,8,0 .quad level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE .fill 5,8,0NEXT_PAGE(level1_fixmap_pgt) .fill 512,8,0 起来很难理解，实则不然。首先我们来看一下 early_level4_pgt。它的前 (4096 - 8) 个字节全为 0，即它的前 511 个项均不使用，之后的一项是 level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE。我们知道 __START_KERNEL_map 是内核的虚拟基地址，因此减去 __START_KERNEL_map 后就得到了 level3_kernel_pgt 的物理地址。现在我们来看一下 _PAGE_TABLE，它是页表项的访问权限： 12#define _PAGE_TABLE (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER | \\ _PAGE_ACCESSED | _PAGE_DIRTY) level3_kernel_pgt 中保存的两项用来映射内核空间，在它的前 510（即 L3_START_KERNEL）项均为 0。这里的 L3_START_KERNEL 保存的是在上层页目录（Page Upper Directory）中包含__START_KERNEL_map 地址的那一条索引，它等于 510。后面一项 level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE 中的 level2_kernel_pgt 比较容易理解，它是一条页表项，包含了指向中层页目录的指针，它用来映射内核空间，并且具有如下的访问权限： 12#define _KERNPG_TABLE (_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | \\ _PAGE_DIRTY) level2_fixmap_pgt 是一系列虚拟地址，它们可以在内核空间中指向任意的物理地址。它们由level2_fixmap_pgt作为入口点、10MB 大小的空间用来为 vsyscalls 做映射。level2_kernel_pgt 则调用了PDMS 宏，在 __START_KERNEL_map 地址处为内核的 .text 创建了 512MB 大小的空间（这 512 MB空间的后面是模块内存空间）。 现在，在看过了这些符号的定义之后，让我们回到开始时介绍的那几行代码。rbp 寄存器包含了实际地址与 startup_64 地址之差，其中 startup_64 的地址是在内核链接时获得的。因此我们只需要把它与各个页表项的基地址相加，就能够得到正确的地址了。在这里这些操作如下： 1234addq %rbp, early_level4_pgt + (L4_START_KERNEL*8)(%rip)addq %rbp, level3_kernel_pgt + (510*8)(%rip)addq %rbp, level3_kernel_pgt + (511*8)(%rip)addq %rbp, level2_fixmap_pgt + (506*8)(%rip) 换句话说，early_level4_pgt 的最后一项就是 level3_kernel_pgt，level3_kernel_pgt 的最后两项分别是 level2_kernel_pgt 和 level2_fixmap_pgt， level2_fixmap_pgt 的第507项就是 level1_fixmap_pgt 页目录。 在这之后我们就得到了： 12345early_level4_pgt[511] -&gt; level3_kernel_pgt[0]level3_kernel_pgt[510] -&gt; level2_kernel_pgt[0]level3_kernel_pgt[511] -&gt; level2_fixmap_pgt[0]level2_kernel_pgt[0] -&gt; 512 MB kernel mappinglevel2_fixmap_pgt[507] -&gt; level1_fixmap_pgt 需要注意的是，我们并不修正 early_level4_pgt 以及其他页目录的基地址，我们会在构造、填充这些页目录结构的时候修正。我们修正了页表基地址后，就可以开始构造这些页目录了。 Identity Map Paging 现在我们可以进入到对初期页表进行 Identity 映射的初始化过程了。在 Identity 映射分页中，虚拟地址会被映射到地址相同的物理地址上，即 1 : 1。下面我们来看一下细节。首先我们找到 _text 与 _early_level4_pgt 的 RIP 相对地址，并把他们放入 rdi 与 rbx 寄存器中。 12leaq _text(%rip), %rdileaq early_level4_pgt(%rip), %rbx 在此之后我们使用 rax 保存 _text 的地址。同时，在全局页目录表中有一条记录中存放的是 _text 的地址。为了得到这条索引，我们把 _text 的地址右移 PGDIR_SHIFT 位。 123456movq %rdi, %raxshrq $PGDIR_SHIFT, %raxleaq (4096 + _KERNPG_TABLE)(%rbx), %rdxmovq %rdx, 0(%rbx,%rax,8)movq %rdx, 8(%rbx,%rax,8) 其中 PGDIR_SHIFT 为 39。PGDIR_SHIFT表示的是在虚拟地址下的全局页目录位的屏蔽值（mask）。下面的宏定义了所有类型的页目录的屏蔽值： 123#define PGDIR_SHIFT 39#define PUD_SHIFT 30#define PMD_SHIFT 21 此后我们就将 level3_kernel_pgt 的地址放进 rdx 中，并将它的访问权限设置为 _KERNPG_TABLE，然后将 level3_kernel_pgt 填入 early_level4_pgt 的两项中。 然后我们给 rdx 寄存器加上 4096（即 early_level4_pgt 的大小），并把 rdi 寄存器的值（即 _text 的物理地址）赋值给 rax 寄存器。之后我们把上层页目录中的两个项写入 level3_kernel_pgt： 12345678addq $4096, %rdxmovq %rdi, %raxshrq $PUD_SHIFT, %raxandl $(PTRS_PER_PUD-1), %eaxmovq %rdx, 4096(%rbx,%rax,8)incl %eaxandl $(PTRS_PER_PUD-1), %eaxmovq %rdx, 4096(%rbx,%rax,8) 下一步我们把中层页目录表项的地址写入 level2_kernel_pgt，然后修正内核的 text 和 data 的虚拟地址： 123456789 leaq level2_kernel_pgt(%rip), %rdi leaq 4096(%rdi), %r81: testq $1, 0(%rdi) jz 2f addq %rbp, 0(%rdi)2: addq $8, %rdi cmp %r8, %rdi jne 1b 这里首先把 level2_kernel_pgt 的地址赋值给 rdi，并把页表项的地址赋值给 r8 寄存器。下一步我们来检查 level2_kernel_pgt 中的存在位，如果其为0，就把 rdi 加上8以便指向下一个页。然后我们将其与 r8（即页表项的地址）作比较，不相等的话就跳转回前面的标签 1 ，反之则继续运行。 接下来我们使用 rbp （即 _text 的物理地址）来修正 phys_base 物理地址。将 early_level4_pgt 的物理地址与 rbp 相加，然后跳转至标签 1： 123addq %rbp, phys_base(%rip)movq $(early_level4_pgt - __START_KERNEL_map), %raxjmp 1f 其中 phys_base 与 level2_kernel_pgt 第一项相同，为 512 MB的内核映射 跳转至内核入口点之前的最后准备 此后我们就跳转至标签1来开启 PAE 和 PGE （Paging Global Extension），并且将phys_base的物理地址（见上）放入 rax 就寄存器，同时将其放入 cr3 寄存器： 1234561: movl $(X86_CR4_PAE | X86_CR4_PGE), %ecx movq %rcx, %cr4 addq phys_base(%rip), %rax movq %rax, %cr3 接下来我们检查CPU是否支持 NX 位： 123movl $0x80000001, %eaxcpuidmovl %edx,%edi 首先将 0x80000001 放入 eax 中，然后执行 cpuid 指令来得到处理器信息。这条指令的结果会存放在 edx 中，我们把他再放到 edi 里。 现在我们把 MSR_EFER （即 0xc0000080）放入 ecx，然后执行 rdmsr 指令来读取CPU中的Model Specific Register (MSR)。 12movl $MSR_EFER, %ecxrdmsr 返回结果将存放于 edx:eax 。下面展示了 EFER 各个位的含义： 12345678910111263 32 --------------------------------------------------------------------------------| || Reserved MBZ || | --------------------------------------------------------------------------------31 16 15 14 13 12 11 10 9 8 7 1 0 --------------------------------------------------------------------------------| | T | | | | | | | | | || Reserved MBZ | C | FFXSR | LMSLE |SVME|NXE|LMA|MBZ|LME|RAZ|SCE|| | E | | | | | | | | | | -------------------------------------------------------------------------------- 在这里我们不会介绍每一个位的含义，没有涉及到的位和其他的 MSR 将会在专门的部分介绍。在我们将 EFER 读入 edx:eax 之后，通过 btsl 来将 _EFER_SCE （即第0位）置1，设置 SCE 位将会启用 SYSCALL 以及 SYSRET 指令。下一步我们检查 edi（即 cpuid 的结果（见上）） 中的第20位。如果第 20 位（即 NX 位）置位，我们就只把 EFER_SCE写入MSR。 123456 btsl $_EFER_SCE, %eax btl $20,%edi jnc 1f btsl $_EFER_NX, %eax btsq $_PAGE_BIT_NX,early_pmd_flags(%rip)1: wrmsr 如果支持 NX 那么我们就把 _EFER_NX 也写入MSR。在设置了 NX 后，还要对 cr0 （control register） 中的一些位进行设置： X86_CR0_PE - 系统处于保护模式; X86_CR0_MP - 与CR0的TS标志位一同控制 WAIT/FWAIT 指令的功能； X86_CR0_ET - 386允许指定外部数学协处理器为80287或80387; X86_CR0_NE - 如果置位，则启用内置的x87浮点错误报告，否则启用PC风格的x87错误检测； X86_CR0_WP - 如果置位，则CPU在特权等级为0时无法写入只读内存页; X86_CR0_AM - 当AM位置位、EFLGS中的AC位置位、特权等级为3时，进行对齐检查; X86_CR0_PG - 启用分页. 12345#define CR0_STATE (X86_CR0_PE | X86_CR0_MP | X86_CR0_ET | \\ X86_CR0_NE | X86_CR0_WP | X86_CR0_AM | \\ X86_CR0_PG)movl $CR0_STATE, %eaxmovq %rax, %cr0 为了从汇编执行C语言代码，我们需要建立一个栈。首先将栈指针 指向一个内存中合适的区域，然后重置FLAGS寄存器 123movq stack_start(%rip), %rsppushq $0popfq 在这里最有意思的地方在于 stack_start。它也定义在 arch/x86/boot/compressed/head_64.S中： 12GLOBAL(stack_start).quad init_thread_union+THREAD_SIZE-8 对于 GLOABL 我们应该很熟悉了。它在 arch/x86/include/asm/linkage.h 头文件中定义如下： 123#define GLOBAL(name) \\ .globl name; \\ name: THREAD_SIZE 定义在 arch/x86/include/asm/page_64_types.h，它依赖于 KASAN_STACK_ORDER 的值: 12#define THREAD_SIZE_ORDER (2 + KASAN_STACK_ORDER)#define THREAD_SIZE (PAGE_SIZE &lt;&lt; THREAD_SIZE_ORDER) 首先来考虑当禁用了 kasan并且 PAGE_SIZE 大小为4096时的情况。此时 THREAD_SIZE 将为 16 KB，代表了一个线程的栈的大小。为什么是线程？我们知道每一个进程可能会有父进程和子进程。事实上，父进程和子进程使用不同的栈空间，每一个新进程都会拥有一个新的内核栈。在Linux内核中，这个栈由 thread_info 结构中的一个union表示： 1234union thread_union &#123; struct thread_info thread_info; unsigned long stack[THREAD_SIZE/sizeof(long)];&#125;; 例如，init_thread_union定义如下： 12union thread_union init_thread_union __init_task_data = &#123; INIT_THREAD_INFO(init_task) &#125;; 其中 INIT_THREAD_INFO 接受 task_struct 结构类型的参数，并进行一些初始化操作： 1234567#define INIT_THREAD_INFO(tsk) \\&#123; \\ .task = &amp;tsk, \\ .flags = 0, \\ .cpu = 0, \\ .addr_limit = KERNEL_DS, \\&#125; task_struct 结构在内核中代表了对进程的描述。因此，thread_union 包含了关于一个进程的低级信息，并且其位于进程栈底： 12345678910111213+-----------------------+| || || || Kernel stack || || || ||-----------------------|| || struct thread_info || |+-----------------------+ 需要注意的是我们在栈顶保留了 8 个字节的空间，用来保护对下一个内存页的非法访问。 在初期启动栈设置好之后，使用 lgdt 指令来更新全局描述符表： 1lgdt early_gdt_descr(%rip) 其中 early_gdt_descr 定义如下： 1234early_gdt_descr: .word GDT_ENTRIES*8-1early_gdt_descr_base: .quad INIT_PER_CPU_VAR(gdt_page) 需要重新加载 全局描述附表 的原因是，虽然目前内核工作在用户空间的低地址中，但很快内核将会在它自己的内存地址空间中运行。下面让我们来看一下 early_gdt_descr 的定义。全局描述符表包含了32项，用于内核代码、数据、线程局部存储段等： 1#define GDT_ENTRIES 32 现在来看一下 early_gdt_descr_base. 首先，gdt_page 的定义在arch/x86/include/asm/desc.h中: 123struct gdt_page &#123; struct desc_struct gdt[GDT_ENTRIES];&#125; __attribute__((aligned(PAGE_SIZE))); 它只包含了一项 desc_struct 的数组gdt。desc_struct定义如下: 1234567891011121314struct desc_struct &#123; union &#123; struct &#123; unsigned int a; unsigned int b; &#125;; struct &#123; u16 limit0; u16 base0; unsigned base1: 8, type: 4, s: 1, dpl: 2, p: 1; unsigned limit: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8; &#125;; &#125;; &#125; __attribute__((packed)); 它跟 GDT 描述符的定义很像。同时需要注意的是，gdt_page结构是 PAGE_SIZE( 4096) 对齐的，即 gdt 将会占用一页内存。 下面我们来看一下 INIT_PER_CPU_VAR，它定义在 arch/x86/include/asm/percpu.h，只是将给定的参数与 init_per_cpu__连接起来： 1#define INIT_PER_CPU_VAR(var) init_per_cpu__##var 所以在宏展开之后，我们会得到 init_per_cpu__gdt_page。而在 linker script 中可以发现 12#define INIT_PER_CPU(x) init_per_cpu__##x = x + __per_cpu_loadINIT_PER_CPU(gdt_page); INIT_PER_CPU 扩展后也将得到 init_per_cpu__gdt_page 并将它的值设置为相对于 __per_cpu_load 的偏移量。这样，我们就得到了新GDT的正确的基地址。 per-CPU变量是2.6内核中的特性。顾名思义，当我们创建一个 per-CPU 变量时，每个CPU都会拥有一份它自己的拷贝，在这里我们创建的是 gdt_page per-CPU变量。这种类型的变量有很多有点，比如由于每个CPU都只访问自己的变量而不需要锁等。因此在多处理器的情况下，每一个处理器核心都将拥有一份自己的 GDT 表，其中的每一项都代表了一块内存，这块内存可以由在这个核心上运行的线程访问。 在加载好了新的全局描述附表之后，跟之前一样我们重新加载一下各个段： 123456xorl %eax,%eaxmovl %eax,%dsmovl %eax,%ssmovl %eax,%esmovl %eax,%fsmovl %eax,%gs 在所有这些步骤都结束后，我们需要设置一下 gs 寄存器，令它指向一个特殊的栈 irqstack，用于处理中断： 1234movl $MSR_GS_BASE,%ecxmovl initial_gs(%rip),%eaxmovl initial_gs+4(%rip),%edxwrmsr 其中， MSR_GS_BASE 为： 1#define MSR_GS_BASE 0xc0000101 我们需要把 MSR_GS_BASE 放入 ecx 寄存器，同时利用 wrmsr 指令向 eax 和 edx 处的地址加载数据（即指向 initial_gs）。cs, fs, ds 和 ss 段寄存器在64位模式下不用来寻址，但 fs 和 gs 可以使用。 fs 和 gs 有一个隐含的部分（与实模式下的 cs 段寄存器类似），这个隐含部分存储了一个描述符，其指向 Model Specific Registers。因此上面的 0xc0000101 是一个 gs.base MSR 地址。当发生系统调用或者 中断时，入口点处并没有内核栈，因此 MSR_GS_BASE 将会用来存放中断栈。 接下来我们把实模式中的 bootparam 结构的地址放入 rdi (要记得 rsi 从一开始就保存了这个结构体的指针)，然后跳转到C语言代码： 12345movq initial_code(%rip),%raxpushq $0pushq $__KERNEL_CSpushq %raxlretq 这里我们把 initial_code 放入 rax 中，并且向栈里分别压入一个无用的地址、__KERNEL_CS 和 initial_code 的地址。随后的 lreq 指令表示从栈上弹出返回地址并跳转。initial_code 同样定义在这个文件里： 123456.balign 8GLOBAL(initial_code).quad x86_64_start_kernel......... 可以看到 initial_code 包含了 x86_64_start_kernel 的地址，其定义在 arch/x86/kerne/head64.c： 12345asmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data) &#123; ... ... ...&#125; 这个函数接受一个参数 real_mode_data（刚才我们把实模式下数据的地址保存到了 rdi 寄存器中）。 这个函数是内核中第一个执行的C语言代码！ 走进start_kernel 在我们真正到达“内核入口点”-init/main.c中的start_kernel函数之前，我们还需要最后的准备工作： 首先在 x86_64_start_kernel 函数中可以看到一些检查工作： 12345678BUILD_BUG_ON(MODULES_VADDR &lt; __START_KERNEL_map);BUILD_BUG_ON(MODULES_VADDR - __START_KERNEL_map &lt; KERNEL_IMAGE_SIZE);BUILD_BUG_ON(MODULES_LEN + KERNEL_IMAGE_SIZE &gt; 2*PUD_SIZE);BUILD_BUG_ON((__START_KERNEL_map &amp; ~PMD_MASK) != 0);BUILD_BUG_ON((MODULES_VADDR &amp; ~PMD_MASK) != 0);BUILD_BUG_ON(!(MODULES_VADDR &gt; __START_KERNEL));BUILD_BUG_ON(!(((MODULES_END - 1) &amp; PGDIR_MASK) == (__START_KERNEL &amp; PGDIR_MASK)));BUILD_BUG_ON(__fix_to_virt(__end_of_fixed_addresses) &lt;= MODULES_END); 这些检查包括：模块的虚拟地址不能低于内核 text 段基地址 __START_KERNEL_map ，包含模块的内核 text 段的空间大小不能小于内核镜像大小等等。BUILD_BUG_ON 宏定义如下： 1#define BUILD_BUG_ON(condition) ((void)sizeof(char[1 - 2*!!(condition)])) 我们来理解一下这些巧妙的设计是怎么工作的。首先以第一个条件 MODULES_VADDR &lt; __START_KERNEL_map 为例：!!conditions 等价于 condition != 0，这代表如果 MODULES_VADDR &lt; __START_KERNEL_map 为真，则 !!(condition) 为1，否则为0。执行2*!!(condition)之后数值变为 2 或 0。因此，这个宏执行完后可能产生两种不同的行为： 编译错误。因为我们尝试取获取一个字符数组索引为负数的变量的大小。 没有编译错误。 就是这么简单，通过C语言中某些常量导致编译错误的技巧实现了这一设计。 接下来 start_kernel 调用了 cr4_init_shadow 函数，其中存储了每个CPU中 cr4 的Shadow Copy。上下文切换可能会修改 cr4 中的位，因此需要保存每个CPU中 cr4 的内容。在这之后将会调用 reset_early_page_tables 函数，它重置了所有的全局页目录项，同时向 cr3 中重新写入了的全局页目录表的地址： 123456for (i = 0; i &lt; PTRS_PER_PGD-1; i++) early_level4_pgt[i].pgd = 0;next_early_pgt = 0;write_cr3(__pa_nodebug(early_level4_pgt)); 很快我们就会设置新的页表。在这里我们遍历了所有的全局页目录项（其中 PTRS_PER_PGD 为 512），将其设置为0。之后将 next_early_pgt 设置为0（，同时把 early_level4_pgt 的物理地址写入 cr3。__pa_nodebug 是一个宏，将被扩展为： 1((unsigned long)(x) - __START_KERNEL_map + phys_base) 此后我们清空了从 __bss_stop 到 __bss_start 的 _bss 段，下一步将是建立初期 IDT（中断描述符表） 的处理代码. 初期中断和异常处理 我们在上一部分谈到了下面这个循环： 12for (i = 0; i &lt; NUM_EXCEPTION_VECTORS; i++) set_intr_gate(i, early_idt_handler_array[i]); 这段代码位于 arch/x86/kernel/head64.c。在分析这段代码之前，我们先来了解一些关于中断和中断处理程序的知识。 理论 中断是一种由软件或硬件产生的、向CPU发出的事件。例如，如果用户按下了键盘上的一个按键时，就会产生中断。此时CPU将会暂停当前的任务，并且将控制流转到特殊的程序中—— 中断处理程序(Interrupt Handler)。一个中断处理程序会对中断进行处理，然后将控制权交还给之前暂停的任务中。中断分为三类： 软件中断 - 当一个软件可以向CPU发出信号，表明它需要系统内核的相关功能时产生。这些中断通常用于系统调用； 硬件中断 - 当一个硬件有任何事件发生时产生，例如键盘的按键被按下； 异常 - 当CPU检测到错误时产生，例如发生了除零错误或者访问了一个不存在的内存页。 每一个中断和异常都可以由一个数来表示，这个数叫做 向量号 ，它可以取从 0 到 255 中的任何一个数。通常在实践中前 32 个向量号用来表示异常，32 到 255 用来表示用户定义的中断。可以看到在上面的代码中，NUM_EXCEPTION_VECTORS 就定义为： 1#define NUM_EXCEPTION_VECTORS 32 CPU会从 APIC 或者 CPU 引脚接收中断，并使用中断向量号作为 中断描述符表 的索引。下面的表中列出了 0-31 号异常： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647----------------------------------------------------------------------------------------------|Vector|Mnemonic|Description |Type |Error Code|Source |----------------------------------------------------------------------------------------------|0 | #DE |Divide Error |Fault|NO |DIV and IDIV ||---------------------------------------------------------------------------------------------|1 | #DB |Reserved |F/T |NO | ||---------------------------------------------------------------------------------------------|2 | --- |NMI |INT |NO |external NMI ||---------------------------------------------------------------------------------------------|3 | #BP |Breakpoint |Trap |NO |INT 3 ||---------------------------------------------------------------------------------------------|4 | #OF |Overflow |Trap |NO |INTO instruction ||---------------------------------------------------------------------------------------------|5 | #BR |Bound Range Exceeded|Fault|NO |BOUND instruction ||---------------------------------------------------------------------------------------------|6 | #UD |Invalid Opcode |Fault|NO |UD2 instruction ||---------------------------------------------------------------------------------------------|7 | #NM |Device Not Available|Fault|NO |Floating point or [F]WAIT ||---------------------------------------------------------------------------------------------|8 | #DF |Double Fault |Abort|YES |Ant instrctions which can generate NMI||---------------------------------------------------------------------------------------------|9 | --- |Reserved |Fault|NO | ||---------------------------------------------------------------------------------------------|10 | #TS |Invalid TSS |Fault|YES |Task switch or TSS access ||---------------------------------------------------------------------------------------------|11 | #NP |Segment Not Present |Fault|NO |Accessing segment register ||---------------------------------------------------------------------------------------------|12 | #SS |Stack-Segment Fault |Fault|YES |Stack operations ||---------------------------------------------------------------------------------------------|13 | #GP |General Protection |Fault|YES |Memory reference ||---------------------------------------------------------------------------------------------|14 | #PF |Page fault |Fault|YES |Memory reference ||---------------------------------------------------------------------------------------------|15 | --- |Reserved | |NO | ||---------------------------------------------------------------------------------------------|16 | #MF |x87 FPU fp error |Fault|NO |Floating point or [F]Wait ||---------------------------------------------------------------------------------------------|17 | #AC |Alignment Check |Fault|YES |Data reference ||---------------------------------------------------------------------------------------------|18 | #MC |Machine Check |Abort|NO | ||---------------------------------------------------------------------------------------------|19 | #XM |SIMD fp exception |Fault|NO |SSE[2,3] instructions ||---------------------------------------------------------------------------------------------|20 | #VE |Virtualization exc. |Fault|NO |EPT violations ||---------------------------------------------------------------------------------------------|21-31 | --- |Reserved |INT |NO |External interrupts |---------------------------------------------------------------------------------------------- 为了能够对中断进行处理，CPU使用了一种特殊的结构 - 中断描述符表（IDT）。IDT 是一个由描述符组成的数组，其中每个描述符都为8个字节，与全局描述附表一致；不过不同的是，我们把IDT中的每一项叫做 门(gate) 。为了获得某一项描述符的起始地址，CPU 会把向量号乘以8，在64位模式中则会乘以16。在前面我们已经见过，CPU使用一个特殊的 GDTR 寄存器来存放全局描述符表的地址，中断描述符表也有一个类似的寄存器 IDTR ，同时还有用于将基地址加载入这个寄存器的指令 lidt 。 64位模式下 IDT 的每一项的结构如下： 123456789101112131415161718192021222324127 96 --------------------------------------------------------------------------------| || Reserved || | --------------------------------------------------------------------------------95 64 --------------------------------------------------------------------------------| || Offset 63..32 || | --------------------------------------------------------------------------------63 48 47 46 44 42 39 34 32 --------------------------------------------------------------------------------| | | D | | | | | | || Offset 31..16 | P | P | 0 |Type |0 0 0 | 0 | 0 | IST || | | L | | | | | | | --------------------------------------------------------------------------------31 15 16 0 --------------------------------------------------------------------------------| | || Segment Selector | Offset 15..0 || | | -------------------------------------------------------------------------------- 其中: Offset - 代表了到中断处理程序入口点的偏移； DPL - 描述符特权级别； P - Segment Present 标志; Segment selector - 在GDT或LDT中的代码段选择子； IST - 用来为中断处理提供一个新的栈。 最后的 Type 域描述了这一项的类型，中断处理程序共分为三种： 任务描述符 中断描述符 陷阱描述符 中断和陷阱描述符包含了一个指向中断处理程序的远 (far) 指针，二者唯一的不同在于CPU处理 IF 标志的方式。如果是由中断门进入中断处理程序的，CPU 会清除 IF 标志位，这样当当前中断处理程序执行时，CPU 不会对其他的中断进行处理；只有当当前的中断处理程序返回时，CPU 才在 iret 指令执行时重新设置 IF 标志位。 中断门的其他位为保留位，必须为0。下面我们来看一下 CPU 是如何处理中断的： CPU 会在栈上保存标志寄存器、cs段寄存器和程序计数器IP； 如果中断是由错误码引起的（比如 #PF）， CPU会在栈上保存错误码； 在中断处理程序执行完毕后，由iret指令返回。 设置并加载 IDT 我们分析到了如下代码： 12for (i = 0; i &lt; NUM_EXCEPTION_VECTORS; i++) set_intr_gate(i, early_idt_handler_array[i]); 这里循环内部调用了 set_intr_gate ，它接受两个参数： 中断号，即 向量号； 中断处理程序的地址。 同时，这个函数还会将中断门插入至 IDT 表中，代码中的 &amp;idt_descr 数组即为 IDT。 首先让我们来看一下 early_idt_handler_array 数组，它定义在 arch/x86/include/asm/segment.h头文件中，包含了前32个异常处理程序的地址： 1234#define EARLY_IDT_HANDLER_SIZE 9#define NUM_EXCEPTION_VECTORS 32extern const char early_idt_handler_array[NUM_EXCEPTION_VECTORS][EARLY_IDT_HANDLER_SIZE]; early_idt_handler_array 是一个大小为 288 字节的数组，每一项为 9 个字节，其中2个字节的备用指令用于向栈中压入默认错误码（如果异常本身没有提供错误码的话），2个字节的指令用于向栈中压入向量号，剩余5个字节用于跳转到异常处理程序 在上面的代码中，我们只通过一个循环向 IDT 中填入了前32项内容，这是因为在整个初期设置阶段，中断是禁用的。early_idt_handler_array 数组中的每一项指向的都是同一个通用中断处理程序，定义在 arch/x86/kernel/head_64.S。我们先暂时跳过这个数组的内容，看一下 set_intr_gate 的定义。 set_intr_gate 宏定义在 arch/x86/include/asm/desc.h： 12345678#define set_intr_gate(n, addr) \\ do &#123; \\ BUG_ON((unsigned)n &gt; 0xFF); \\ _set_gate(n, GATE_INTERRUPT, (void *)addr, 0, 0, \\ __KERNEL_CS); \\ _trace_set_gate(n, GATE_INTERRUPT, (void *)trace_##addr,\\ 0, 0, __KERNEL_CS); \\ &#125; while (0) 首先 BUG_ON 宏确保了传入的中断向量号不会大于255，因为我们最多只有 256 个中断。然后它调用了 _set_gate 函数，它会将中断门写入 IDT： 12345678static inline void _set_gate(int gate, unsigned type, void *addr, unsigned dpl, unsigned ist, unsigned seg)&#123; gate_desc s; pack_gate(&amp;s, type, (unsigned long)addr, dpl, ist, seg); write_idt_entry(idt_table, gate, &amp;s); write_trace_idt_entry(gate, &amp;s);&#125; 在 _set_gate 函数的开始，它调用了 pack_gate 函数。这个函数会使用给定的参数填充 gate_desc 结构： 1234567891011121314static inline void pack_gate(gate_desc *gate, unsigned type, unsigned long func, unsigned dpl, unsigned ist, unsigned seg)&#123; gate-&gt;offset_low = PTR_LOW(func); gate-&gt;segment = __KERNEL_CS; gate-&gt;ist = ist; gate-&gt;p = 1; gate-&gt;dpl = dpl; gate-&gt;zero0 = 0; gate-&gt;zero1 = 0; gate-&gt;type = type; gate-&gt;offset_middle = PTR_MIDDLE(func); gate-&gt;offset_high = PTR_HIGH(func);&#125; 在这个函数里，我们把从主循环中得到的中断处理程序入口点地址拆成三个部分，填入门描述符中。下面的三个宏就用来做这个拆分工作： 123#define PTR_LOW(x) ((unsigned long long)(x) &amp; 0xFFFF)#define PTR_MIDDLE(x) (((unsigned long long)(x) &gt;&gt; 16) &amp; 0xFFFF)#define PTR_HIGH(x) ((unsigned long long)(x) &gt;&gt; 32) 调用 PTR_LOW 可以得到 x 的低 2 个字节，调用 PTR_MIDDLE 可以得到 x 的中间 2 个字节，调用 PTR_HIGH 则能够得到 x 的高 4 个字节。接下来我们来位中断处理程序设置段选择子，即内核代码段 __KERNEL_CS。然后将 Interrupt Stack Table 和 描述符特权等级 （最高特权等级）设置为0，以及在最后设置 GAT_INTERRUPT 类型。 现在我们已经设置好了IDT中的一项，那么通过调用 native_write_idt_entry 函数来复制到 IDT： 1234static inline void native_write_idt_entry(gate_desc *idt, int entry, const gate_desc *gate)&#123; memcpy(&amp;idt[entry], gate, sizeof(*gate));&#125; 主循环结束后，idt_table 就已经设置完毕了，其为一个 gate_desc 数组。然后我们就可以通过下面的代码加载 中断描述符表： 1load_idt((const struct desc_ptr *)&amp;idt_descr); 其中，idt_descr 为： 1struct desc_ptr idt_descr = &#123; NR_VECTORS * 16 - 1, (unsigned long) idt_table &#125;; load_idt 函数只是执行了一下 lidt 指令： 1asm volatile(&quot;lidt %0&quot;::&quot;m&quot; (*dtr)); 你可能已经注意到了，在代码中还有对 _trace_* 函数的调用。这些函数会用跟 _set_gate 同样的方法对 IDT 门进行设置，但仅有一处不同：这些函数并不设置 idt_table ，而是 trace_idt_table ，用于设置追踪点。 至此我们已经了解到，通过设置并加载 中断描述符表 ，能够让CPU在发生中断时做出相应的动作 初期中断处理函数 在上面的代码中，我们用 early_idt_handler_array 的地址来填充了 IDT ，这个 early_idt_handler_array 定义在 arch/x86/kernel/head_64.S： 123456789101112.globl early_idt_handler_arrayearly_idt_handlers: i = 0 .rept NUM_EXCEPTION_VECTORS .if (EXCEPTION_ERRCODE_MASK &gt;&gt; i) &amp; 1 pushq $0 .endif pushq $i jmp early_idt_handler_common i = i + 1 .fill early_idt_handler_array + i*EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc .endr 这段代码自动生成为前 32 个异常生成了中断处理程序。首先，为了统一栈的布局，如果一个异常没有返回错误码，那么我们就手动在栈中压入一个 0。然后再在栈中压入中断向量号，最后跳转至通用的中断处理程序 early_idt_handler_common 。我们可以通过 objdump 命令的输出一探究竟： 12345678910111213141516$ objdump -D vmlinux.........ffffffff81fe5000 &lt;early_idt_handler_array&gt;:ffffffff81fe5000: 6a 00 pushq $0x0ffffffff81fe5002: 6a 00 pushq $0x0ffffffff81fe5004: e9 17 01 00 00 jmpq ffffffff81fe5120 &lt;early_idt_handler_common&gt;ffffffff81fe5009: 6a 00 pushq $0x0ffffffff81fe500b: 6a 01 pushq $0x1ffffffff81fe500d: e9 0e 01 00 00 jmpq ffffffff81fe5120 &lt;early_idt_handler_common&gt;ffffffff81fe5012: 6a 00 pushq $0x0ffffffff81fe5014: 6a 02 pushq $0x2......... 由于在中断发生时，CPU 会在栈上压入标志寄存器、CS 段寄存器和 RIP 寄存器的内容。因此在 early_idt_handler 执行前，栈的布局如下： 123456|--------------------|| %rflags || %cs || %rip || rsp --&gt; error code ||--------------------| 下面我们来看一下 early_idt_handler_common 的实现。它也定义在 arch/x86/kernel/head_64.S 文件中。首先它会检查当前中断是否为 不可屏蔽中断(NMI)，如果是则简单地忽略它们： 12cmpl $2,(%rsp)je .Lis_nmi 其中 is_nmi 为: 123is_nmi: addq $16,%rsp INTERRUPT_RETURN 这段程序首先从栈顶弹出错误码和中断向量号，然后通过调用 INTERRUPT_RETURN ，即 iretq 指令直接返回。 如果当前中断不是 NMI ，则首先检查 early_recursion_flag 以避免在 early_idt_handler_common 程序中递归地产生中断。如果一切都没问题，就先在栈上保存通用寄存器，为了防止中断返回时寄存器的内容错乱： 123456789pushq %raxpushq %rcxpushq %rdxpushq %rsipushq %rdipushq %r8pushq %r9pushq %r10pushq %r11 然后我们检查栈上的段选择子： 12cmpl $__KERNEL_CS,96(%rsp)jne 11f 段选择子必须为内核代码段，如果不是则跳转到标签 11 ，输出 PANIC 信息并打印栈的内容。然后我们来检查向量号，如果是 #PF 即 缺页中断（Page Fault），那么就把 cr2 寄存器中的值赋值给 rdi ，然后调用 early_make_pgtable ： 123456cmpl $14,72(%rsp)jnz 10fGET_CR2_INTO(%rdi)call early_make_pgtableandl %eax,%eaxjz 20f 如果向量号不是 #PF ，那么就恢复通用寄存器： 123456789popq %r11popq %r10popq %r9popq %r8popq %rdipopq %rsipopq %rdxpopq %rcxpopq %rax 并调用 iret 从中断处理程序返回。 第一个中断处理程序到这里就结束了。由于它只是一个初期中断处理程序，因此只处理缺页中断。下面让我们首先来看一下缺页中断处理程序，其他中断的处理程序我们之后再进行分析。 缺页中断处理程序 在这里我们需要提供 #PF 中断处理程序，以便为之后将内核加载至 4G 地址以上，并且能访问位于4G以上的 boot_params 结构体。 early_make_pgtable 的实现在 arch/x86/kernel/head64.c，它接受一个参数：从 cr2 寄存器得到的地址，这个地址引发了内存中断。下面让我们来看一下： 1234567891011int __init early_make_pgtable(unsigned long address)&#123; unsigned long physaddr = address - __PAGE_OFFSET; unsigned long i; pgdval_t pgd, *pgd_p; pudval_t pud, *pud_p; pmdval_t pmd, *pmd_p; ... ... ...&#125; 首先它定义了一些 *val_t 类型的变量。这些类型均为： 1typedef unsigned long pgdval_t; 此外，我们还会遇见 *_t (不带val)的类型，比如 pgd_t ……这些类型都定义在 arch/x86/include/asm/pgtable_types.h，形式如下： 1typedef struct &#123; pgdval_t pgd; &#125; pgd_t; 例如， 1extern pgd_t early_level4_pgt[PTRS_PER_PGD]; 在这里 early_level4_pgt 代表了初期顶层页表目录，它是一个 pdg_t 类型的数组，其中的 pgd 指向了下一级页表。 在确认不是非法地址后，我们取得页表中包含引起 #PF 中断的地址的那一项，将其赋值给 pgd 变量： 12pgd_p = &amp;early_level4_pgt[pgd_index(address)].pgd;pgd = *pgd_p; 接下来我们检查一下 pgd ，如果它包含了正确的全局页表项的话，我们就把这一项的物理地址处理后赋值给 pud_p ： 1pud_p = (pudval_t *)((pgd &amp; PTE_PFN_MASK) + __START_KERNEL_map - phys_base); 其中 PTE_PFN_MASK 是一个宏： 1#define PTE_PFN_MASK ((pteval_t)PHYSICAL_PAGE_MASK) 展开后将为： 1(~(PAGE_SIZE-1)) &amp; ((1 &lt;&lt; 46) - 1) 或者写为： 10b1111111111111111111111111111111111111111111111 它是一个46bit大小的页帧屏蔽值。 如果 pgd 没有包含有效的地址，我们就检查 next_early_pgt 与 EARLY_DYNAMIC_PAGE_TABLES（即 64 ）的大小。EARLY_DYNAMIC_PAGE_TABLES 它是一个固定大小的缓冲区，用来在需要的时候建立新的页表。如果 next_early_pgt 比 EARLY_DYNAMIC_PAGE_TABLES 大，我们就用一个上层页目录指针指向当前的动态页表，并将它的物理地址与 _KERPG_TABLE 访问权限一起写入全局页目录表： 123456789if (next_early_pgt &gt;= EARLY_DYNAMIC_PAGE_TABLES) &#123; reset_early_page_tables(); goto again;&#125; pud_p = (pudval_t *)early_dynamic_pgts[next_early_pgt++];for (i = 0; i &lt; PTRS_PER_PUD; i++) pud_p[i] = 0;*pgd_p = (pgdval_t)pud_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE; 然后我们来修正上层页目录的地址： 12pud_p += pud_index(address);pud = *pud_p; 下面我们对中层页目录重复上面同样的操作。最后，我们确定了页中间目录的地址，其中包含了内核文本和数据的虚拟地址映射。 12pmd = (physaddr &amp; PMD_MASK) + early_pmd_flags;pmd_p[pmd_index(address)] = pmd; 到此缺页中断处理程序就完成了它所有的工作，此时 early_level4_pgt 就包含了指向合法地址的项。 进入内核入口点之前最后的准备工作 boot_params again 在上一个部分中我们讲到了设置中断描述符表，并将其加载进 IDTR 寄存器。下一步是调用 copy_bootdata 函数： 1copy_bootdata(__va(real_mode_data)); 这个函数接受一个参数—— read_mode_data 的虚拟地址。boot_params 结构体是在 arch/x86/include/uapi/asm/bootparam.h作为第一个参数传递到 arch/x86/kernel/head_64.S 中的 x86_64_start_kernel 函数的： 123/* rsi is pointer to real mode structure with interesting info. pass it to C */movq %rsi, %rdi 下面我们来看一看 __va 宏。 这个宏定义在 init/main.c： 1#define __va(x) ((void *)((unsigned long)(x)+PAGE_OFFSET)) 其中 PAGE_OFFSET 就是 __PAGE_OFFSET（即 0xffff880000000000），也是所有对物理地址进行直接映射后的虚拟基地址。因此我们就得到了 boot_params 结构体的虚拟地址，并把他传入 copy_bootdata 函数中。在这个函数里我们把 real_mod_data （定义在 arch/x86/kernel/setup.h） 拷贝进 boot_params： 1extern struct boot_params boot_params; copy_boot_data 的实现如下: 12345678910111213static void __init copy_bootdata(char *real_mode_data)&#123; char * command_line; unsigned long cmd_line_ptr; memcpy(&amp;boot_params, real_mode_data, sizeof boot_params); sanitize_boot_params(&amp;boot_params); cmd_line_ptr = get_cmd_line_ptr(); if (cmd_line_ptr) &#123; command_line = __va(cmd_line_ptr); memcpy(boot_command_line, command_line, COMMAND_LINE_SIZE); &#125;&#125; 首先，这个函数的声明中有一个 __init 前缀，这表示这个函数只在初始化阶段使用，并且它所使用的内存将会被释放。 在这个函数中首先声明了两个用于解析内核命令行的变量，然后使用memcpy 函数将 real_mode_data 拷贝进 boot_params。如果系统引导工具（bootloader）没能正确初始化 boot_params 中的某些成员的话，那么在接下来调用的 sanitize_boot_params 函数中将会对这些成员进行清零，比如 ext_ramdisk_image 等。此后我们通过调用 get_cmd_line_ptr 函数来得到命令行的地址： 123unsigned long cmd_line_ptr = boot_params.hdr.cmd_line_ptr;cmd_line_ptr |= (u64)boot_params.ext_cmd_line_ptr &lt;&lt; 32;return cmd_line_ptr; get_cmd_line_ptr 函数将会从 boot_params 中获得命令行的64位地址并返回。最后，我们检查一下是否正确获得了 cmd_line_ptr，并把它的虚拟地址拷贝到一个字节数组 boot_command_line 中： 1extern char __initdata boot_command_line[]; 这一步完成之后，我们就得到了内核命令行和 boot_params 结构体。之后，内核通过调用 load_ucode_bsp 函数来加载处理器微代码（microcode），不过我们目前先暂时忽略这一步。 微代码加载之后，内核会对 console_loglevel 进行检查，同时通过 early_printk 函数来打印出字符串 Kernel Alive。 初始化内存页 至此，我们已经拷贝了 boot_params 结构体，接下来将对初期页表进行一些设置以便在初始化内核的过程中使用。我们之前已经对初始化了初期页表，以便支持换页。现在则通过调用 reset_early_page_tables 函数将初期页表中大部分项清零（在之前的部分也有介绍），只保留内核高地址的映射。然后我们调用： 1clear_page(init_level4_pgt); init_level4_pgt 同样定义在 arch/x86/kernel/head_64.S 123456NEXT_PAGE(init_level4_pgt) .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_level4_pgt + L4_PAGE_OFFSET*8, 0 .quad level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE .org init_level4_pgt + L4_START_KERNEL*8, 0 .quad level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE 这段代码为内核的代码段、数据段和 bss 段映射了前 2.5G 个字节。clear_page 函数定义在 arch/x86/lib/clear_page_64.S： 1234567891011121314151617181920212223ENTRY(clear_page) CFI_STARTPROC xorl %eax,%eax movl $4096/64,%ecx .p2align 4 .Lloop: decl %ecx#define PUT(x) movq %rax,x*8(%rdi) movq %rax,(%rdi) PUT(1) PUT(2) PUT(3) PUT(4) PUT(5) PUT(6) PUT(7) leaq 64(%rdi),%rdi jnz .Lloop nop ret CFI_ENDPROC .Lclear_page_end: ENDPROC(clear_page) 顾名思义，这个函数会将页表清零。这个函数的开始和结束部分有两个宏 CFI_STARTPROC 和 CFI_ENDPROC，他们会展开成 GNU 汇编指令，用于调试： 12#define CFI_STARTPROC .cfi_startproc#define CFI_ENDPROC .cfi_endproc 在 CFI_STARTPROC 之后我们将 eax 寄存器清零，并将 ecx 赋值为 64（用作计数器）。接下来从 .Lloop 标签开始循环，首先就是将 ecx 减一。然后将 rax 中的值（目前为0）写入 rdi 指向的地址，rdi 中保存的是 init_level4_pgt 的基地址。接下来重复7次这个步骤，但是每次都相对 rdi 多偏移8个字节。之后 init_level4_pgt 的前64个字节就都被填充为0了。接下来我们将 rdi 中的值加上64，重复这个步骤，直到 ecx 减至0。最后就完成了将 init_level4_pgt 填零。 在将 init_level4_pgt 填0之后，再把它的最后一项设置为内核高地址的映射： 1init_level4_pgt[511] = early_level4_pgt[511]; 在前面我们已经使用 reset_early_page_table 函数清除 early_level4_pgt 中的大部分项，而只保留内核高地址的映射。 x86_64_start_kernel 函数的最后一步是调用： 1x86_64_start_reservations(real_mode_data); 并传入 real_mode_data 参数。 x86_64_start_reservations 函数与 x86_64_start_kernel 函数定义在同一个文件中： 123456789void __init x86_64_start_reservations(char *real_mode_data)&#123; if (!boot_params.hdr.version) copy_bootdata(__va(real_mode_data)); reserve_ebda_region(); start_kernel();&#125; 内核入口点前的最后一步 在 x86_64_start_reservations 函数中首先检查了 boot_params.hdr.version： 12if (!boot_params.hdr.version) copy_bootdata(__va(real_mode_data)); 如果它为0，则再次调用 copy_bootdata，并传入 real_mode_data 的虚拟地址。 接下来则调用了 reserve_ebda_region 函数，它定义在 arch/x86/kernel/head.c。这个函数为 EBDA（即Extended BIOS Data Area，扩展BIOS数据区域）预留空间。扩展BIOS预留区域位于常规内存顶部（译注：常规内存（Conventiional Memory）是指前640K字节内存），包含了端口、磁盘参数等数据。 接下来我们来看一下 reserve_ebda_region 函数。它首先会检查是否启用了半虚拟化： 12if (paravirt_enabled()) return; 如果开启了半虚拟化，那么就退出 reserve_ebda_region 函数，因为此时没有扩展BIOS数据区域。下面我们首先得到低地址内存的末尾地址： 12lowmem = *(unsigned short *)__va(BIOS_LOWMEM_KILOBYTES);lowmem &lt;&lt;= 10; 首先我们得到了BIOS地址内存的虚拟地址，以KB为单位，然后将其左移10位（即乘以1024）转换为以字节为单位。然后我们需要获得扩展BIOS数据区域的地址： 1ebda_addr = get_bios_ebda(); 其中， get_bios_ebda 函数定义在 arch/x86/include/asm/bios_ebda.h： 123456static inline unsigned int get_bios_ebda(void)&#123; unsigned int address = *(unsigned short *)phys_to_virt(0x40E); address &lt;&lt;= 4; return address;&#125; 下面我们来尝试理解一下这段代码。这段代码中，首先我们将物理地址 0x40E 转换为虚拟地址，0x0040:0x000e 就是包含有扩展BIOS数据区域基地址的代码段。这里我们使用了 phys_to_virt 函数进行地址转换，而不是之前使用的 __va 宏。不过，事实上他们两个基本上是一样的： 1234static inline void *phys_to_virt(phys_addr_t address)&#123; return __va(address);&#125; 而不同之处在于，phys_to_virt 函数的参数类型 phys_addr_t 的定义依赖于 CONFIG_PHYS_ADDR_T_64BIT： 12345#ifdef CONFIG_PHYS_ADDR_T_64BIT typedef u64 phys_addr_t;#else typedef u32 phys_addr_t;#endif 具体的类型是由 CONFIG_PHYS_ADDR_T_64BIT 设置选项控制的。此后我们得到了包含扩展BIOS数据区域虚拟基地址的段，把它左移4位后返回。这样，ebda_addr 变量就包含了扩展BIOS数据区域的基地址。 下一步我们来检查扩展BIOS数据区域与低地址内存的地址，看一看它们是否小于 INSANE_CUTOFF 宏： 12345if (ebda_addr &lt; INSANE_CUTOFF) ebda_addr = LOWMEM_CAP;if (lowmem &lt; INSANE_CUTOFF) lowmem = LOWMEM_CAP; INSANE_CUTOFF 为： 1#define INSANE_CUTOFF 0x20000U 即 128 KB. 上一步我们得到了低地址内存中的低地址部分以及扩展BIOS数据区域，然后调用 memblock_reserve 函数来在低内存地址与1MB之间为扩展BIOS数据预留内存区域。 123lowmem = min(lowmem, ebda_addr);lowmem = min(lowmem, LOWMEM_CAP);memblock_reserve(lowmem, 0x100000 - lowmem); memblock_reserve 函数定义在 mm/block.c，它接受两个参数： 基物理地址 区域大小 然后在给定的基地址处预留指定大小的内存。memblock_reserve 是在这本书中我们接触到的第一个Linux内核内存管理框架中的函数。 Linux内核管理框架初探 memblock_reserve 函数只是调用了： 1memblock_reserve_region(base, size, MAX_NUMNODES, 0); memblock_reserve_region 接受四个参数： 内存区域的物理基地址 内存区域的大小 最大 NUMA 节点数 标志参数 flags 在 memblock_reserve_region 函数一开始，就是一个 memblock_type 结构体类型的变量： 1struct memblock_type *_rgn = &amp;memblock.reserved; memblock_type 类型代表了一块内存，定义如下： 123456struct memblock_type &#123; unsigned long cnt; unsigned long max; phys_addr_t total_size; struct memblock_region *regions;&#125;; 因为我们要为扩展BIOS数据区域预留内存块，所以当前内存区域的类型就是预留。memblock 结构体的定义为： 123456789struct memblock &#123; bool bottom_up; phys_addr_t current_limit; struct memblock_type memory; struct memblock_type reserved;#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP struct memblock_type physmem;#endif&#125;; 它描述了一块通用的数据块。我们用 memblock.reserved 的值来初始化 _rgn。memblock 全局变量定义如下： 123456789101112131415struct memblock memblock __initdata_memblock = &#123; .memory.regions = memblock_memory_init_regions, .memory.cnt = 1, .memory.max = INIT_MEMBLOCK_REGIONS, .reserved.regions = memblock_reserved_init_regions, .reserved.cnt = 1, .reserved.max = INIT_MEMBLOCK_REGIONS,#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP .physmem.regions = memblock_physmem_init_regions, .physmem.cnt = 1, .physmem.max = INIT_PHYSMEM_REGIONS,#endif .bottom_up = false, .current_limit = MEMBLOCK_ALLOC_ANYWHERE,&#125;; 我们现在不会继续深究这个变量，但在内存管理部分的中我们会详细地对它进行介绍。需要注意的是，这个变量的声明中使用了 __initdata_memblock： 1#define __initdata_memblock __meminitdata 而 __meminit_data 为： 1#define __meminitdata __section(.meminit.data) 自此我们得出这样的结论：所有的内存块都将定义在 .meminit.data 区段中。在我们定义了 _rgn 之后，使用了 memblock_dbg 宏来输出相关的信息。你可以在从内核命令行传入参数 memblock=debug 来开启这些输出。 在输出了这些调试信息后，是对下面这个函数的调用： 1memblock_add_range(_rgn, base, size, nid, flags); 它向 .meminit.data 区段添加了一个新的内存块区域。由于 _rgn 的值是 &amp;memblock.reserved，下面的代码就直接将扩展BIOS数据区域的基地址、大小和标志填入 _rgn 中： 123456789if (type-&gt;regions[0].size == 0) &#123; WARN_ON(type-&gt;cnt != 1 || type-&gt;total_size); type-&gt;regions[0].base = base; type-&gt;regions[0].size = size; type-&gt;regions[0].flags = flags; memblock_set_region_node(&amp;type-&gt;regions[0], nid); type-&gt;total_size = size; return 0;&#125; 在填充好了区域后，接着是对 memblock_set_region_node 函数的调用。它接受两个参数： 填充好的内存区域的地址 NUMA节点ID 其中我们的区域由 memblock_region 结构体来表示： 12345678struct memblock_region &#123; phys_addr_t base; phys_addr_t size; unsigned long flags;#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP int nid;#endif&#125;; NUMA节点ID依赖于 MAX_NUMNODES 宏，定义在 include/linux/numa.h 1#define MAX_NUMNODES (1 &lt;&lt; NODES_SHIFT) 其中 NODES_SHIFT 依赖于 CONFIG_NODES_SHIFT 配置参数，定义如下： 12345#ifdef CONFIG_NODES_SHIFT #define NODES_SHIFT CONFIG_NODES_SHIFT#else #define NODES_SHIFT 0#endif memblick_set_region_node 函数只是填充了 memblock_region 中的 nid 成员： 1234static inline void memblock_set_region_node(struct memblock_region *r, int nid)&#123; r-&gt;nid = nid;&#125; 在这之后我们就在 .meminit.data 区段拥有了为扩展BIOS数据区域预留的第一个 memblock。reserve_ebda_region 已经完成了它该做的任务，我们回到 arch/x86/kernel/head64.c 继续。 至此我们已经结束了进入内核之前所有的准备工作。x86_64_start_reservations 的最后一步是调用 init/main.c 中的： 1start_kernel()","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blackforest1990.github.io/categories/Linux/"}],"tags":[{"name":"Kernel","slug":"Kernel","permalink":"https://blackforest1990.github.io/tags/Kernel/"}]},{"title":"位","slug":"位","date":"2024-04-17T02:47:49.000Z","updated":"2024-04-17T06:18:24.718Z","comments":true,"path":"2024/04/17/位/","link":"","permalink":"https://blackforest1990.github.io/2024/04/17/%E4%BD%8D/","excerpt":"","text":"Bits速查表 字码word 在电脑领域，对于某种特定的计算机设计而言，字（英语：word）是用于表示其自然的数据单位的术语。在这个特定电脑中，字是其用来一次性处理事务的一个固定长度的位（bit）组。一个字的位数（即字长）是电脑系统结构中的一个重要特性。 字长度在计算机结构和操作的多个方面均有体现。电脑中大多数寄存器的大小是一个字长。电脑处理的典型数值也可能是以字长为单位。CPU和内存之间的数据传送单位也通常是一个字长。还有在内存中用于指明一个存储位置的地址也经常是以字长为单位的。 现代电脑的字长通常为16、32、64位。其他曾经使用过的字长有：8、9、12、18、24、36、39、40、48、60位；slab是早期的另一个字长实例。某些最早期的电脑是十进制的而不是二进制的，通常拥有10位或者12位的十进制数字作为字长，还有一些早期的电脑根本就没有固定字长。 字的使用 根据计算机的组织情况，字长单位可能被应用到： 整数–计算机处理的整数值通常可以有若干种不同的长度，但是其中总有一种正好是该构架的字长。如果有的话，其他的整数长度很可能是字长的倍数或分数。小尺寸的整数尺寸通常是为了提高存储效率；当它被加载处理器时，它经常被转换成字长尺寸的形式。 浮点数–计算机处理的浮点通常是一个字长或字长的倍数。 地址–计算机处理的存储器地址必须有足够的尺寸，以便可以表示需要的数值范围，但是又不能过大。经常使用的尺寸是字，不过也可以是字的倍数或分数。 寄存器– 处理器寄存器根据它要处理的数据类型被设计成适当的尺寸，例如：整数、浮点数、地址。许多计算机构架使用“通用”寄存器，它们可以存储任何类型的数据，可以允许存储哪怕是最大的数据类型。它们的尺寸通常是其构架的字长。 存储器－处理器传送–当处理器从存储器子系统读取数据至寄存器，或者，写寄存器数据到存储器，传送的数据通常是字。在简单的存储系统中，字在数据总线上传送，它一般为一个字或半个字。在使用缓存的存储系统，在处理器和一级缓存之间传送的是一个字长，而在更低级的存储层次上传送的可能是更大的尺寸（这尺寸是字的倍数）。 编址单位–在一个给定的构架中，连续的地址值对应连续的存储器单位；该单位就是编址单位。在大部分计算机中，这个单位或者是一个字符（例如：字节）或者是一个字（少部分计算机也使用位（bit）编址单位）。如果单位是字，那么用指定长度的地址就可以访问较大数量的存储空间。另一方面，如果单位是字节，那么就可以访问单个的字符。 指令– 机器指令通常是字长的分数或倍数。因为指令和数据经常共享同一个存储子系统，所以自然作出这样的选择。 位操作 位操作是通过算法操作位或其他比字短的数据片段的行为。需要位操作的计算机编程任务包括低级设备控制、错误检测和纠正算法、数据压缩、加密算法和优化。对于大多数其他任务，现代编程语言允许程序员直接使用抽象而不是代表这些抽象的位。 源代码中进行位操作的部分利用了按位操作符：与（AND）、或（OR）、异或（XOR）、取反（NOT），可能还包括其他类似布尔运算符的操作；还有位移操作和计算1和0的操作，查找最高和最低的1或0，设置、重置和测试位，提取和插入字段，掩码和清零字段，在指定的位位置或字段之间收集和散布位。整数算术运算符也可以与其他运算符一起影响位操作。 一些更有用和更复杂的位操作必须在编程语言中编码为习惯用法并由编译器综合，包括： 从指定位位置向上清除（保留字的下半部分） 从指定位位置向下清除（保留字的上部） 从低位向下屏蔽（清除低位字） 掩码从高位向上（清除低位字） 位域提取 位域插入 位域分散/聚集操作，将位域的连续部分分布在机器字上，或将字中不同的位域收集到位域的连续部分中（请参阅最近的英特尔 PEXT/PDEP 运算符）。用于密码学和视频编码。 矩阵求逆 一些算术运算可以简化为更简单的运算和位运算： 将乘以常数减少为移位加法序列 将常数除法简化为移位减法序列 掩码是用于按位运算的数据，特别是在位字段中。使用掩码，可以在单个按位操作中将Byte、nibble、word（等）中的多个位设置为开、关或从开反转（反之亦然）。当有条件地应用于操作时，掩码的更全面的应用被称为预测。 具体技巧可以参考这个网站：https://bits.stephan-brumme.com/","categories":[{"name":"编程","slug":"编程","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"bit","slug":"bit","permalink":"https://blackforest1990.github.io/tags/bit/"}]},{"title":"算法","slug":"算法","date":"2024-04-17T01:33:17.000Z","updated":"2024-04-17T01:45:56.604Z","comments":true,"path":"2024/04/17/算法/","link":"","permalink":"https://blackforest1990.github.io/2024/04/17/%E7%AE%97%E6%B3%95/","excerpt":"","text":"搜索算法 二分查找 二分搜索被定义为一种在排序数组中使用的搜索算法，通过重复将搜索间隔一分为二。二分查找的思想是利用数组已排序的信息，将时间复杂度降低到O(log N)。 何时在数据结构中应用二分查找的条件： 应用二分查找算法： 数据结构必须是有序的。 访问数据结构的任何元素都需要恒定的时间。 二分查找算法： 在这个算法中， 通过查找中间索引“mid”将搜索空间分为两半。 将搜索空间的中间元素与键进行比较。 如果在中间元素找到密钥，则过程终止。 如果在中间元素没有找到键，则选择哪一半将用作下一个搜索空间。 如果键小于中间元素，则使用左侧进行下一步搜索。 如果键大于中间元素，则使用右侧进行下一步搜索。 这个过程一直持续到找到密钥或者总搜索空间耗尽为止。 如何实现二分查找？ 二分查找算法可以通过以下两种方式实现 迭代二分搜索算法 递归二分查找算法 超越数组：离散二分搜索 这就是我们开始抽象二分搜索的地方。序列（数组）实际上只是一个将整数（索引）与相应值关联起来的函数。然而，没有理由将我们对二分搜索的使用限制在有形序列上。事实上，我们可以对任何定义域为整数集的单调函数 f 使用上述相同的算法。唯一的区别是我们用函数求值替换了数组查找：我们现在正在寻找一些 x 使得 f(x) 等于目标值。搜索空间现在更正式地是函数定义域的一个子区间，而目标值是值域的一个元素。二分查找的强大之处开始展现：我们最多需要O(log N)次比较来找到目标值，而且我们也不需要多次评估函数。此外，在这种情况下，我们不受可用内存等实际量的限制，就像数组的情况一样。","categories":[{"name":"编程","slug":"编程","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://blackforest1990.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"“新小说提纲”","slug":"“新小说提纲”","date":"2024-04-03T07:34:27.000Z","updated":"2024-04-11T07:38:46.056Z","comments":true,"path":"2024/04/03/“新小说提纲”/","link":"","permalink":"https://blackforest1990.github.io/2024/04/03/%E2%80%9C%E6%96%B0%E5%B0%8F%E8%AF%B4%E6%8F%90%E7%BA%B2%E2%80%9D/","excerpt":"","text":"背景：三国时代，灵气回归，改变了地球的环境，修道和修武成了唯一的依仗，同时基因锁的打开，让人类的大脑重新进化，开始探索星球，上古疑似拥有璀璨的文明，但同时可能存在巨大威胁。 猪脚：幽州人，装逼大王，实用主义者，人类之盾，自称汉室宗亲，make 大汉 great again，刘备刘玄德是也。 修行体系：目前两种，道法和武修，尽可能简单，后续会再进行发展。 道法：入道 + 金丹 武修：练皮 + 练骨 + 练气 世界观宏大一些，有罗马","categories":[{"name":"策划","slug":"策划","permalink":"https://blackforest1990.github.io/categories/%E7%AD%96%E5%88%92/"}],"tags":[{"name":"小说提纲","slug":"小说提纲","permalink":"https://blackforest1990.github.io/tags/%E5%B0%8F%E8%AF%B4%E6%8F%90%E7%BA%B2/"}]},{"title":"数据结构","slug":"数据结构","date":"2024-04-03T07:33:43.000Z","updated":"2024-04-17T07:10:08.410Z","comments":true,"path":"2024/04/03/数据结构/","link":"","permalink":"https://blackforest1990.github.io/2024/04/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"算法复杂度主要分为两部分：时间和空间，目前主要的算法的复杂度统计如下 时间复杂度：时间复杂度是指在计算机科学与工程领域完成一个算法所需要的时间，是衡量一个算法优劣的重要参数。时间复杂度越小，说明该算法效率越高，则该算法越有价值。 空间复杂度：空间复杂度是指计算机科学领域完成一个算法所需要占用的存储空间，一般是输入参数的函数。它是算法优劣的重要度量指针，一般来说，空间复杂度越小，算法越好。 大O符号（O符号）：表示算法的渐近上界。如果一个算法的时间复杂度为O(g(n))，那么对于足够大的输入规模n，该算法的运行时间不会超过g(n)的常数倍。例如，如果一个算法的时间复杂度为O(n)，则表示其运行时间不会超过n的线性函数。 **大θ符号（θ符号）**用于描述算法的渐近紧确界，表示算法的时间复杂度在上界和下界之间。具体来说，如果一个算法的时间复杂度为θ(g(n))，则对于足够大的输入规模n，该算法的运行时间会在g(n)的常数倍内。换句话说，θ(g(n))表示算法的运行时间与g(n)成正比。如果一个算法的时间复杂度既是O(g(n))又是Ω(g(n))，则可以用θ(g(n))表示。 **大Ω符号（Ω符号）**用于描述算法的渐近下界，表示算法的时间复杂度至少为某个函数的增长率。具体来说，如果一个算法的时间复杂度为Ω(g(n))，则对于足够大的输入规模n，该算法的运行时间会至少与g(n)成正比。 数组 在计算机科学中，数组数据结构（英语：array data structure），简称数组（英语：Array），是由相同类型的元素（element）的集合所组成的数据结构，分配一块连续的内存来存储。利用元素的索引（index）可以计算出该元素对应的存储地址。 数组是最早期和最重要的数据结构之一，很多程序都会用到数组。它们也用于实现许多其他数据结构，譬如列表（list）和字符串（string）。它们有成效地开展了计算机的寻址逻辑。在大多数现代计算机和许多外部存储设备中，存储器如同一维数组，索引就是其地址。编译器、处理单元（特别是向量处理器），经常会针对数组操作进行优化。 数组是计算机科学中的基本数据结构。它们用于多种应用，包括： 存储数据以供处理 实现堆栈和队列等数据结构 在表和矩阵中表示数据 创建动态数据结构，例如链表和树 数组类型 数组可以按两种方式分类： 基于内存分配 根据尺寸 数组运算 对数组执行的常见操作包括： 遍历：以特定顺序（例如顺序、反向）访问数组的每个元素。 插入：将新元素添加到数组的特定索引处。 删除：从数组中删除特定索引处的元素。 搜索：查找数组中元素的索引。 数组的优点 数组允许随机访问元素。这使得按位置访问元素更快。 数组具有更好的缓存局部性，这在性能上有很大的差异。 数组使用单个名称表示相同类型的多个数据项。 数组用于实现其他数据结构，如链表、堆栈、队列、树、图等。 链表 链表（英语：Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。 链表基本术语 头：链表的头是指向链表第一个节点的指针或第一个节点的引用。该指针标记链表的开始。 节点：链表由一系列节点组成，每个节点有两部分：数据和下一个指针。 数据：数据是链表中存储信息的节点部分。 下一个指针：下一个指针是指向链表下一个节点的节点部分。 链表的重要性 动态数据结构：可以在运行时根据操作插入或删除来分配或取消分配内存大小。 易于插入/删除：元素的插入和删除比数组简单，因为插入和删除后不需要移动元素，只需更新地址。 高效的内存利用：众所周知，链表是一种动态数据结构，其大小根据要求增加或减少，从而避免了内存的浪费。 实现：可以使用链表来实现各种高级数据结构，如堆栈、队列、图、哈希图等。 链表类型 单向链表：链表中最简单的一种是单向链表，它包含两个域，一个信息域和一个指针域。这个链接指向列表中的下一个节点，而最后一个节点则指向一个空值。 双向链表：一种更复杂的链表是“双向链表”或“双面链表”。每个节点有两个连接：一个指向前一个节点，（当此“连接”为第一个“连接”时，指向空值或者空列表）；而另一个指向下一个节点，（当此“连接”为最后一个“连接”时，指向空值或者空列表） 循环链表：循环链表是一种链表，其中第一个和最后一个节点也相互连接形成一个循环，末尾没有NULL。 链表和数组的对比 Array Linked List Arrays are stored in contiguous location. Linked Lists are not stored in contiguous location. Fixed size. Dynamic Size Memory is allocated at compile time. Memory is allocated at run time. Uses less memory than Linked Lists. Uses more memory than Arrays as it stores both data and address of next node. Elements can be accessed easily in O(1) time. Elements can be access by traversing through all the nodes till we reach the required node. Insertion and deletion operation is slower than Linked List. Insertion and deletion operation is faster than Linked List. 链表的缺点 内存使用：链表中指针的使用较多，因此比较复杂并且需要更多内存。 访问节点：由于动态内存分配，随机访问是不可能的。 搜索操作代价高昂：搜索元素代价高昂，需要 O(n) 时间复杂度。 逆序遍历：遍历比较耗时，单链表中无法逆序遍历。 链表的应用 线性数据结构（例如堆栈、队列）和非线性数据结构（例如哈希图和图）可以使用链表来实现。 动态内存分配：我们使用空闲块的链表。 图的实现：图的邻接列表表示是最流行的，因为它使用链表来存储相邻顶点。 在网络浏览器和编辑器中，双向链表可用于构建向前和向后导航按钮。 循环双向链表也可用于实现斐波那契堆等数据结构。 堆栈 堆栈（stack）又称为栈或堆叠，是计算机科学中的一种抽象资料类型，只允许在有序的线性资料集合的一端（称为堆栈顶端，top）进行加入数据（push）和移除数据（pop）的运算。因而按照后进先出（LIFO, Last In First Out）的原理运作，堆栈常用一维数组或链表来实现。 基本操作 为了在堆栈中进行操作，向我们提供了某些操作。 push()将一个元素插入栈中 pop()从堆栈中删除一个元素 top()返回栈顶元素。 如果堆栈为空，isEmpty()返回 true，否则返回 false。 如果堆栈已满，isFull()返回 true，否则返回 false。 堆栈实现 可以在堆栈上执行的基本操作包括压入、弹出和查看。有两种方法来实现堆栈 – 使用数组 使用链接列表 在基于数组的实现中，推送操作是通过递增顶部元素的索引并将新元素存储在该索引处来实现的。 pop操作是通过将元素存储在顶部，将顶部元素的索引递减并返回存储的值来实现的。 在基于链表的实现中，推送操作是通过使用新元素创建新节点并将当前顶节点的下一个指针设置为新节点来实现的。出栈操作是通过将当前顶节点的next指针设置为下一个节点并返回当前顶节点的值来实现的。 数组与链表实现的优缺点 Array Linked List 易于实施 堆栈的链表实现可以根据运行时的需要进行增长和收缩 由于不涉及指针，因此节省了内存 它被用在许多虚拟机中，比如 JVM 它不是动态的，即它不会根据运行时的需要而增长和收缩。 由于涉及指针，需要额外的内存 堆栈的总大小必须事先定义 堆栈中不可能进行随机访问 堆栈的优缺点 优点 缺点 简单性：堆栈是一种简单且易于理解的数据结构，使其适用于广泛的应用程序。 访问受限：只能从顶部访问堆栈中的元素，因此很难检索或修改堆栈中间的元素 效率：堆栈上的压入和弹出操作可以在恒定时间(O(1))内执行，从而提供对数据的高效访问。 溢出的可能性：如果压入堆栈的元素多于堆栈所能容纳的元素，就会发生溢出错误，从而导致数据丢失。 后进先出 (LIFO)：堆栈遵循 LIFO 原则，确保最后添加到堆栈的元素是第一个被删除的元素。此行为在许多场景中都很有用，例如函数调用和表达式求值。 不适合随机访问：堆栈不允许随机访问元素，这使得它们不适合需要按特定顺序访问元素的应用程序。 有限的内存使用：堆栈只需要存储已压入其中的元素，与其他数据结构相比，堆栈的内存效率更高。 容量有限：堆栈具有固定容量，如果需要存储的元素数量未知或高度可变，这可能会成为限制。 堆栈的应用 中缀到后缀前缀的转换 许多地方都有重做/撤消功能，例如编辑器、Photoshop。 网络浏览器中的前进和后退功能 在内存管理中，任何现代计算机都使用堆栈作为运行目的的主要管理。计算机系统中运行的每个程序都有自己的内存分配。 堆栈还有助于在计算机中实现函数调用。最后调用的函数总是最先完成。 队列 队列，计算机科学中的一种抽象资料类型，是先进先出（FIFO, First-In-First-Out）的线性表。在具体应用中通常用链表或者数组来实现。队列只允许在后端（称为rear）进行插入操作，在前端（称为front）进行删除操作。 队列类型 **输入受限队列：**这是一个简单的队列。在这种类型的队列中，只能从一端获取输入，但可以从任意一端进行删除。 **输出受限队列：**这也是一个简单的队列。在这种类型的队列中，可以从两端获取输入，但只能从一端进行删除。 **循环队列：**这是一种特殊类型的队列，其中最后一个位置连接回第一个位置。这里的操作也是按照 FIFO 顺序执行的。 **双端队列：**在双端队列中插入和删除操作，都可以从两端进行。 **优先级队列：**优先级队列是一种特殊的队列，其中的元素根据分配给它们的优先级进行访问。 队列基本操作 **Enqueue() –**将一个元素添加（或存储）到队列末尾。 **Dequeue() –**从队列中删除元素。 **Peek() 或 front() -**获取队列前端节点可用的数据元素，而不删除它。 **after() –**此操作返回后端的元素而不删除它。 **isFull() –**验证队列是否已满。 **isNull() –**检查队列是否为空。 队列的应用 队列的应用很常见。在计算机系统中，可能存在等待打印机、访问磁盘存储、甚至在分时系统中等待使用 CPU 的任务队列。在单个程序中，可能有多个请求需要保留在队列中，或者一个任务可能会创建其他任务，而这些任务必须通过将它们保留在队列中来依次完成。 它具有单一资源和多个消费者。 它在慢速和快速设备之间进行同步。 在网络中，队列用于路由器/交换机和邮件队列等设备。 变体：出队、优先级队列和双端优先级队列。 哈希表 散列表（Hash table，也叫哈希表），是根据键（Key）而直接访问在存储器存储位置的数据结构。也就是说，它通过计算出一个键值的函数，将所需查询的数据映射到表中一个位置来让人访问，这加快了查找速度。这个映射函数称做[散列函数，存放记录的数组称做散列表。 若关键字为k，则其值存放在f(x)的存储位置上。由此，不需比较便可直接获取所查记录。称这个对应关系f为散列函数，按这个思想建立的表为散列表。 对不同的关键字可能得到同一散列地址，即k1 ≠ k2,而f(k1)=f(k2)，这种现象称为冲突（Collision）。具有相同函数值的关键字对该散列函数来说称做同义词。综上所述，根据散列函数f(k)和处理冲突的方法将一组关键字映射到一个有限的连续的地址集上，并以关键字在地址集中的“像”作为记录在表中的存储位置，这种表便称为散列表，这一映射过程称为散列，所得的存储位置称散列地址。 若对于关键字集合中的任一个关键字，经散列函数镜像到地址集合中任何一个地址的概率是相等的，则称此类散列函数为均匀散列函数（Uniform Hash function），这就使关键字经过散列函数得到一个“随机的地址”，从而减少冲突。 好的哈希函数的属性 将每个项目映射到其自己的唯一槽的哈希函数称为完美哈希函数。如果我们知道项目并且集合永远不会改变，我们就可以构造一个完美的哈希函数，但问题是，给定任意项目集合，没有系统的方法来构造完美的哈希函数。幸运的是，即使哈希函数并不完美，我们仍然可以获得性能效率。我们可以通过增加哈希表的大小来实现完美的哈希函数，以便可以容纳每一个可能的值。因此，每个项目都会有一个独特的插槽。虽然这种方法对于少量项目是可行的，但当可能性数量很大时就不实用了。 因此，我们可以构造我们的哈希函数来执行相同的操作，但在构造我们自己的哈希函数时必须注意的事情。 一个好的哈希函数应该具有以下属性： 高效可计算。 应该均匀地分配键。 应尽量减少冲突。 应具有较低的负载因子（表中的项目数除以表的大小）。 如何处理碰撞？ 处理碰撞主要有两种方法： 单独链接： 开放寻址： 细节请参考：https://www.geeksforgeeks.org/introduction-to-hashing-data-structure-and-algorithm-tutorials/?ref=ghm 什么是重新哈希？ 顾名思义，重新哈希意味着再次哈希。基本上，当负载因子增加到超过其预定义值（负载因子的默认值为 0.75）时，复杂性就会增加。因此，为了克服这个问题，数组的大小增加（加倍），所有值再次散列并存储在新的双倍大小的数组中，以保持低负载因子和低复杂性。 哈希的应用 哈希表用于多种应用，包括： 数据库：基于唯一键存储和检索数据 缓存：存储经常访问的数据以加快检索速度 符号表：将标识符映射到编程语言中的值 网络路由：确定数据包的最佳路径 树 通用树 通用树是节点的集合，其中每个节点都是一个由记录和对其子节点的引用列表组成的数据结构（不允许重复引用）。与链表不同，每个节点存储多个节点的地址。每个节点存储其子节点的地址，第一个节点的地址将存储在称为根的单独指针中。 通用树是 N 叉树，具有以下属性： ​ 1. 每个节点都有很多孩子。 ​ 2.每个节点的节点数量是事先未知的。 例子： 为了表示上面的树，我们必须考虑最坏的情况，即具有最多子节点的节点（在上面的示例中，有 6 个子节点），并为每个节点分配那么多的指针。 基于该方法的节点表示可以写为： 12345678910//Node declaration struct Node&#123; int data; struct Node *firstchild; struct Node *secondchild; struct Node *thirdchild; struct Node *fourthchild; struct Node *fifthchild; struct Node *sixthchild; &#125; 内存浪费——并非所有情况下都需要所有指针。因此，存在大量内存浪费。 子节点数量未知- 每个节点的子节点数量事先未知。 简单的方法： 为了将子节点的地址存储在节点中，我们可以使用数组或链表。但我们将面临他们两个的一些问题。 在链表中，我们不能随机访问任何孩子的地址。 在array中，我们可以随机访问任何孩子的地址，但我们只能在其中存储固定数量的孩子的地址。 更好的方法： 我们可以使用动态数组来存储孩子的地址。我们可以随机访问任何孩子的地址，并且向量的大小也不固定。 高效的方法： 第一个孩子/下一个兄弟姐妹代表 在每个节点上，从左到右链接同一父级（兄弟姐妹）的子级。 删除从父级到除第一个子级之外的所有子级的链接。 由于我们在孩子之间有联系，因此我们不需要父母到所有孩子的额外联系。这种表示允许我们从父元素的第一个子元素开始遍历所有元素。 第一个子节点/下一个同级表示的节点声明可以写为： 123456//Node declaration struct Node&#123; int data; struct Node *firstChild; struct Node *nextSibling; &#125; 优点： 内存效率高——不需要额外的链接，因此节省了大量内存。 被视为二叉树 - 由于我们能够将任何通用树转换为二元表示，因此我们可以将具有第一个子/下一个兄弟表示的所有通用树视为二叉树。我们只使用firstChild和nextSibling，而不是左右指针。 许多算法可以更容易地表达，因为它只是一棵二叉树。 每个节点的大小都是固定的，因此不需要辅助数组或向量。 二叉树 二叉树是一种树形数据结构，其中每个节点最多可以有两个子节点，称为左子节点和右子节点。 二叉树中最顶部的节点称为根，最底部的节点称为叶。二叉树可以可视化为层次结构，根位于顶部，叶子位于底部。 二叉树的表示： 树中的每个节点包含以下内容： 数据 指向左孩子的指针 指向右孩子的指针 1234567// Structure of each node of the treestruct node &#123; int data; struct node* left; struct node* right;&#125;; 二叉树的基本操作 插入一个元素。 删除一个元素。 寻找一个元素。 删除一个元素。 遍历一个元素。 二叉树的辅助操作 找到树的高度 查找树的级别 求整棵树的大小。 二叉树的应用 在编译器中，使用表达式树，它是二叉树的应用。 霍夫曼编码树用于数据压缩算法。 优先级队列是二叉树的另一个应用，用于以 O(1) 时间复杂度搜索最大值或最小值。 表示分层数据。 用于 Microsoft Excel 和电子表格等编辑软件。 对于数据库中的分段索引很有用，对于在系统中存储缓存也很有用， 语法树用于大多数著名的编译器进行编程，例如 GCC 和 AOCL 来执行算术运算。 用于实现优先级队列。 用于在更短的时间内查找元素（二叉搜索树） 用于启用计算机中的快速内存分配。 用于执行编码和解码操作。 二叉树可用于组织和检索大型数据集中的信息，例如倒排索引和 kd 树。 二叉树可以用来表示游戏中计算机控制角色的决策过程，例如决策树。 二叉树可用于实现搜索算法，例如二叉搜索树可用于快速查找排序列表中的元素。 二叉树可用于实现排序算法，例如堆排序，它使用二叉堆对元素进行有效排序。 二叉树遍历 树遍历算法大致可以分为两类： 深度优先搜索 (DFS) 算法 广度优先搜索 (BFS) 算法 使用深度优先搜索（DFS）算法的树遍历可以进一步分为三类： **前序遍历（当前-左-右）：**在访问左子树或右子树内的任何节点之前先访问当前节点。这里的遍历是根-左孩子-右孩子。 **中序遍历（左-当前-右）：**在访问左子树内的所有节点之后但在访问右子树内的任何节点之前访问当前节点。这里的遍历是左孩子-根-右孩子。 **后序遍历（左-右-当前）：**访问完左右子树的所有节点后，再访问当前节点。 这里的遍历是左孩子-右孩子-根。 使用广度优先搜索（BFS）算法的树遍历可以进一步分为一类： 层级顺序遍历： 逐级、从左到右地访问同一层级的节点。这里，遍历是逐级的。意思是最左边的孩子先遍历完，然后从左到右同级的其他孩子都遍历完。 让我们用所有四种遍历方法来遍历下面的树： 前序遍历： 1-2-4-5-3-6-7 中序遍历： 4-2-5-1-6-3-7 后序遍历： 4-5-2-6-7-3-1 层序遍历： 1-2-3-4-5-6-7 二叉搜索树 **二叉搜索树（BST）**是一种特殊类型的二叉树，其中节点的左子节点的值小于该节点的值，而右子节点的值大于该节点的值。这个属性称为 BST 属性，它使得高效地搜索、插入和删除树中的元素成为可能。 二叉搜索树的性质： 节点的左子树仅包含键小于该节点键的节点。 节点的右子树仅包含键大于该节点键的节点。 这意味着根左边的所有内容都小于根的值，根右边的所有内容都大于根的值。由于这种执行，二分查找非常容易。 左子树和右子树也必须是二叉搜索树。 不能有重复的节点（BST可能有重复的值，处理方式不同） 处理二叉搜索树中的重复值： 我们必须始终遵循一致的过程，即要么将重复值存储在根的左侧，要么将重复值存储在根的右侧，但要与您的方法保持一致。 在 BST 上执行的操作： 1.将节点插入 BST 中 新密钥始终插入到叶子处。从根开始搜索键直到叶节点。一旦找到叶节点，新节点就会添加为叶节点的子节点。 2.删除BST的节点：用于从BST中删除具有特定键的节点并返回新的BST。 删除节点的不同场景： 要删除的节点是叶子节点：很简单，将其置空即可。 要删除的节点有一个子节点：只需用子节点替换该节点即可。 要删除的节点有两个子节点：这里我们必须以这样的方式删除节点，即生成的树遵循 BST 的属性。 诀窍是找到节点的中序后继者。将中序后继的内容复制到该节点，并删除中序后继。 删除BST节点时要注意以下事项： 需要弄清楚要删除的节点将被什么替换。 希望对现有树结构的破坏最小化 可以从删除的节点左子树或右子树中取出替换节点。 如果从左子树中选择，我们必须选择左子树中的最大值。 如果从右子树中选择，我们必须取右子树中的最小值。 3.遍历（BST 的中序遍历）：对于二叉搜索树（BST），中序遍历以非降序给出节点。我们首先访问左孩子，然后是根，然后是右孩子。 BST的应用： 图算法：BST 可用于实现图算法，例如最小生成树算法。 优先级队列：BST可用于实现优先级队列，其中具有最高优先级的元素位于树的根部，而具有较低优先级的元素存储在子树中。 自平衡二叉搜索树：BST可以用作AVL树、红黑树等自平衡数据结构。 数据存储和检索：BST 可用于快速存储和检索数据，例如在数据库中，搜索特定记录可以在对数时间内完成。 优点： 快速搜索：在 BST 中搜索特定值的平均时间复杂度为 O(log n)，其中 n 是树中的节点数。这比在数组或链表中搜索元素要快得多，在最坏的情况下，数组或链表的时间复杂度为 O(n)。 中序遍历： BST可以按顺序遍历，即访问左子树、根和右子树。这可用于对数据集进行排序。 空间效率高：BST 空间效率高，因为与数组和链表不同，它们不存储任何冗余信息。 缺点： 倾斜树：如果一棵树变得倾斜，则搜索、插入和删除操作的时间复杂度将是 O(n) 而不是 O(log n)，这会使树效率低下。 需要额外的时间：自平衡树在插入和删除操作期间需要额外的时间来保持平衡。 效率： BST 对于具有许多重复项的数据集效率不高，因为它们会浪费空间。","categories":[{"name":"编程","slug":"编程","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blackforest1990.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"virtual memory","slug":"virtual-memory","date":"2024-03-26T02:22:06.000Z","updated":"2024-03-31T10:24:40.976Z","comments":true,"path":"2024/03/26/virtual-memory/","link":"","permalink":"https://blackforest1990.github.io/2024/03/26/virtual-memory/","excerpt":"","text":"虚拟内存是一种允许执行部分未完全加载到内存中的进程的技术。这种方案的一个主要优势是程序可以比物理内存更大。此外，虚拟内存将主存储器抽象为一个非常大、统一的存储数组，将用户视图中的逻辑内存与物理内存分离开来。这种技术使程序员摆脱了内存存储限制的担忧。虚拟内存还允许进程轻松共享文件并实现共享内存。此外，它提供了一个高效的进程创建机制。然而，虚拟内存不易实现，如果使用不慎可能会显著降低性能。 背景 内存管理算法是必要的，因为有一个基本要求：正在执行的指令必须在物理内存中。满足这一要求的第一种方法是将整个逻辑地址空间放置在物理内存中。动态加载可以帮助缓解这个限制，但通常需要程序员采取特殊预防措施并进行额外的工作。 指令必须在物理内存中才能执行的要求看起来既必要又合理；但同时也很不幸，因为它限制了程序的大小与物理内存的大小相同。事实上，对真实程序的检查表明，在许多情况下，并不需要整个程序。例如，考虑以下情况： 程序通常会包含处理异常错误条件的代码。由于这些错误在实践中很少发生，甚至几乎从不发生，因此这些代码几乎不会被执行。 数组、列表和表通常分配比实际需要更多的内存。例如，一个数组可能声明为 100 行 100 列，即使它很少会超过 10 行 10 列。汇编器符号表可能有 3000 个符号的空间，尽管平均程序只有不到 200 个符号。 程序的某些选项和功能可能很少被使用。例如，美国政府计算机上用于平衡预算的程序已经多年没有被使用过了。 即使在那些需要整个程序的情况下，也不一定需要同时加载全部程序。 能够执行仅部分加载到内存中的程序将带来许多好处： 程序将不再受物理内存限制。用户能够为极大的虚拟地址空间编写程序，简化编程任务。 由于每个用户程序所需的物理内存更少，因此可以运行更多程序，CPU利用率和吞吐量相应增加，但响应时间或周转时间不会增加。 加载或交换用户程序到内存中将需要更少的I/O，因此每个用户将运行更快。 进程的虚拟地址空间是指进程在内存中存储的逻辑（或虚拟）视图。通常，这个视图是进程从某个逻辑地址（比如说，地址0）开始，并以连续的内存存在，如下图所示。然而，实际上物理内存可能是以页框（page frames）的形式组织的，而分配给一个进程的物理页框可能不是连续的。将逻辑页映射到内存中的物理页框是由内存管理单元（MMU）负责的。 堆通过动态内存分配向上增长。同样的，栈通过连续函数调用向内存下方增长。堆和栈之间的空白区域是虚拟地址空间的一部分，但只有在堆和栈增长时才需要实际的物理页。包含空洞的虚拟地址空间被称为稀疏地址空间sparse address space。使用稀疏地址空间是有益的，因为随着堆或栈段的增长，空洞可以被填充，或者如果我们希望在程序执行期间动态链接库（或可能是其他共享对象）。 除了将逻辑内存与物理内存分离外，虚拟内存还通过页面共享允许两个或多个进程共享文件和内存。这带来了以下好处： 系统库可以通过将共享对象映射到虚拟地址空间中而被多个进程共享。虽然每个进程都将库视为其虚拟地址空间的一部分，但库实际所在的物理内存页面是所有进程共享的。通常，每个链接到库的进程的空间中都将库映射为只读。 类似地，进程可以共享内存。虚拟内存允许一个进程创建一个可与另一个进程共享的内存区域。共享此区域的进程将其视为其虚拟地址空间的一部分，但实际的物理内存页面是共享的。 在进程创建时，可以通过 fork() 系统调用共享页面，从而加快进程创建的速度。 需求分页 考虑一下如何将可执行程序从磁盘加载到内存中。一种选择是在程序执行时将整个程序加载到物理内存中。然而，这种方法的问题是我们可能最初并不需要将整个程序加载到内存中。另一种策略是仅在需要时加载页面。这种技术称为需求分页demanding paging，常用于虚拟内存系统中。使用需求分页的虚拟内存系统仅在程序执行期间需要时加载页面。因此，从未访问过的页面也不会加载到物理内存中。 需求分页系统类似于带有交换的分页系统，其中进程驻留在磁盘中。当我们想要执行一个进程时，我们将其交换到内存中。然而，与将整个进程交换到内存不同，我们使用了一种惰性换页器lazy swapper。惰性换页器永远不会将一个页面交换到内存中，除非该页面将被需要。在需求分页系统的背景下，使用术语“换页器”是技术上不正确的。换页器操作整个进程，而页式存储管理器则涉及进程的单个页面。因此，我们在需求分页中使用“页式存储管理器pager”而不是“换页器swapper”。 基本原理 当要将一个进程换入时，页式存储管理器会猜测在进程再次被交换出之前将使用哪些页面。页式存储管理器不是将整个进程换入内存，而是仅将那些页面换入内存。因此，它避免了将不会被使用的页面读入内存，从而减少了交换时间和所需物理内存的数量。 使用这种方案，我们需要一些形式的硬件支持来区分哪些页面在内存中，哪些页面在磁盘上。有效-无效位方案可以用于这个目的。然而，这次，当该位被设置为“有效”时，相关的页面既合法又在内存中。如果该位被设置为“无效”，则页面可能无效（即不在进程的逻辑地址空间中），或者有效但当前在磁盘上。将页面带入内存的页表项被设置为通常的方式，但是不在内存中的页面的页表项要么只是标记为无效，要么包含页面在磁盘上的地址。 注意，如果进程从未尝试访问该页面，则将页面标记为无效将不会产生任何效果。因此，如果我们猜测正确，并且只换入实际需要的所有页面以及仅这些页面，那么该进程将与我们完全换入所有页面时运行的方式完全相同。当进程执行并访问驻留在内存中的页面时，执行会正常进行。 但是如果进程尝试访问未被换入内存的页面会发生什么呢？对标记为无效的页面的访问会导致页面错误(page fault)。在通过页表进行地址转换时，分页硬件会注意到无效位被设置，从而导致陷阱传递给操作系统。这个陷阱是操作系统未能将所需页面换入内存的结果。处理这个页面错误的过程很简单: 我们检查该进程的内部表（通常与进程控制块一起保存）来确定引用是有效还是无效的内存访问。 如果引用无效，我们终止该进程。如果引用有效但我们尚未换入该页面，则现在换入该页面。 我们找到一个空闲帧（例如，通过从空闲帧列表中获取一个）。 我们安排一个磁盘操作，将所需的页面读入新分配的帧中。 当磁盘读取完成后，我们修改保存在进程和页表中的内部表，表示该页面现在在内存中。 我们重新启动被陷阱中断的指令。该进程现在可以访问该页面，就好像它一直在内存中一样。 在极端情况下，我们可以开始执行一个没有任何页面在内存中的进程。当操作系统将指令指针设置为进程的第一条指令，该指令位于一个非内存驻留页面上时，进程立即出现页面错误。在将此页面换入内存后，进程继续执行，必要时出现页面错误，直到它需要的每个页面都在内存中为止。在那时，它可以执行而不再出现页面错误。这种方案是纯需求分页pure demand paging：直到需要时才将页面换入内存。 理论上，一些程序可能在每次指令执行时访问多个新页面的内存（一个页面用于指令，多个页面用于数据），可能导致每个指令多次页面错误。这种情况将导致系统性能不可接受。幸运的是，对正在运行的进程的分析显示，这种行为极不可能发生。程序倾向于具有引用局部性，这导致了需求分页的合理性能。 支持需求分页的硬件与支持分页和交换的硬件相同： 页表。这个表具有通过有效-无效位或保护位的特殊值来标记条目无效的能力。 辅助存储器。这个存储器保存那些不在主存储器中的页面。辅助存储器通常是一个高速磁盘。它被称为交换设备，用于此目的的磁盘部分称为交换空间。 需求分页的一个关键要求是在页面错误后能够重新启动任何指令。因为当页面错误发生时，我们会保存被中断进程的状态（寄存器、条件码、指令计数器），所以我们必须能够将进程在完全相同的位置和状态下重新启动，唯一的区别是所需页面现在在内存中并且可访问。在大多数情况下，这个要求很容易满足。页面错误可能发生在任何内存引用上。如果页面错误发生在指令取指时，我们可以重新取指再次执行指令。如果在取操作数时发生页面错误，我们必须重新取指并解码指令，然后再次取操作数。 作为最坏情况的示例，考虑一个三地址指令，例如将A的内容与B相加，并将结果放入C。执行该指令的步骤如下： 取指并解码指令（ADD）。 取A。 取B。 将A和B相加。 将和存储在C中。 如果我们尝试将结果存储在C时出现页面错误（因为C所在的页面当前不在内存中），我们将不得不获取所需的页面，将其换入内存，更新页表，并重新启动指令。重新启动将需要重新取指，重新解码指令，再次获取两个操作数，然后再次执行加法。然而，重复工作并不多（不到一个完整指令），并且只有在发生页面错误时才需要重复。 当一个指令可能修改多个不同的位置时，主要困难出现在这里。这个问题可以通过两种不同的方式解决。在一种解决方案中，微码计算并尝试访问两个块的两端。如果将要发生页面错误，它将发生在此步骤之前，即在任何内容被修改之前。然后可以进行移动操作；我们知道不会发生页面错误，因为所有相关页面都在内存中。另一种解决方案使用临时寄存器来保存被覆盖位置的值。如果发生页面错误，则在陷阱发生之前，所有旧值都会被写回内存。这个操作将内存恢复到指令开始之前的状态，以便可以重复执行指令。 性能 需求分页可以显著影响计算机系统的性能。为了理解其中的原因，让我们计算需求分页内存的有效访问时间。对于大多数计算机系统来说，内存访问时间，表示为 ma，范围在 10 到 200 纳秒之间。只要没有页面错误，有效访问时间就等于内存访问时间。然而，如果发生页面错误，我们必须首先从磁盘读取相关页面，然后访问所需的字。 设 p 为页面错误的概率（0 ≤ p ≤ 1）。我们期望 p 接近于零，也就是说，我们期望只有很少的页面错误发生。那么有效访问时间可以表示为： 有效访问时间=(1−p)×ma+p×页面错误时间 我们面临三个页面错误服务时间的主要组成部分： 处理页面错误中断。 读取页面。 重新启动进程。 第一个和第三个任务可以通过仔细编码减少到几百条指令。这些任务每个可能需要 1 到 100 微秒。然而，页面切换时间可能接近 8 毫秒。（典型硬盘的平均延迟为 3 毫秒，寻道时间为 5 毫秒，传输时间为 0.05 毫秒。因此，总的分页时间约为 8 毫秒，包括硬件和软件时间。）还要记住，我们只考虑设备服务时间。如果一系列进程正在等待设备，我们必须将设备排队时间添加到我们等待分页设备空闲以服务我们的请求的时间中，从而增加了交换的时间。 需求分页的另一个方面是交换空间的处理和整体利用。与文件系统相比，磁盘 I/O 到交换空间通常更快。这是因为交换空间是以更大的块分配的，并且不使用文件查找和间接分配方法。因此，系统可以通过在进程启动时将整个文件映像复制到交换空间，然后从交换空间执行需求分页来获得更好的分页吞吐量。另一种选择是最初从文件系统需求页面，但在替换时将页面写入交换空间。这种方法确保只有需要的页面从文件系统中读取，而所有后续的页面都是从交换空间进行的。 移动操作系统通常不支持交换。相反，这些系统从文件系统进行需求分页，并且如果内存受限，可以从应用程序中回收只读页面（例如代码）。如果稍后需要这些数据，可以从文件系统进行需求分页。在 iOS 下，除非应用程序被终止或明确释放内存，否则匿名内存页面永远不会从应用程序中回收。 写时复制 我们说明了通过需求分页加载包含第一条指令的页面可以快速启动进程。然而，使用 fork() 系统调用进行进程创建可能最初会绕过对需求分页的需要，这是通过使用类似于页面共享的技术实现的。这种技术可以实现快速的进程创建，并将新创建进程所需的页面数最小化。 传统上，fork() 的工作方式是为子进程创建父进程的地址空间副本，复制属于父进程的页面。然而，考虑到许多子进程在创建后立即调用 exec() 系统调用，复制父进程的地址空间可能是不必要的。相反，我们可以使用一种称为写时复制（copy-on-write）的技术，该技术允许父进程和子进程最初共享相同的页面。这些共享的页面被标记为写时复制页面，这意味着如果任一进程对共享页面进行写操作，将创建共享页面的副本。 在进程 1 修改页面 C 之前 在进程 1 修改页面 C 之后。 例如，假设子进程尝试修改包含栈部分的页面，并将这些页面设置为写时复制。操作系统将创建此页面的副本，并将其映射到子进程的地址空间。然后，子进程将修改其复制的页面，而不是父进程的页面。显然，在使用写时复制技术时，只会复制被任一进程修改的页面；所有未修改的页面可以由父进程和子进程共享。此外，只有可能被修改的页面需要标记为写时复制。不能修改的页面（包含可执行代码的页面）可以由父进程和子进程共享。写时复制是许多操作系统使用的常见技术，包括 Windows XP、Linux 和 Solaris。 确定要使用写时复制来复制页面时，重要的是要注意要分配空闲页面的位置。许多操作系统为此类请求提供了一个空闲页面池。这些空闲页面通常在进程的堆栈或堆需要扩展时分配，或者在需要管理写时复制页面时分配。操作系统通常使用一种称为按需零填充（zero-fill-on-demand）的技术来分配这些页面。按需零填充页面在分配之前已经被清零，因此擦除了先前的内容。 UNIX的几个版本（包括Solaris和Linux）提供了fork()系统调用的变体——vfork()（即虚拟内存fork），其操作方式与带有写时复制的fork()不同。使用vfork()时，父进程被挂起，而子进程使用父进程的地址空间。由于vfork()不使用写时复制，因此如果子进程更改父进程地址空间的任何页面，则修改后的页面将在父进程恢复执行后可见。因此，必须谨慎使用vfork()以确保子进程不会修改父进程的地址空间。vfork()的预期用法是当子进程在创建后立即调用exec()。由于不进行页面复制，vfork()是一种非常高效的进程创建方法，有时被用于实现UNIX命令行shell界面。 页面替换 在我们之前对页面错误率的讨论中，我们假设每个页面最多只产生一次页面错误，即在首次引用时。然而，这种表述并不严格准确。如果一个包含十页的进程实际上只使用了其中一半，那么按需分页可以节省加载那些从未被使用的五页所需的I/O操作。我们也可以通过运行两倍数量的进程来增加我们的多道程序设计程度。因此，如果我们有四十个页面帧，我们可以运行八个进程，而不是如果每个进程都需要十个页面帧（其中五个从未被使用）的话只能运行四个。 如果我们增加了我们的多道程序设计程度，我们会过度分配内存。如果我们运行六个进程，每个进程的大小为十页，但实际上只使用了五页，那么我们的CPU利用率和吞吐量就会更高，而且还有十个页面帧可供使用。然而，对于特定的数据集，这些进程中的每一个可能会突然尝试使用其全部十个页面，这会导致当只有四十个页面可用时需要六十个页面帧。 此外，还需考虑到系统内存不仅用于保存程序页面。I/O 缓冲区也占用了相当大的内存空间。这种使用会增加内存放置算法的压力。决定分配多少内存给 I/O 以及多少内存给程序页面是一个重要的挑战。一些系统会为 I/O 缓冲区分配固定百分比的内存，而其他系统则允许用户进程和 I/O 子系统竞争全部系统内存。 内存过度分配表现如下。当用户进程执行时，发生页面错误。操作系统确定所需页面位于磁盘上的位置，但随后发现空闲帧列表上没有可用的空闲帧；所有内存都在使用中。 此时，操作系统有几种选择。它可以终止用户进程。但是，分页是操作系统试图提高计算机系统利用率和吞吐量的方式。用户不应意识到他们的进程正在运行在一个分页系统上——分页应该在逻辑上对用户透明。因此，这个选项不是最佳选择。 操作系统可以选择将一个进程调出，释放其所有的页框，降低多道程序设计的水平。在某些情况下，这是一个很好的选择，在这里，我们讨论最常见的解决方案：页面替换。 基础页面替换 页面替换采用如下方法。如果没有空闲页框，我们将找到一个当前未被使用的页框并释放它。我们可以通过将其内容写入交换空间并更改页表(以及其他所有表)以指示该页不再驻留在内存中来释放一个页框。我们现在可以使用释放的页框来保存进程发生错误的页面。我们修改页面错误服务例程以包括页面替换： 找到磁盘上所需页面的位置。 找到一个空闲页框： a. 如果有空闲页框，则使用它。 b. 如果没有空闲页框，则使用页面替换算法选择一个受害页框。 c. 将受害页框写入磁盘；相应地更改页面和页框表。 将所需页面读入新释放的页框；更改页面和页框表。 从页面错误发生处继续用户进程。 请注意，如果没有空闲的页框，则需要两次页面传输（一次出，一次入）。这种情况有效地将页面错误服务时间加倍，并相应地增加了有效访问时间。 我们可以通过使用修改位来减少这种开销。当使用此方案时，硬件中的每个页面或页框都与一个修改位相关联。当页面中的任何字节被写入时，硬件会设置该页面的修改位，表示该页面已被修改。当我们选择一个页面进行替换时，我们会检查其修改位。如果该位已设置，我们就知道该页面从磁盘中读取以来已被修改。在这种情况下，我们必须将页面写入磁盘。然而，如果修改位未设置，则表示该页面自从被读入内存以来尚未被修改。在这种情况下，我们不需要将内存页面写入磁盘：它已经在那里了。这种技术也适用于只读页面（例如，二进制代码的页面）。这些页面无法修改；因此，它们可以在需要时被丢弃。如果页面没有被修改，这种方案可以显著减少服务页面错误所需的时间，因为它将I/O时间减少了一半。 页面替换是需求分页的基础。它完成了逻辑内存和物理内存之间的分离。借助这种机制，可以在较小的物理内存上为程序员提供巨大的虚拟内存。如果没有需求分页，用户地址将被映射到物理地址，而这两组地址可能是不同的。然而，一个进程的所有页面仍然必须在物理内存中。通过需求分页，逻辑地址空间的大小不再受物理内存的限制。如果我们有一个由二十个页面组成的用户进程，我们可以通过简单地使用需求分页并使用替换算法在必要时找到一个空闲帧来执行它。如果要替换已修改的页面，则其内容将被复制到磁盘。对该页面的后续引用将导致页面错误。此时，页面将重新加载到内存中，可能替换进程中的其他页面。 要实现需求分页，我们必须解决两个主要问题：我们必须开发一个帧分配算法和一个页面替换算法。也就是说，如果内存中有多个进程，我们必须决定为每个进程分配多少帧；当需要进行页面替换时，我们必须选择要替换的帧。设计解决这些问题的适当算法是一项重要任务，因为磁盘 I/O 成本很高。即使是对需求分页方法的轻微改进也会带来系统性能的巨大提升。 页面替换算法 最简单的页面替换算法是先进先出（FIFO）算法。FIFO替换算法将每个页面与其被引入内存的时间关联起来。当需要替换页面时，选择最老的页面。注意，记录页面引入时间并不是严格必要的。我们可以创建一个FIFO队列来保存内存中的所有页面。我们替换队列头部的页面。当将页面引入内存时，我们将其插入到队列的尾部。 FIFO页面置换算法易于理解和编程。然而，它的性能并不总是良好的。一方面，被替换的页面可能是很久以前使用过的初始化模块，现在已经不再需要了。另一方面，它可能包含一个早期初始化并且一直被频繁使用的变量。 贝雷迪异常：对于某些页面替换算法，随着分配的页面数增加，页面错误率可能会增加。贝雷迪异常的发现导致人们寻找最优页面替换算法——即所有算法中页面错误率最低且永远不会受到贝雷迪异常影响的算法。这样的算法确实存在，并被称为 OPT 或 MIN。它就是这样的：替换最长时间不会被使用的页面。使用这种页面替换算法可以保证在固定数量的页面帧下获得最低的可能页面错误率。 不幸的是，最佳页面替换算法很难实现，因为它需要对参考字符串的未来知识。因此，最佳算法主要用于比较研究。例如，了解到尽管一个新算法不是最佳的，但在最差情况下它与最佳的差距不超过12.3%，平均差距不超过4.7%，可能是有用的。 如果最佳算法不可行，也许可以使用最佳算法的近似。这种方法就是最近最少使用（LRU）算法。LRU替换将每个页面与其上次使用的时间关联起来。当需要替换页面时，LRU选择最长时间未被使用的页面。我们可以将这种策略看作是最佳页面替换算法向后查找时间，而不是向前查找。（奇怪的是，如果我们让S^R是参考字符串S的反转，那么OPT算法在S上的页面错误率与OPT算法在S^R上的页面错误率相同。同样，LRU算法在S上的页面错误率与LRU算法在S^R上的页面错误率相同。） LRU策略通常被用作页面替换算法，并被认为是有效的。主要问题是如何实现LRU替换。LRU页面替换算法可能需要大量的硬件辅助。问题在于确定由最后使用时间定义的帧的顺序。有两种可行的实现方式： 计数器。在最简单的情况下，我们将每个页表条目与一个使用时间字段关联，并在CPU中添加一个逻辑时钟或计数器。每次内存引用时，时钟都会递增。每当对某个页面进行引用时，时钟寄存器的内容会被复制到该页面的页表条目中的使用时间字段中。通过这种方式，我们始终可以获得对每个页面的最后引用的“时间”。我们用最小的时间值替换页面。这个方案需要在页表中搜索LRU页面，并且对于每次内存访问都需要写入内存（写入页表中的使用时间字段）。当页表发生变化（由于CPU调度）时，也必须维护时间。必须考虑时钟的溢出。 栈。实现LRU替换的另一种方法是保持一个页面号的栈。每当引用一个页面时，它就会被从栈中移除并放在顶部。通过这种方式，最近使用的页面总是在栈的顶部，而最近不使用的页面总是在底部。由于条目必须从栈的中间移除，最好使用一个带有头指针和尾指针的双向链表来实现这种方法。从栈中删除一个页面并将其放在栈顶最多需要更改六个指针。每次更新的成本略高一些，但是不需要搜索替换；尾指针指向栈底，即LRU页面。这种方法特别适用于LRU替换的软件或微码实现。 请注意，如果没有标准TLB寄存器以外的硬件支持，LRU的任何实现都是不可想象的。时钟字段或堆栈的更新必须针对每个内存引用进行。如果我们为每个引用使用中断来允许软件更新这些数据结构，那么每个内存引用的速度将至少减慢十倍，因此每个用户进程的速度也将减慢十倍。很少有系统能够容忍这种级别的内存管理开销。 除了特定的页面替换算法外，通常还会使用其他程序。例如，系统通常保持一组空闲页面帧池。当发生页面错误时，会像以前一样选择一个牺牲页面帧。但是，在将牺牲页面写出之前，会将所需的页面从空闲页面帧池中读入一个空闲页面帧中。这个过程允许进程尽快重新启动，而无需等待牺牲页面被写出。当牺牲页面稍后被写出时，其页面帧将被添加到空闲页面帧池中。 这个想法的扩展是维护一个已修改页面的列表。每当分页设备空闲时，就会选择一个已修改的页面，并将其写入磁盘。然后将其修改位重置。这个方案增加了选择替换页面时页面干净的可能性，并且不需要被写出。 分配帧 接下来我们转向分配的问题。我们如何将固定数量的空闲内存分配给各个进程？如果我们有93个空闲帧和两个进程，每个进程会得到多少个帧？ 最简单的情况是单用户系统。考虑一个单用户系统，具有128 KB内存，由1 KB大小的页面组成。该系统有128个帧。操作系统可能占用35 KB，留下93个帧供用户进程使用。在纯需求分页下，所有93个帧最初都会被放在空闲帧列表上。当用户进程开始执行时，它会生成一系列页面错误。前93个页面错误都会从空闲帧列表中获取空闲帧。当空闲帧列表用尽时，会使用页面替换算法来选择将内存中的93个页面之一替换为第94个页面，以此类推。当进程终止时，这93个帧将再次被放置在空闲帧列表上。 这种简单策略有许多变体。我们可以要求操作系统从空闲帧列表中分配所有的缓冲区和表空间。当操作系统不使用这些空间时，它可以用于支持用户分页。我们可以尝试始终在空闲帧列表中保留三个空闲帧。因此，当发生页面错误时，可以使用一个空闲帧进行分页。在进行页面交换时，可以选择替换页面，然后将其写入磁盘，而用户进程继续执行。还有其他变体也是可能的，但基本策略是清楚的：为用户进程分配任何空闲帧。 最小帧数 最少分配最低数量的帧的一个原因涉及性能。显然，随着分配给每个进程的帧数减少，页面错误率会增加，从而减慢进程执行速度。此外，在执行指令尚未完成时发生页面错误时，必须重新启动该指令。因此，我们必须有足够的帧来容纳任何单个指令可能引用的所有不同页面。 例如，考虑一台机器，其中所有的内存引用指令可能只引用一个内存地址。在这种情况下，我们至少需要一个帧用于指令，一个帧用于内存引用。此外，如果允许一级间接寻址（例如，对于在页面16上的加载指令可以引用页面0上的地址，这是对页面23的间接引用），那么分页需要每个进程至少三个帧。想象一下如果一个进程只有两个帧会发生什么。 最小帧数由计算机体系结构定义。例如，PDP-11的移动指令对于某些寻址模式包含多个字，因此指令本身可能横跨两个页面。此外，它的两个操作数可能是间接引用，总共需要六个帧。另一个例子是IBM 370的MVC指令。由于该指令是从存储位置到存储位置，它占据6个字节，可能横跨两个页面。要移动的字符块和要移动到的区域也可能各自横跨两个页面。这种情况需要六个帧。当MVC指令是EXECUTE指令的操作数，而EXECUTE指令横跨页面边界时，情况最糟，这种情况下需要八个帧。 最坏情况发生在允许多级间接寻址的计算机体系结构中（例如，每个16位字可以包含一个15位地址加上一个1位间接指示器）。理论上，一个简单的加载指令可以引用一个间接地址，该地址可以引用另一个页面上的间接地址，依此类推，直到虚拟内存中的每个页面都被访问。因此，在最坏的情况下，整个虚拟内存必须存在于物理内存中。为了克服这个困难，我们必须对间接级别设置限制（例如，将指令的间接级别限制在最多16级）。当第一次间接引用发生时，一个计数器被设置为16；对于该指令的每个后续间接引用，计数器都会递减。如果计数器递减到0，则会触发陷阱。这种限制将指令中每条的最大内存引用次数减少到17，需要相同数量的帧。 虽然每个进程的最小帧数由体系结构定义，但最大帧数由可用物理内存量定义。在这之间，我们仍然有很大的选择余地来进行帧分配。 分配算法 最简单的将 m 个帧分配给 n 个进程的方法是给每个进程平均分配 m/n 个帧（暂时忽略操作系统所需的帧）。例如，如果有 93 个帧和五个进程，则每个进程将获得 18 个帧。剩下的三个帧可以用作自由帧缓冲池。这种方案称为等量分配equal allocation。 另一种选择是意识到各个进程将需要不同数量的内存。考虑一个帧大小为 1 KB 的系统。如果一个小型学生进程占用 10 KB，而一个交互式数据库占用 127 KB，在一个有 62 个空闲帧的系统中，给每个进程分配 31 个帧并不合理。 为了解决这个问题，我们可以使用比例分配proportional allocation，根据进程的大小分配可用内存。设进程 pi 的虚拟内存大小为 si，定义 S = Σ si。 然后，如果总可用帧数为 m，则将 ai 个帧分配给进程 pi，其中 ai 大约为 ai = si/S × m。当然，我们必须将每个 ai 调整为一个整数，该整数大于指令集所需的最小帧数，并且总和不超过 m。 在等量分配和比例分配中，分配当然可能会根据多道程序设计水平而变化。如果增加了多道程序设计水平，每个进程都会失去一些帧以提供新进程所需的内存。相反，如果多道程序设计水平降低，分配给已离开的进程的帧可以分配给其余的进程。 请注意，无论是等量分配还是比例分配，高优先级进程都会和低优先级进程被同等对待。然而，根据定义，我们可能希望给予高优先级进程更多的内存以加速其执行，而对低优先级进程产生不利影响。一种解决方案是使用比例分配方案，其中帧的比例取决于进程的优先级而不是相对大小，或者取决于大小和优先级的组合。 全局分配 vs 本地分配 **另一个影响帧分配给各个进程方式的重要因素是页面置换。**在多个进程竞争帧的情况下，我们可以将页面置换算法分类为两类：全局置换和局部置换。全局置换允许一个进程从所有帧的集合中选择一个替换帧，即使该帧当前已分配给另一个进程。局部置换要求每个进程仅从自己分配的帧集合中进行选择。 例如，考虑一种分配方案，其中我们允许高优先级进程从低优先级进程中选择帧进行替换。一个进程可以从自己的帧或任何低优先级进程的帧中选择替换。这种方法允许高优先级进程以牺牲低优先级进程为代价增加其帧分配量。使用局部置换策略，分配给一个进程的帧数不会改变。而使用全局置换时，一个进程可能只会选择已分配给其他进程的帧，从而增加其分配给它的帧数（假设其他进程不选择它的帧进行替换）。 全局替换算法的一个问题是，一个进程无法控制自己的页面错误率。一个进程在内存中的页面集合不仅取决于该进程的页面行为，还取决于其他进程的页面行为。因此，同一个进程可能会因为完全外部的情况而表现出不同的性能（例如，一次执行需要0.5秒，而下一次执行需要10.3秒）。而使用局部替换算法则不会出现这种情况。在局部替换下，一个进程在内存中的页面集合仅受该进程的页面行为影响。然而，局部替换可能会通过不向进程提供其他不常用的页面来妨碍进程。因此，全局替换通常会导致更高的系统吞吐量，因此更常用。 Non-Uniform Memory Access(NUMA) 到目前为止，在我们对虚拟内存的覆盖范围中，我们假设所有的主存储器都是相等的——或者至少是平等地访问的。然而，在许多计算机系统中，并非如此。通常情况下，在具有多个CPU的系统中，给定的CPU可以比其他部分更快地访问某些主存储器部分。这些性能差异是由CPU和存储器在系统中的连接方式引起的。通常情况下，这样的系统由多个系统板组成，每个系统板包含多个CPU和一些内存。这些系统板之间的连接方式各不相同，从系统总线到高速网络连接都有。位于特定系统板上的CPU可以比在系统中其他板上的内存访问内存时的延迟要小。内存访问时间差异明显的系统统称为非一致性存储访问（NUMA）系统，毫无例外，它们比内存和CPU位于同一主板上的系统更慢。 在NUMA系统中，管理哪些页面框架存储在哪些位置可以显着影响性能。如果我们在这样的系统中将内存视为均匀的，那么与修改内存分配算法以考虑NUMA的情况相比，CPU可能会等待更长时间以访问内存。调度系统必须进行类似的更改。这些更改的目标是使内存框架“尽可能接近”于运行进程的CPU。 “接近”的定义是“具有最小的延迟”，通常意味着与CPU位于同一系统板上。 算法上的更改包括使调度程序跟踪每个进程上次运行的最后一个CPU。如果调度程序尝试将每个进程调度到其先前的CPU，并且内存管理系统尝试将页面框架分配给接近所调度的CPU的进程，那么将会产生改善的缓存命中率和减少的内存访问时间。 抖动 如果分配给低优先级进程的页面数低于计算机体系结构所需的最小数量，那么我们必须暂停该进程的执行。然后，我们应该将其余页面换出，释放所有已分配的页面。这一规定引入了一种中间CPU调度级别的换入换出。 实际上，看任何没有“足够”页面的进程。如果该进程没有足够的页面来支持活跃使用的页面，它将很快发生页面错误。此时，它必须替换某些页面。然而，由于它的所有页面都在活跃使用中，它必须立即替换一个将再次需要的页面。因此，它很快又发生了故障，一次又一次地替换页面。 这种高频繁的页面交换活动称为抖动tharshing。如果一个进程花费的时间用于页面交换比执行还要多，那么这个进程就是在抖动。 抖动原因 抖动会导致严重的性能问题。考虑以下场景，这是基于早期分页系统的实际行为。 操作系统监视CPU利用率。如果CPU利用率过低，我们会通过引入一个新进程来增加多道程序设计的程度。使用全局页面替换算法；它替换页面时不考虑它们所属的进程。现在假设一个进程进入了执行的新阶段并需要更多的帧。它开始发生故障并从其他进程那里取走帧。然而，这些进程需要这些页面，因此它们也发生故障，并从其他进程那里取走帧。这些发生故障的进程必须使用分页设备进行页面交换。当它们排队等待分页设备时，就绪队列就会清空。当进程等待分页设备时，CPU利用率下降。 CPU调度程序看到CPU利用率下降，因此增加了多道程序设计的程度。新进程试图通过从运行中的进程中获取帧来启动，导致更多的页面故障和更长的分页设备队列。结果，CPU利用率进一步下降，CPU调度程序试图进一步增加多道程序设计的程度。发生了抖动，系统吞吐量急剧下降。页面故障率大大增加。因此，有效的内存访问时间增加。没有进行任何工作，因为进程花费了所有时间来进行分页。 随着多道程序设计的程度增加，CPU利用率也会增加，尽管增长速度变慢，直到达到最大值。如果进一步增加多道程序设计的程度，抖动就会开始，并且CPU利用率会急剧下降。在这一点上，为了增加CPU利用率并停止抖动，我们必须减少多道程序设计的程度。 我们可以通过使用本地替换算法（或优先级替换算法）来限制抖动的影响。使用本地替换算法时，如果一个进程开始抖动，它就无法从另一个进程那里窃取帧并导致后者也抖动。然而，问题并没有完全解决。如果进程在抖动，它们大部分时间都会排队等待分页设备。由于分页设备的平均队列时间更长，页面错误的平均服务时间会增加。因此，即使是一个没有抖动的进程，其有效访问时间也会增加。 为了防止抖动，我们必须为一个进程提供它所需的尽可能多的帧。但是我们如何知道它需要多少帧呢？有几种技术可以做到这一点。工作集策略首先查看一个进程实际使用了多少帧。该方法定义了进程执行的局部性模型。局部性模型表明，当一个进程执行时，它会从一个局部性移动到另一个。局部性是一组一起被活跃使用的页面。一个程序通常由几个不同的局部性组成，这些局部性locality可能会重叠。 例如，当调用一个函数时，它定义了一个新的局部性。在这个局部性中，会对函数调用的指令、其本地变量以及全局变量的子集进行内存引用。当我们退出函数时，进程离开了这个局部性，因为函数的局部变量和指令不再活跃使用。我们可能会稍后返回到这个局部性。 因此，我们可以看到，局部性是由程序结构及其数据结构定义的。局部性模型表明，所有的程序都会展现出这种基本的内存引用结构。请注意，局部性模型是本书迄今讨论缓存的未明确说明的原则。如果对任何类型的数据的访问是随机的而不是有模式的，那么缓存将毫无用处。假设我们为一个进程分配了足够多的帧来容纳其当前的局部性。它将因局部性中的页面而出现页面错误，直到所有这些页面都在内存中；然后，在它改变局部性之前，它将不会再出现页面错误。如果我们没有分配足够多的帧来容纳当前局部性的大小，那么进程将出现抖动，因为它无法将所有正在活跃使用的页面保留在内存中。 工作集模型 如前所述，工作集模型基于局部性的假设。该模型使用一个参数，表示工作集窗口。其思想是检查最近的个页面引用。最近的个页面引用中的页面集合是工作集。如果一个页面正在活跃使用，则它将位于工作集中。如果不再使用，则在其上一次引用后经过个时间单位后，它将从工作集中删除。因此，工作集是程序局部性的一个近似值。例如，下图中显示的内存引用序列，如果个内存引用，则在时间 t1 时的工作集为 {1, 2, 5, 6, 7}。到时间 t2 时，工作集已变为 {3, 4}。工作集的准确性取决于的选择。如果太小，它将无法涵盖整个局部性；如果太大，它可能会重叠几个局部性。在极端情况下，如果为无穷大，则工作集是在进程执行期间访问的页面集合。 因此，工作集最重要的属性是其大小。如果我们为系统中的每个进程计算工作集大小 WSSi，那么我们可以考虑到 D=∑WSSi, 其中 D 是对页面帧的总需求。每个进程都在活跃使用其工作集中的页面。因此，进程 i 需要 WSSi个页面帧。如果总需求大于可用页面帧的总数（D&gt;m），则会发生抖动，因为某些进程将没有足够的页面帧。 一旦选择了 ω，使用工作集模型就很简单了。操作系统监视每个进程的工作集，并为该工作集分配足够的页面帧，以满足其工作集大小。如果有足够多的额外页面帧，则可以启动另一个进程。如果工作集大小之和增加，超过了可用页面帧的总数，则操作系统会选择一个进程进行挂起。该进程的页面将被写出（交换），其页面帧将被重新分配给其他进程。挂起的进程可以稍后重新启动。 这种工作集策略可以防止抖动，同时保持尽可能高的多道程序设计程度。因此，它优化了 CPU 利用率。工作集模型的困难在于跟踪工作集。工作集窗口是一个移动窗口。在每个内存引用时，一个新的引用出现在其中一个端点，最老的引用从另一个端点消失。如果一个页面在工作集窗口中的任何位置被引用，则它就在工作集中。 我们可以用一个固定间隔的定时器中断和一个引用位来近似工作集模型。例如，假设 ▲等于 10,000 次引用，并且我们可以在每 5,000 次引用时触发一个定时器中断。当我们收到一个定时器中断时，我们会复制并清除每个页面的引用位值。因此，如果发生页面错误，我们可以检查当前的引用位和内存中的两个位，以确定一个页面是否在最近的 10,000 到 15,000 次引用中被使用过。如果它被使用过，这些位中至少有一个会被打开。如果它没有被使用过，这些位会关闭。至少有一个位打开的页面将被视为在工作集中。 需要注意的是，这种安排并不完全准确，因为我们无法确定在 5,000 次引用内引用发生的位置。我们可以通过增加历史位和中断频率（例如，每 1,000 次引用一个中断和 10 位）来减少不确定性。然而，为了处理这些更频繁的中断，服务的成本也将相应增加。 页面错误频率 工作集模型取得了成功，了解工作集对于预取页面也可能非常有用，但它似乎是一种笨拙的控制抖动的方法。使用页面错误频率（PFF）的策略采取了更直接的方法。 抖动会导致页面错误率高。因此，我们希望控制页面错误率。当页面错误率过高时，我们知道该进程需要更多帧。相反，如果页面错误率过低，则进程可能拥有太多帧。我们可以为所需的页面错误率建立上下限。如果实际页面错误率超过上限，我们为进程分配另一个帧。如果页面错误率低于下限，则从进程中删除一个帧。因此，我们可以直接测量和控制页面错误率以防止抖动。 与工作集策略类似，我们可能需要交换出一个进程。如果页面错误率增加且没有空闲帧可用，我们必须选择某些进程并将其交换到后备存储。然后，释放的帧将分配给页面错误率高的进程。 实际上，磁盘交换引起的抖动对性能影响很大。在实施计算机设施时，当前的最佳实践是尽可能提供足够的物理内存，以避免抖动和交换。从智能手机到大型机，除非在极端情况下，提供足够的内存以同时保持所有工作集在内存中，可以为用户提供最佳的使用体验。 Memory-Mapped Files 考虑使用标准系统调用 open()、read() 和 write() 对磁盘上的文件进行顺序读取。每个文件访问都需要一个系统调用和磁盘访问。另外，我们可以利用到目前为止讨论的虚拟内存技术，将文件 I/O 视为常规的内存访问。这种方法被称为内存映射文件，它允许将虚拟地址空间的一部分与文件逻辑关联起来。正如我们将要看到的，这可以显著提高性能。 基本机制 内存映射文件通过将磁盘块映射到内存中的一个页面（或多个页面）来实现。对文件的初始访问通过普通的需求分页进行，导致页面错误。然而，文件的一页大小的部分被从文件系统读入到物理页面（一些系统可能选择一次读入多于一页大小的内存块）。对文件的后续读写操作被处理为常规的内存访问。通过内存而不是使用 read() 和 write() 系统调用的开销来操作文件，简化了文件访问和使用，并提高了速度。 需要注意的是，对内存中映射的文件的写入不一定是立即的（同步的）写入到磁盘上的文件。一些系统可能选择在操作系统定期检查内存中的页面是否已修改时更新物理文件。当文件关闭时，所有内存映射的数据都被写回磁盘，并从进程的虚拟内存中移除。 多个进程可以同时映射同一个文件，以实现数据的共享。任何一个进程的写操作都会修改虚拟内存中的数据，并且可以被所有映射了同一文件段的其他进程看到。通过我们之前对虚拟内存的讨论，应该清楚内存映射段的共享是如何实现的：每个共享进程的虚拟内存映射指向同一物理内存页面，即保存着磁盘块副本的页面。内存映射系统调用还可以支持写时复制功能，允许进程以只读模式共享文件，但在修改任何数据时拥有自己的副本。为了协调对共享数据的访问，涉及的进程可能会使用互斥机制。 通常情况下，共享内存实际上是通过内存映射文件实现的。在这种情况下，进程可以通过将通信进程将同一个文件映射到它们的虚拟地址空间来进行通信。内存映射文件充当了通信进程之间的共享内存区域。 ​ Memory-Mapped I/O 在I/O方面，每个I/O控制器都包括用于保存命令和正在传输的数据的寄存器。通常，特殊的I/O指令允许在这些寄存器和系统内存之间进行数据传输。为了更方便地访问I/O设备，许多计算机体系结构提供了内存映射I/O。在这种情况下，一段内存地址范围被保留，并映射到设备寄存器。对这些内存地址进行读写会导致数据与设备寄存器之间的传输。这种方法适用于具有快速响应时间的设备，例如视频控制器。在IBM PC上，屏幕上的每个位置都映射到一个内存位置。在屏幕上显示文本几乎和将文本写入适当的内存映射位置一样简单。 内存映射I/O对于连接调制解调器和打印机等其他设备的串行和并行端口也很方便。CPU通过读写几个设备寄存器（称为I/O端口）来通过这些设备传输数据。要通过内存映射的串行端口发送一长串字节，CPU将一个数据字节写入数据寄存器，并设置控制寄存器中的一个位来表示该字节可用。设备接收数据字节，然后清除控制寄存器中的位，以表示它已准备好接收下一个字节。然后CPU可以传输下一个字节。如果CPU使用轮询来监视控制位，不断地循环以查看设备是否准备就绪，这种操作方法称为programmed I/O（PIO）。如果CPU不轮询控制位，而是在设备准备好接收下一个字节时接收到中断，则数据传输被称为中断驱动interrupt driven。 分配内核内存 当运行在用户模式下的进程请求额外的内存时，页面将从内核维护的空闲页面帧列表中分配。这个列表通常是通过页面替换算法来填充的，并且很可能包含分散在物理内存中的空闲页面。此外，如果用户进程请求一个字节的内存，将会产生内部碎片，因为进程将被授予整个页面帧。 内核内存通常从一个与用于满足普通用户模式进程的列表不同的空闲内存池中分配。这主要有两个原因： 内核请求的内存用于各种大小的数据结构，其中一些大小小于一页。因此，内核必须谨慎使用内存，并尽量减少由于碎片化而造成的浪费。这一点尤为重要，因为许多操作系统不会将内核代码或数据置于分页系统中。 分配给用户模式进程的页面不一定需要在连续的物理内存中。然而，某些硬件设备直接与物理内存交互——没有虚拟内存接口的好处——因此可能需要驻留在物理连续页面中的内存。 Buddy system 伙伴系统从一个由物理上连续的页面组成的固定大小段中分配内存。内存是使用一个以2的幂为单位的分配器从这个段中分配的，该分配器按2的幂大小的单位满足请求（例如4 KB、8 KB、16 KB等）。以不合适大小的单位发出的请求会向上舍入到最接近的2的幂。例如，对于11 KB的请求，会使用一个16 KB的段来满足。 让我们考虑一个简单的例子。假设内存段的大小最初为256 KB，内核请求21 KB的内存。该段最初被划分为两个伙伴——我们将其称为AL和AR，每个128 KB大小。其中一个伙伴进一步划分为两个64 KB的伙伴——BL和BR。然而，从21 KB到最接近的2的幂是32 KB，因此BL或BR中的一个再次被划分为两个32 KB的伙伴，CL和CR。其中一个伙伴用于满足21 KB的请求，其中CL是分配给21 KB请求的段。 伙伴系统的一个优点是，使用一种称为合并的技术，可以很快地将相邻的伙伴组合成更大的段。例如，当内核释放其分配的CL单元时，系统可以将CL和CR合并成一个64 KB的段。这个段BL，可以进一步与其伙伴BR合并，形成一个128 KB的段。最终，我们可以得到原始的256 KB段。 伙伴系统的明显缺点是，将大小向上舍入到下一个最高的2的幂很可能会导致分配的段内部发生碎片化。例如，一个33 KB的请求只能用一个64 KB的段来满足。事实上，我们无法保证分配的单元少于50%的空间会因为内部碎片而浪费掉。在接下来的部分中，我们将探讨一种内存分配方案，其中没有因碎片化而导致的空间浪费。 slab allocation 第二种内核内存分配策略称为“slab分配”。一个slab由一个或多个物理连续的页面组成。一个cache包含一个或多个slab。对于每个唯一的内核数据结构，都有一个单独的cache，例如，一个用于表示进程描述符的数据结构的单独cache，一个用于文件对象的单独cache，一个用于信号量的单独cache，依此类推。每个cache都填充有实例化为该cache表示的内核数据结构的对象。例如，表示信号量的cache存储信号量对象的实例，表示进程描述符的cache存储进程描述符对象的实例，依此类推。slab、cache和对象之间的关系如下图所示。图中显示了两个大小为3 KB的内核对象和三个大小为7 KB的对象，每个对象都存储在单独的cache中。 slab分配算法使用cache来存储内核对象。当创建一个cache时，一些对象被分配给该cache，并且最初被标记为free。cache中的对象数量取决于关联slab的大小。例如，一个12KB的slab（由三个连续的4KB页面组成）可以存储六个2KB的对象。最初，cache中的所有对象都被标记为自由。当需要一个新的内核数据结构对象时，分配器可以分配cache中的任何自由对象来满足请求。从cache分配的对象被标记为已使用。 让我们考虑这样一个场景：内核向slab分配器请求内存，以获取表示进程描述符的对象。在Linux系统中，进程描述符的类型是struct task_struct，大约需要1.7 KB的内存。当Linux内核创建一个新任务时，它会从其cache中请求所需内存以获取struct task_struct对象。cache将使用一个已经在slab中分配并标记为自由的struct task_struct对象来满足请求。 在Linux中，一个slab可以处于以下三种可能的状态之一： Full。slab中的所有对象都被标记为已使用。 Empty。slab中的所有对象都被标记为free。 Partial。slab包含已使用和free对象。 slab分配器首先尝试从部分slab中的空闲对象中满足请求。如果不存在空闲对象，则从空slab中分配一个空闲对象。如果没有可用的空slab，则从连续的物理页面中分配一个新的slab，并分配给一个cache；对象的内存从这个slab中分配。 slab分配器提供了两个主要好处： 由于每个唯一的内核数据结构都有一个关联的缓存，而每个缓存由一个或多个slab组成，这些slab被划分为与所表示对象大小相同的块，因此不存在由于碎片化而浪费内存的问题。因此，当内核请求内存以表示对象时，slab分配器返回所需内存的确切量。 内存请求可以快速满足。slab分配方案在管理内存时特别有效，因为内核经常会请求分配和释放对象，这是常见的情况。分配和释放内存的行为可能是耗时的过程。然而，对象是提前创建的，因此可以从缓存中快速分配。此外，当内核使用完一个对象并释放它时，它会被标记为自由，并返回到其缓存中，因此可以立即供内核的后续请求使用。 从Linux内核2.6.24版本开始，SLUB分配器取代SLAB成为默认的分配器。SLUB通过减少SLAB分配器所需的大部分开销来解决slab分配的性能问题。一个改变是将在SLAB分配下与每个slab存储的元数据移动到Linux内核用于每个页面的页面结构中。此外，SLUB移除了SLAB分配器为每个缓存中的对象维护的 per CPU队列。对于拥有大量处理器的系统来说，分配给这些队列的内存量是相当可观的。因此，随着系统中处理器数量的增加，SLUB提供了更好的性能。 其他考量 prepaging 纯需求分页的一个明显特点是在启动进程时会发生大量的页面错误。这种情况是由于尝试将初始局部性装入内存而导致的。同样的情况可能在其他时候也会出现。例如，当重新启动一个被交换出的进程时，它的所有页面都在磁盘上，每个页面都必须通过自己的页面错误来带入内存。预取页是为了防止这种高水平的初始分页而采取的一种尝试。该策略是一次性将所有需要的页面带入内存。一些操作系统，特别是Solaris，会为小文件的页面帧进行预分页。 在使用工作集模型的系统中，例如，我们可以为每个进程保留一个其工作集中页面的列表。如果我们必须暂停一个进程（由于I/O等待或缺少空闲页面），我们会记住该进程的工作集。当进程要被恢复时（因为I/O已经完成或者足够的空闲页面已经可用），我们会在重新启动进程之前自动将其整个工作集带回内存。 预取页在某些情况下可能会带来优势。问题只是使用预取页的成本是否比处理相应的页面错误的成本低。很可能许多由预取页带回内存的页面将不会被使用。 假设预取了 s 个页面，并且其中的一部分 实际上被使用了（0 ≤ ≤ 1）。问题是 s* a个节省的页面错误的成本是否大于或小于预取 s*（1− a ）个不必要的页面的成本。如果 a 接近于 0，那么预取失败；如果a 接近于 1，那么预取成功。 Page Size 现有机器的操作系统设计者很少有选择页面大小的机会。然而，当设计新的机器时，必须对最佳页面大小做出决定。正如你所预料的那样，没有单一的最佳页面大小。相反，有一系列因素支持各种大小。页面大小通常是2的幂，通常范围从4,096（2^12）到4,194,304（2^22）字节。 我们如何选择页面大小？一个考虑因素是页表的大小。对于给定的虚拟内存空间，减小页面大小会增加页面数量，因此增加了页表的大小。例如，对于一个4 MB（2^22）的虚拟内存，将有4,096个1,024字节大小的页面，但只有512个8,192字节大小的页面。因为每个活动进程都必须有自己的页表副本，所以较大的页面大小是可取的。 然而，使用较小的页面可以更好地利用内存。如果一个进程从位置00000开始分配内存，并持续分配直到满足其需要，那么它可能不会正好结束在一个页面边界上。因此，最后一页的一部分必须被分配（因为页面是分配的单位），但将不会被使用（造成内部碎片）。假设进程大小和页面大小是独立的，我们可以预期，每个进程的最后一页平均将浪费一半。对于一个512字节的页面，这种损失只有256字节，但对于一个8,192字节的页面，这种损失就是4,096字节。为了最小化内部碎片，我们需要一个较小的页面大小。 另一个问题是读取或写入一个页面所需的时间。I/O 时间由搜索、延迟和传输时间组成。传输时间与传输的量成正比（即页面大小）——这一事实似乎支持使用较小的页面大小。然而，延迟时间和搜索时间通常会远远超过传输时间。以每秒2 MB的传输速率为例，传输512字节只需要0.2毫秒。然而，延迟时间可能是8毫秒，搜索时间是20毫秒。因此，总的 I/O 时间（28.2毫秒）中，只有1%归因于实际的传输。将页面大小加倍只会将 I/O 时间增加到28.4毫秒。读取1024字节大小的单个页面需要28.4毫秒，但读取两个页面，每个页面大小为512字节，需要56.4毫秒。因此，希望最小化 I/O 时间支持使用较大的页面大小。 然而，较小的页面大小应该会减少总的 I/O，因为局部性将会得到改善。较小的页面大小使每个页面更准确地与程序的局部性匹配。例如，考虑一个大小为200 KB的进程，其中只有一半（100 KB）在执行中实际使用。如果我们只有一个大页面，我们必须将整个页面带入内存，总共传输和分配200 KB。相反，如果我们只有1字节大小的页面，那么我们只需要带入实际使用的100 KB，结果只需要传输和分配100 KB。因此，较小的页面大小使我们有更好的分辨率，允许我们隔离出实际需要的内存。而较大的页面大小，我们不仅需要分配和传输所需的内容，还需要分配和传输页面中的任何其他内容，无论是否需要。因此，较小的页面大小应该会导致更少的 I/O 和更少的总分配内存。 但你是否注意到，如果页面大小为1字节，我们将为每个字节生成一个页面错误？一个只使用了一半内存的200 KB进程在页面大小为200 KB时只会生成一个页面错误，但在页面大小为1字节时会生成102,400个页面错误。每个页面错误都会产生大量的开销，需要处理中断、保存寄存器、替换页面、排队等待页面设备以及更新表格。为了最小化页面错误的数量，我们需要有较大的页面大小。 还必须考虑其他因素（例如页面大小与分页设备上扇区大小之间的关系）。这个问题没有最佳答案。正如我们所见，一些因素（内部碎片、局部性）支持较小的页面大小，而另一些因素（表格大小、I/O 时间）支持较大的页面大小。然而，历史趋势是朝着更大的页面大小发展，即使是对于移动系统也是如此。 TLB Reach TLB的命中率指的是在TLB而不是页面表中解析的虚拟地址转换的百分比。显然，命中率与TLB中的条目数相关，提高命中率的方法是增加TLB中的条目数。然而，这并不是廉价的，因为用于构建TLB的关联内存既昂贵又耗电量大。 与命中率相关的是一个类似的指标：TLB覆盖范围（TLB reach）。TLB覆盖范围指的是从TLB可访问的内存量，简单地说，就是条目数乘以页面大小。理想情况下，进程的工作集应存储在TLB中。如果没有存储在其中，进程将花费大量时间在页面表而不是TLB中解析内存引用。如果我们将TLB中的条目数量加倍，那么TLB的覆盖范围也会加倍。然而，对于一些内存密集型应用程序而言，这可能仍然不足以存储工作集。 增加TLB覆盖范围的另一种方法是增加页面大小或提供多种页面大小。如果我们增加页面大小——比如，从8 KB增加到32 KB——我们将使TLB覆盖范围增加四倍。然而，这可能会导致一些不需要如此大页面大小的应用程序出现碎片化。或者，操作系统可以提供多种不同的页面大小。例如，UltraSPARC支持8 KB、64 KB、512 KB和4 MB的页面大小。在这些可用的页面大小中，Solaris使用8 KB和4 MB页面大小。并且通过64个条目的TLB，Solaris的TLB覆盖范围从8 KB页面的512 KB到4 MB页面的256 MB不等。对于大多数应用程序来说，8 KB页面大小是足够的，尽管Solaris使用两个4 MB页面映射了内核代码和数据的前4 MB。Solaris还允许应用程序（如数据库）利用大的4 MB页面大小。 **提供支持多种页面大小需要操作系统而不是硬件来管理TLB。**例如，TLB条目中的一个字段必须指示与TLB条目对应的页面帧的大小。在软件中管理TLB而不是硬件会带来性能成本。然而，增加的命中率和TLB覆盖范围可以抵消性能成本。事实上，最近的趋势表明，向软件管理的TLB和操作系统支持多种页面大小的方向发展。 倒排列表 这种页面管理形式的目的是减少跟踪虚拟地址到物理地址转换所需的物理内存量。我们通过创建一个表来实现这种节省，该表每页物理内存有一个条目，通过 &lt;进程ID，页号&gt; 这一对索引。 因为倒排页表保存了每个物理帧中存储了哪个虚拟内存页面的信息，所以它们减少了存储这些信息所需的物理内存量。然而，倒排页表不再包含关于进程的逻辑地址空间的完整信息，而这些信息在引用的页面当前不在内存中时是必需的。需求分页需要这些信息来处理页面错误。为了使信息可用，必须保留一个外部页表（每个进程一个）。每个这样的表看起来像传统的每个进程页表，并包含每个虚拟页面的位置信息。 但是外部页表是否抵消了倒排页表的效用呢？由于这些表只在发生页面错误时才被引用，它们不需要快速可用。相反，它们根据需要自身被分页进入和退出内存。不幸的是，一个页面错误现在可能会导致虚拟内存管理器生成另一个页面错误，因为它正在将其需要的外部页表分页到内存中，以定位在后备存储上的虚拟页面。这种特殊情况需要在内核中进行仔细处理，并延迟页面查找处理。 程序结构 需求分页设计为对用户程序透明。在许多情况下，用户完全不知道内存的分页性质。然而，在其他情况下，如果用户（或编译器）了解底层的需求分页机制，系统性能可以得到改善。 让我们看一个刻意构造但信息丰富的例子。假设页面大小为128个字。考虑一个C程序，其功能是将一个128乘以128的数组的每个元素初始化为0。以下代码是典型的： 123456789int i, j;int[128]][128] data;for (j = 0; j &lt; 128; j++) for (i = 0; i &lt; 128; i++) data[i][j] = 0; 请注意，数组是按行主序存储的；也就是说，数组按顺序存储为 1data[0][0]，data[0][1]，···，data[0][127]，data[1][0]，data[1][1]，···，data[127][127] 对于每个128个字的页面，每行占用一个页面。因此，前面的代码会在每个页面中将一个字清零，然后在每个页面中将另一个字清零，依此类推。如果操作系统为整个程序分配的帧数少于128帧，则其执行将导致128 × 128 = 16,384个页面错误。相比之下，假设我们将代码更改为 123456789int i, j;int[128][128] data;for (i = 0; i &lt; 128; i++) for (j = 0; j &lt; 128; j++) data[i][j] = 0; 这段代码在开始下一页之前会将一页上的所有字清零，将页面错误的数量减少到了128个。 精心选择数据结构和编程结构可以增加局部性，从而降低页面错误率和工作集中的页面数量。例如，栈具有良好的局部性，因为访问总是发生在顶部。相比之下，哈希表设计为分散引用，产生了较差的局部性。当然，引用的局部性只是衡量数据结构使用效率的一个指标。其他具有较高权重的因素包括搜索速度、总内存引用数以及触及的总页面数。 在后期阶段，编译器和加载器对分页可能会产生重大影响。将代码和数据分离，并生成可重入代码意味着代码页面可以是只读的，因此永远不会被修改。干净的页面不必被分页出去以进行替换。加载器可以避免将程序置于页面边界之间，使每个程序完全位于一个页面中。多次调用彼此的程序可以被打包到同一个页面中。这种打包是运筹学中操作的一种变体：尝试将可变大小的加载段打包到固定大小的页面中，以最小化页面间的引用。这种方法对于大页面大小特别有用。 I/O互锁和页面锁定 在使用需求分页时，有时我们需要允许部分页面在内存中被锁定。这种情况之一发生在对用户（虚拟）内存进行I/O 时。I/O 通常由单独的I/O 处理器实现。例如，USB存储设备的控制器通常会给出要传输的字节数以及缓冲区的内存地址。当传输完成时，CPU会被中断。 我们必须确保不会发生以下事件序列：一个进程发出一个I/O 请求，并被放入该I/O 设备的队列中。与此同时，CPU 被分配给其他进程。这些进程引起页面错误，其中一个进程使用全局替换算法替换了包含等待进程的内存缓冲区的页面。页面被分页出去。一段时间后，当I/O 请求前进到设备队列的头部时，I/O 发生在指定的地址上。然而，这个帧现在正在被另一个进程的不同页面使用。 有两种常见的解决方案来解决这个问题。一种解决方案是永远不要对用户内存执行I/O 操作。相反，数据总是在系统内存和用户内存之间进行复制。I/O 只在系统内存和I/O 设备之间进行。要在磁带上写一个块，我们首先将块复制到系统内存，然后再写入磁带。这种额外的复制可能会导致不可接受的高开销。 另一种解决方案是允许页面被锁定到内存中。在这里，与每个帧相关联的是一个锁定位。如果帧被锁定，它就不能被选择进行替换。在这种方法下，要在磁带上写一个块，我们会将包含该块的页面锁定到内存中。系统然后可以继续正常进行。被锁定的页面不能被替换。当I/O 完成后，这些页面将被解锁。 锁定位在各种情况下都被使用。通常，操作系统内核被锁定到内存中。许多操作系统不能容忍由内核或特定内核模块（包括执行内存管理的模块）引起的页面错误。用户进程也可能需要将页面锁定到内存中。例如，数据库进程可能希望管理一块内存，自己移动磁盘和内存之间的块，因为它最了解如何使用数据。在内存中固定页面是相当普遍的，大多数操作系统都有一个系统调用，允许应用程序请求将其逻辑地址空间的某个区域固定在内存中。注意，这个特性可能被滥用，并且可能导致内存管理算法的压力。因此，应用程序通常需要特殊权限才能提出这样的请求。 锁定位的另一个用途涉及正常的页面替换。考虑以下事件序列：一个低优先级进程发生页面错误。选择一个替换帧，分页系统将必要的页面读入内存。准备继续，低优先级进程进入就绪队列，并等待CPU。由于它是一个低优先级进程，可能有一段时间内不会被CPU调度程序选中。在低优先级进程等待时，一个高优先级进程发生页面错误。在寻找替换页面时，分页系统看到了一个在内存中但尚未被引用或修改的页面：它就是低优先级进程刚刚带入的页面。这个页面看起来是一个完美的替换：它是干净的，不需要写出去，而且显然已经很长时间没有被使用了。 高优先级进程是否应该能够替换低优先级进程是一个策略决定。毕竟，我们只是为了高优先级进程的利益而推迟了低优先级进程。然而，我们浪费了为低优先级进程带入页面所花费的工作。如果我们决定在新带入的页面至少被使用一次之前阻止替换，则我们可以使用锁定位来实现这种机制。当选择一个页面进行替换时，它的锁定位被打开。它会保持打开状态，直到故障处理进程再次被调度。 使用锁定位可能是危险的：锁定位可能被打开但从未关闭。如果发生这种情况（例如，由于操作系统中的错误），锁定的帧将变得无法使用。在单用户系统中，过度使用锁定只会影响执行锁定的用户。多用户系统必须对用户保持更少的信任。例如，Solaris允许锁定“提示”，但如果空闲帧池变得太小，或者某个单独进程请求在内存中锁定过多的页面，系统可以自由地忽略这些提示。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"内存管理","slug":"内存管理","permalink":"https://blackforest1990.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"}]},{"title":"Linux Booting","slug":"Linux-Booting","date":"2024-03-21T05:09:02.000Z","updated":"2024-04-28T06:56:53.719Z","comments":true,"path":"2024/03/21/Linux-Booting/","link":"","permalink":"https://blackforest1990.github.io/2024/03/21/Linux-Booting/","excerpt":"","text":"大家好，我们要开始学习Linux kernel了，很多时候操作系统的东西都令人着迷，他是如何启动的，如何建立堆栈，如何利用内存的，如何设计的，如何应用，让我们从按下电源开始吧。 从引导加载程序内核 启动！ 按下笔记本电脑或台式电脑上的神奇电源按钮，它就会开始工作。主板向供电装置发送信号。电源接收到信号后，向计算机提供适量的电量。一旦主板收到电源良好信号，它就会尝试启动CPU。CPU 重置其寄存器中的所有剩余数据，并为每个寄存器设置预定义值。 80386及更高版本的在计算机复位后在CPU寄存器中定义以下预定义数据： 123IP 0xfff0 //指令指针，它包含了CPU当前执行的指令的偏移地址CS selector 0xf000 //CS (Code Segment) selector代码段选择器，用于访问代码段寄存器CS base 0xffff0000 //代码段的基址，用于计算代码段的物理地址 处理器开始在实模式工作。8086 处理器有一个20位寻址总线，这意味着它可以对0到 2^20 位地址空间（ 1MB ）进行操作。不过它只有16位的寄存器，所以最大寻址空间是 2^16 即 0xffff （64 KB）。实模式使用段式内存管理 来管理整个内存空间。所有内存被分成固定的65536字节（64 KB） 大小的小块。由于我们不能用16位寄存器寻址大于 64KB 的内存, 一个地址包括两个部分：数据段起始地址和从该数据段起的偏移量。为了得到内存中的物理地址，我们要让数据段乘16并加上偏移量： 1PhysicalAddress = Segment * 16 + Offset 不过如果我们使用16位2进制能表示的最大值进行寻址：0xffff:0xffff，根据上面的公式，结果将会是： 12&gt;&gt;&gt; hex((0xffff &lt;&lt; 4) + 0xffff)&#x27;0x10ffef&#x27; 这个地址在 8086 处理器下，将被转换成地址 0x0ffef, 原因是因为，8086 cpu 只有20位地址线，只能表示 2^20 = 1MB 的地址，而上面这个地址已经超出了 1MB 地址的范围，所以 CPU 就舍弃了最高位。 实模式下的 1MB 地址空间分配表： 12345678910110x00000000 - 0x000003FF - Real Mode Interrupt Vector Table0x00000400 - 0x000004FF - BIOS Data Area0x00000500 - 0x00007BFF - Unused0x00007C00 - 0x00007DFF - Our Bootloader0x00007E00 - 0x0009FFFF - Unused0x000A0000 - 0x000BFFFF - Video RAM (VRAM) Memory0x000B0000 - 0x000B7777 - Monochrome Video Memory0x000B8000 - 0x000BFFFF - Color Video Memory0x000C0000 - 0x000C7FFF - Video ROM BIOS0x000C8000 - 0x000EFFFF - BIOS Shadow Area0x000F0000 - 0x000FFFFF - System BIOS CS 寄存器包含两个部分：可视段选择器和隐含基址。 结合之前定义的 CS 基址和 IP 值，逻辑地址应该是： 10xffff0000:0xfff0 这种形式的起始地址为EIP寄存器里的值加上基址地址： 12&gt;&gt;&gt; 0xffff0000 + 0xfff0&#x27;0xfffffff0&#x27; EIP 寄存器是 x86 架构中的一个 32 位寄存器，用于存储当前执行指令的地址，即指令指针。EIP 的含义是 “Extended Instruction Pointer”，它在程序执行期间不断更新，以指向下一条要执行的指令。 EIP 寄存器的值由 CPU 在执行指令时自动更新。 得到的 0xfffffff0 是 4GB - 16 字节。 这个地方是 复位向量(Reset vector) 。 这是CPU在重置后期望执行的第一条指令的内存地址。 10xFFFE_0000 - 0xFFFF_FFFF: 128 kilobyte ROM mapped into address space 0xFFFFFFF0 这个地址被映射到了 ROM，因此 CPU 执行的第一条指令来自于 ROM，而不是 RAM。 它包含一个 jump 指令，这个指令通常指向BIOS入口点。举个例子，如果访问 coreboot 源代码，将看到： 1234567 .section &quot;.reset&quot;, &quot;ax&quot;, %progbits .code16.globl _start_start: .byte 0xe9 .int _start16bit - ( . + 2 ) ... 上面的跳转指令（ opcode - 0xe9）跳转到地址 _start16bit - ( . + 2) 去执行代码。 reset 段是 16 字节代码段， 起始于地址 0xfffffff0，因此 CPU 复位之后，就会跳到这个地址来执行相应的代码 ： 1234567891011SECTIONS &#123; /* Trigger an error if I have an unuseable start address */ _bogus = ASSERT(_start16bit &gt;= 0xffff0000, &quot;_start16bit too low. Please report.&quot;); _ROMTOP = 0xfffffff0; . = _ROMTOP; .reset . : &#123; *(.reset); . = 15; BYTE(0x00); &#125;&#125; 现在BIOS已经开始工作了。在初始化和检查硬件之后，需要寻找到一个可引导设备。可引导设备列表存储在在 BIOS 配置中, BIOS 将根据其中配置的顺序，尝试从不同的设备上寻找引导程序。对于硬盘，BIOS 将尝试寻找引导扇区。如果在硬盘上存在一个MBR分区，那么引导扇区储存在第一个扇区(512字节)的头446字节，引导扇区的最后必须是 0x55 和 0xaa ，这2个字节称为魔术字节（Magic Bytes)，如果 BIOS 看到这2个字节，就知道这个设备是一个可引导设备。举个例子： 12345678910111213141516171819;; Note: this example is written in Intel Assembly syntax;[BITS 16][ORG 0x7c00]boot: mov al, &#x27;!&#x27; mov ah, 0x0e mov bh, 0x00 mov bl, 0x07 int 0x10 jmp $rtimes 510-($-$$) db 0db 0x55db 0xaa 构建并运行： 1nasm -f bin boot.nasm &amp;&amp; qemu-system-x86_64 boot 这让 QEMU 使用刚才新建的 boot 二进制文件作为磁盘镜像。由于这个二进制文件是由上述汇编语言产生，它满足引导扇区(起始设为 0x7c00, 用Magic Bytes结束)的需求。QEMU将这个二进制文件作为磁盘镜像的主引导记录(MBR)。 在这个例子中，这段代码被执行在16位的实模式，起始于内存0x7c00。之后调用 0x10 中断打印 ! 符号。用0填充剩余的510字节并用两个Magic Bytes 0xaa 和 0x55 结束。 引导程序 有多种引导程序可以选择, 比如 GRUB 2 和 syslinux。Linux内核通过 Boot protocol 来定义应该如何实现引导程序。在这里我们将只介绍 GRUB 2。 现在 BIOS 已经选择了一个启动设备，并且将控制权转移给了启动扇区中的代码，在我们的例子中，启动扇区代码是 boot.img。因为这段代码只能占用一个扇区，因此非常简单，只做一些必要的初始化，然后就跳转到 GRUB 2’s core image 去执行。 Core image 的代码请参考 diskboot.img，一般来说 core image 在磁盘上存储在启动扇区之后到第一个可用分区之前。core image 的初始化代码会把整个 core image （包括 GRUB 2的内核代码和文件系统驱动） 引导到内存中。 引导完成之后，grub_main将被调用。 grub_main 初始化控制台，计算模块基地址，设置 root 设备，读取 grub 配置文件，加载模块。最后，将 GRUB 置于 normal 模式，在这个模式中，grub_normal_execute (from grub-core/normal/main.c) 将被调用以完成最后的准备工作，然后显示一个菜单列出所用可用的操作系统。当某个操作系统被选择之后，grub_menu_execute_entry 开始执行，它将调用 GRUB 的 boot 命令，来引导被选中的操作系统。 引导程序必须填充 kernel setup header （位于 kernel setup code 偏移 0x01f1 处） 的必要字段。kernel setup header的定义开始于 arch/x86/boot/header.S： 123456789 .globl hdrhdr: setup_sects: .byte 0 root_flags: .word ROOT_RDONLY syssize: .long 0 ram_size: .word 0 vid_mode: .word SVGA_MODE root_dev: .word 0 boot_flag: .word 0xAA55 bootloader必须填充在 Linux boot protocol 中标记为 write 的头信息，比如 type_of_loader，这些头信息可能来自命令行，或者通过计算得到。在这里我们不会详细介绍所有的 kernel setup header，我们将在需要的时候逐个介绍。不过，你可以自己通过 boot protocol 来了解这些设置, 在内核被引导入内存后，内存使用情况将如下表所示： 123456789101112131415161718192021 | Protected-mode kernel |100000 +------------------------+ | I/O memory hole |0A0000 +------------------------+ | Reserved for BIOS | Leave as much as possible unused ~ ~ | Command line | (Can also be below the X+10000 mark)X+10000 +------------------------+ | Stack/heap | For use by the kernel real-mode code.X+08000 +------------------------+ | Kernel setup | The kernel real-mode code. | Kernel boot sector | The kernel legacy boot sector. X +------------------------+ | Boot loader | &lt;- Boot sector entry point 0x7C00001000 +------------------------+ | Reserved for MBR/BIOS |000800 +------------------------+ | Typically used by MBR |000600 +------------------------+ | BIOS use only |000000 +------------------------+ 所以当 bootloader 完成任务，将执行权移交给 kernel，kernel 的代码从以下地址开始执行： 10x1000 + X + sizeof(KernelBootSector) + 1 //`X` 是 kernel bootsector 被引导入内存的位置 我们可以通过 memory dump 来检查这个地址, 到这里，引导程序完成它的使命，并将控制权移交给了 Linux kernel。下面我们就来看看 kernel setup code 都做了些什么。 内核设置 经过上面的一系列操作，我们终于进入到内核了。不过从技术上说，内核还没有被运行起来，因为首先我们需要正确设置内核，启动内存管理，进程管理等等。内核设置代码的运行起点是 arch/x86/boot/header.S中定义的 _start函数。 在 _start 函数开始之前，还有很多的代码，那这些代码是做什么的呢？ 实际上 _start 开始之前的代码是 kernel 自带的 bootloader。在很久以前，是可以使用这个 bootloader 来启动 Linux 的。不过在新的 Linux 中，这个 bootloader 代码已经不再启动 Linux 内核，而只是输出一个错误信息。 为了能够作为 bootloader 来使用, header.S 开始处定义了 [MZ] MZ 魔术数字, 并且定义了 PE 头，在 PE 头中定义了输出的字符串： 1234567891011#ifdef CONFIG_EFI_STUB# &quot;MZ&quot;, MS-DOS header.byte 0x4d.byte 0x5a#endif.........pe_header: .ascii &quot;PE&quot; .word 0 之所以代码需要这样写，这个是因为遵从 UEFI 的硬件需要这样的结构才能正常引导操作系统。 去除这些作为 bootloader 使用的代码，真正的内核代码就从 _start 开始了： 123// header.S line 292.globl _start_start: 其他的 bootloader (grub2 and others) 知道 _start 所在的位置（ 从 MZ 头开始偏移 0x200 字节 ），所以这些 bootloader 就会忽略所有在这个位置前的代码（这些之前的代码位于 .bstext 段中）， 直接跳转到这个位置启动内核。 123456//// arch/x86/boot/setup.ld//. = 0; // current position.bstext : &#123; *(.bstext) &#125; // put .bstext section to position 0.bsdata : &#123; *(.bsdata) &#125; 12345678 .globl _start_start: .byte 0xeb .byte start_of_setup-1f1: // // rest of the header // _start 开始就是一个 jmp 语句（jmp 语句的 opcode 是 0xeb ），跟在后面的是一个相对地址 （ start_of_setup - 1f ）。在汇编代码中 Nf 代表了当前代码之后第一个标号为 N 的代码段的地址。回到我们的代码，在 _start 标号之后的第一个标号为 1 的代码段中包含了剩下的 setup header 结构。在标号为 1 的代码段结束之后，紧接着就是标号为 start_of_setup 的代码段 （这个代码段中的第一条指令实际上是内核开始执行之后的第一条指令） 。 下面让我们来看一下 GRUB2 的代码是如何跳转到 _start 标号处的。从 Linux 内核代码中，我们知道 _start 标号的代码位于偏移 0x200 处。在 GRUB2 的源代码中我们可以看到下面的代码： 12state.gs = state.fs = state.es = state.ds = state.ss = segment;state.cs = segment + 0x20; 在我的机器上，因为我的内核代码被加载到了内存地址 0x10000 处，所以在上面的代码执行完成之后 cs = 0x1020 （ 因此第一条指令的内存地址将是 cs &lt;&lt; 4 + 0 = 0x10200，刚好是 0x10000 开始后的 0x200 处的指令）： 12fs = es = ds = ss = 0x1000cs = 0x1020 从 start_of_setup 标号开始的代码需要完成下面这些事情： 将所有段寄存器的值设置成一样的内容 设置堆栈 设置 bss （静态变量区） 跳转到main.c 段寄存器设置 首先，内核保证将 ds 和 es 段寄存器指向相同地址，随后，使用 cld 指令来清理方向标志位： 123movw %ds, %axmovw %ax, %escld 为了能够跳转到 _start 标号出执行代码，grub2 将 cs 段寄存器的值设置成了 0x1020，这个值和其他段寄存器都是不一样的，因此下面的代码就是将 cs 段寄存器的值和其他段寄存器一致： 123pushw %dspushw $6flretw 上面的代码使用了一个小小的技巧lretw(“Load Return with Word Return”)来重置 cs 寄存器的内容，下面我们就来仔细分析。 这段代码首先将 ds寄存器的值入栈，然后将标号为 6的代码段地址入栈 ，接着执行 lretw 指令，这条指令，将把标号为 6 的内存地址放入 ip 寄存器 （instruction pointer），将 ds 寄存器的值放入 cs 寄存器。 这样一来 ds 和 cs 段寄存器就拥有了相同的值。 设置堆栈 绝大部分的 setup 代码都是为 C 语言运行环境做准备。在设置了 ds 和 es 寄存器之后，接下来step 的代码将检查 ss 寄存器的内容，如果寄存器的内容不对，那么将进行更正： 1234movw %ss, %dxcmpw %ax, %dxmovw %sp, %dxje 2f 当进入这段代码的时候， ss 寄存器的值可能是一下三种情况之一： ss 寄存器的值是 0x10000 ( 和其他除了 cs 寄存器之外的所有寄存器的一样） ss 寄存器的值不是 0x10000，但是 CAN_USE_HEAP 标志被设置了 ss 寄存器的值不是 0x10000，同时 CAN_USE_HEAP 标志没有被设置 下面我们就来分析在这三中情况下，代码都是如何工作的： ss 寄存器的值是 0x10000，在这种情况下，代码将直接跳转到标号为 2 的代码处执行: 1234562: andw $~3, %dx jnz 3f movw $0xfffc, %dx3: movw %ax, %ss movzwl %dx, %esp sti 这段代码首先将 dx 寄存器的值（就是当前sp 寄存器的值）4字节对齐，然后检查是否为0（如果是0，堆栈就不对了，因为堆栈是从大地址向小地址发展的），如果是0，那么就将 dx 寄存器的值设置成 0xfffc （64KB地址段的最后一个4字节地址）。如果不是0，那么就保持当前值不变。接下来，就将 ax 寄存器的值（ 0x10000 ）设置到 ss 寄存器，并根据 dx 寄存器的值设置正确的 sp。这样我们就得到了正确的堆栈设置，具体请参考下图： 下面让我们来看 ss != ds的情况，首先将 setup code 的结束地址_end写入 dx 寄存器。然后检查 loadflags 中是否设置了 CAN_USE_HEAP 标志。 根据 kernel boot protocol 的定义，loadflags是一个标志字段。这个字段的 Bit 7 就是 CAN_USE_HEAP 标志： 12345678 Field name: loadflags This field is a bitmask. Bit 7 (write): CAN_USE_HEAPSet this bit to 1 to indicate that the value entered in theheap_end_ptr is valid. If this field is clear, some setup codefunctionality will be disabled. loadflags 字段其他可以设置的标志包括： 1234#define LOADED_HIGH (1&lt;&lt;0)#define QUIET_FLAG (1&lt;&lt;5)#define KEEP_SEGMENTS (1&lt;&lt;6)#define CAN_USE_HEAP (1&lt;&lt;7) 123456789101112131415movw $_end, %dx testb $CAN_USE_HEAP, loadflags jz 1f movw heap_end_ptr, %dx1: addw $STACK_SIZE, %dx jnc 2f xorw %dx, %dx # Prevent wraparound2: # Now %dx should point to the end of our stack space andw $~3, %dx # dword align (might as well...) jnz 3f movw $0xfffc, %dx # Make sure we&#x27;re not zero3: movw %ax, %ss movzwl %dx, %esp # Clear upper half of %esp sti # Now we should have a working stack 如果 CAN_USE_HEAP 被置位，那么将 heap_end_ptr 放入 dx 寄存器，然后加上 STACK_SIZE （最小堆栈大小是 512 bytes）。在加法完成之后，如果结果没有溢出，那么就跳转到标号为 2 的代码处继续执行，接着我们就得到了如下图所示的堆栈： 最后一种情况就是 CAN_USE_HEAP 没有置位， 那么我们就将 dx 寄存器的值加上 STACK_SIZE，然后跳转到标号为 2 的代码处继续执行，接着我们就得到了如下图所示的堆栈： BSS段设置 在我们正式执行 C 代码之前，我们还有2件事情需要完成。1）设置正确的 BSS段 ；2）检查 magic 签名。接下来的代码，首先检查 magic 签名 setup_sig，如果签名不对，直接跳转到 setup_bad 部分执行代码： 12cmpl $0x5a5aaa55, setup_sigjne setup_bad BSS 段用来存储那些没有被初始化的静态变量。对于这个段使用的内存， Linux 首先使用下面的代码将其全部清零： 123456movw $__bss_start, %dimovw $_end+3, %cxxorl %eax, %eaxsubw %di, %cxshrw $2, %cxrep; stosl 在这段代码中，首先将__bss_start地址放入 di 寄存器，然后将 _end + 3 （4字节对齐） 地址放入 cx，接着使用 xor 指令将 ax 寄存器清零，接着计算 BSS 段的大小 （ cx - di ），然后将大小放入 cx 寄存器。接下来将 cx 寄存器除4，最后使用 rep; stosl 指令将 ax 寄存器的值（0）写入 寄存器整个 BSS 段。 代码执行完成之后，我们将得到如下图所示的 BSS 段: 跳转到 main 函数 到目前为止，我们完成了堆栈和 BSS 的设置，现在我们可以正式跳入 main() 函数来执行 C 代码了： 1call main main() 函数定义在 arch/x86/boot/main.c。 在内核安装代码的第一步 保护模式 在操作系统可以使用Intel 64位CPU的长模式之前，内核必须首先将CPU切换到保护模式运行。什么是保护模式？保护模式于1982年被引入到Intel CPU家族，并且从那之后，直到Intel 64出现，保护模式都是Intel CPU的主要运行模式。淘汰实模式的主要原因是因为在实模式下，系统能够访问的内存非常有限。在实模式下，系统最多只能访问1M内存，而且在很多时候，实际能够访问的内存只有640K。 保护模式带来了很多的改变，不过主要的改变都集中在内存管理方法。在保护模式中，实模式的20位地址线被替换成32位地址线，因此系统可以访问多达4GB的地址空间。另外，在保护模式中引入了内存分页功能。 保护模式提供了2种完全不同的内存管理机制： 段式内存管理 内存分页 在保护模式中，内存段的定义和实模式完全不同。在保护模式中，每个内存段不再是64K大小，段的大小和起始位置是通过一个叫做段描述符的数据结构进行描述。所有内存段的段描述符存储在一个叫做全局描述符表(GDT)的内存结构中。 全局描述符表这个内存数据结构在内存中的位置并不是固定的，它的地址保存在一个特殊寄存器 GDTR 中。具体的汇编代码看起来是这样的： 1lgdt gdt lgdt 汇编代码将把全局描述符表的基地址和大小保存到 GDTR 寄存器中。GDTR 是一个48位的寄存器，这个寄存器中的保存了2部分的内容: 全局描述符表的大小 (16位） 全局描述符表的基址 (32位) 就像前面的段落说的，全局描述符表包含了所有内存段的段描述符。每个段描述符长度是64位，结构如下图描述： 1234567891031 24 19 16 7 0------------------------------------------------------------| | |B| |A| | | | |0|E|W|A| || BASE 31:24 |G|/|L|V| LIMIT |P|DPL|S| TYPE | BASE 23:16 | 4| | |D| |L| 19:16 | | | |1|C|R|A| |------------------------------------------------------------| | || BASE 15:0 | LIMIT 15:0 | 0| | |------------------------------------------------------------ Limit[20位] 被保存在上述内存结构的0-15和48-51位。根据上述内存结构中G位的设置，这20位内存定义的内存长度是不一样的。下面是一些具体的例子： 如果G = 0, 并且Limit = 0， 那么表示段长度是1 byte 如果G = 1, 并且Limit = 0, 那么表示段长度是4K bytes 如果G = 0，并且Limit = 0xfffff，那么表示段长度是1M bytes 如果G = 1，并且Limit = 0xfffff，那么表示段长度是4G bytes 从上面的例子我们可以看出： 如果G = 0, 那么内存段的长度是按照1 byte进行增长的 ( Limit每增加1，段长度增加1 byte )，最大的内存段长度将是1M bytes； 如果G = 1, 那么内存段的长度是按照4K bytes进行增长的 ( Limit每增加1，段长度增加4K bytes )，最大的内存段长度将是4G bytes; 段长度的计算公式是 base_seg_length * ( LIMIT + 1)。 Base[32-bits] 被保存在上述地址结构的0-15， 32-39以及56-63位。Base定义了段基址。 Type/Attribute (40-44 bits) 定义了内存段的类型以及支持的操作。 S 标记（ 第44位 ）定义了段的类型，S = 0说明这个内存段是一个系统段；S = 1说明这个内存段是一个代码段或者是数据段（ 堆栈段是一种特殊类型的数据段，堆栈段必须是可以进行读写的段 ）。在S = 1的情况下，上述内存结构的第43位决定了内存段是数据段还是代码段。如果43位 = 0，说明是一个数据段，否则就是一个代码段 对于数据段和代码段，下面的表格给出了段类型定义 123456789101112131415161718192021| Type Field | Descriptor Type | Description|-----------------------------|-----------------|------------------| Decimal | || 0 E W A | || 0 0 0 0 0 | Data | Read-Only| 1 0 0 0 1 | Data | Read-Only, accessed| 2 0 0 1 0 | Data | Read/Write| 3 0 0 1 1 | Data | Read/Write, accessed| 4 0 1 0 0 | Data | Read-Only, expand-down| 5 0 1 0 1 | Data | Read-Only, expand-down, accessed| 6 0 1 1 0 | Data | Read/Write, expand-down| 7 0 1 1 1 | Data | Read/Write, expand-down, accessed| C R A | || 8 1 0 0 0 | Code | Execute-Only| 9 1 0 0 1 | Code | Execute-Only, accessed| 10 1 0 1 0 | Code | Execute/Read| 11 1 0 1 1 | Code | Execute/Read, accessed| 12 1 1 0 0 | Code | Execute-Only, conforming| 14 1 1 0 1 | Code | Execute-Only, conforming, accessed| 13 1 1 1 0 | Code | Execute/Read, conforming| 15 1 1 1 1 | Code | Execute/Read, conforming, accessed 从上面的表格我们可以看出，当第43位是0的时候，这个段描述符对应的是一个数据段，如果该位是1，那么表示这个段描述符对应的是一个代码段。对于数据段，第42，41，40位表示的是(E扩展，W可写，A可访问）；对于代码段，第42，41，40位表示的是(C一致，R可读，A可访问）。 如果E = 0，数据段是向上扩展数据段，反之为向下扩展数据段。关于向上扩展和向下扩展数据段，可以参考下面的链接。在一般情况下，应该是不会使用向下扩展数据段的。 如果W = 1，说明这个数据段是可写的，否则不可写。所有数据段都是可读的。 A位表示该内存段是否已经被CPU访问。 如果C = 1，说明这个代码段可以被低优先级的代码访问，比如可以被用户态代码访问。反之如果C = 0，说明只能同优先级的代码段可以访问。 如果R = 1，说明该代码段可读。代码段是永远没有写权限的。 DPL（2-bits, bit 45 和 46）定义了该段的优先级。具体数值是0-3。 P 标志(bit 47) - 说明该内存段是否已经存在于内存中。如果P = 0，那么在访问这个内存段的时候将报错。 AVL 标志(bit 52) - 这个位在Linux内核中没有被使用。 L 标志(bit 53) - 只对代码段有意义，如果L = 1，说明该代码段需要运行在64位模式下。 D/B flag(bit 54) - 根据段描述符描述的是一个可执行代码段、下扩数据段还是一个堆栈段，这个标志具有不同的功能。（对于32位代码和数据段，这个标志应该总是设置为1；对于16位代码和数据段，这个标志被设置为0。）。 可执行代码段。此时这个标志称为D标志并用于指出该段中的指令引用有效地址和操作数的默认长度。如果该标志置位，则默认值是32位地址和32位或8位的操作数；如果该标志为0，则默认值是16位地址和16位或8位的操作数。指令前缀0x66可以用来选择非默认值的操作数大小；前缀0x67可用来选择非默认值的地址大小。 栈段（由SS寄存器指向的数据段）。此时该标志称为B（Big）标志，用于指明隐含堆栈操作（如PUSH、POP或CALL）时的栈指针大小。如果该标志置位，则使用32位栈指针并存放在ESP寄存器中；如果该标志为0，则使用16位栈指针并存放在SP寄存器中。如果堆栈段被设置成一个下扩数据段，这个B标志也同时指定了堆栈段的上界限。 下扩数据段。此时该标志称为B标志，用于指明堆栈段的上界限。如果设置了该标志，则堆栈段的上界限是0xFFFFFFFF（4GB）；如果没有设置该标志，则堆栈段的上界限是0xFFFF（64KB）。 在保护模式下，段寄存器保存的不再是一个内存段的基地址，而是一个称为段选择子的结构。每个段描述符都对应一个段选择子。段选择子是一个16位的数据结构，下图显示了这个数据结构的内容： 123-----------------------------| Index | TI | RPL |----------------------------- 其中， Index 表示在GDT中，对应段描述符的索引号。 TI 表示要在GDT还是LDT中查找对应的段描述符 RPL 表示请求者优先级。这个优先级将和段描述符中的优先级协同工作，共同确定访问是否合法。 在保护模式下，每个段寄存器实际上包含下面2部分内容： 可见部分 - 段选择子 隐藏部分 - 段描述符 在保护模式中，cpu是通过下面的步骤来找到一个具体的物理地址的： 代码必须将相应的段选择子装入某个段寄存器 CPU根据段选择子从GDT中找到一个匹配的段描述符，然后将段描述符放入段寄存器的隐藏部分 在没有使用向下扩展段的时候，那么内存段的基地址就是段描述符中的基地址，段描述符的limit + 1就是内存段的长度。如果你知道一个内存地址的偏移，那么在没有开启分页机制的情况下，这个内存的物理地址就是基地址+偏移 当代码要从实模式进入保护模式的时候，需要执行下面的操作： 禁止中断发生 使用命令 lgdt 将GDT表装入 GDTR 寄存器 设置CR0寄存器的PE位为1，使CPU进入保护模式 跳转开始执行保护模式代码 将启动参数拷贝到&quot;zeropage&quot; 让我们从main函数开始看起，这个函数中，首先调用了copy_boot_params(void)。 这个函数将内核设置信息拷贝到boot_params结构的相应字段。大家可以在arch/x86/include/uapi/asm/bootparam.h找到boot_params结构的定义。 boot_params结构中包含struct setup_header hdr字段。这个结构包含了linux boot protocol中定义的相同字段，并且由boot loader填写。在内核编译的时候copy_boot_params完成两个工作： 将header.S中定义的 hdr 结构中的内容拷贝到 boot_params 结构的字段 struct setup_header hdr 中。 如果内核是通过老的命令行协议运行起来的，那么就更新内核的命令行指针。 这里需要注意的是拷贝 hdr 数据结构的 memcpy 函数不是C语言中的函数，而是定义在 copy.S。让我们来具体分析一下这段代码： 123456789101112131415GLOBAL(memcpy) pushw %si ;push si to stack pushw %di ;push di to stack movw %ax, %di ;move &amp;boot_param.hdr to di movw %dx, %si ;move &amp;hdr to si pushw %cx ;push cx to stack ( sizeof(hdr) ) shrw $2, %cx rep; movsl ;copy based on 4 bytes popw %cx ;pop cx andw $3, %cx ;cx = cx % 4 rep; movsb ;copy based on one byte popw %di popw %si retlENDPROC(memcpy) 在copy.S文件中，你可以看到所有的方法都开始于 GLOBAL 宏定义，而结束于 ENDPROC 宏定义。 你可以在 arch/x86/include/asm/linkage.h中找到 GLOBAL 宏定义。这个宏给代码段分配了一个名字标签，并且让这个名字全局可用。 123#define GLOBAL(name) \\ .globl name; \\ name: 你可以在include/linux/linkage.h中找到 ENDPROC 宏的定义。 这个宏通过 END(name) 代码标识了汇编函数的结束，同时将函数名输出，从而静态分析工具可以找到这个函数。 123#define ENDPROC(name) \\ .type name, @function ASM_NL \\ END(name) memcpy 的实现代码是很容易理解的。首先，代码将 si 和 di 寄存器的值压入堆栈进行保存，这么做的原因是因为后续的代码将修改 si 和 di 寄存器的值。memcpy 函数（也包括其他定义在copy.s中的其他函数）使用了 fastcall 调用规则，意味着所有的函数调用参数是通过 ax, dx, cx寄存器传入的，而不是传统的通过堆栈传入。因此在使用下面的代码调用 memcpy 函数的时候 1memcpy(&amp;boot_params.hdr, &amp;hdr, sizeof hdr); 函数的参数是这样传递的 ax 寄存器指向 boot_param.hdr 的内存地址 dx 寄存器指向 hdr 的内存地址 cx 寄存器包含 hdr 结构的大小 memcpy 函数在将 si 和 di 寄存器压栈之后，将 boot_param.hdr 的地址放入 di 寄存器，将 hdr 的地址放入 si 寄存器，并且将 hdr 数据结构的大小压栈。 接下来代码首先以4个字节为单位，将 si 寄存器指向的内存内容拷贝到 di 寄存器指向的内存。当剩下的字节数不足4字节的时候，代码将原始的 hdr 数据结构大小出栈放入 cx ，然后对 cx 的值对4求模，接下来就是根据 cx 的值，以字节为单位将 si 寄存器指向的内存内容拷贝到 di 寄存器指向的内存。当拷贝操作完成之后，将保留的 si 以及 di 寄存器值出栈，函数返回。 控制台初始化 在 hdr 结构体被拷贝到 boot_params.hdr 成员之后，系统接下来将进行控制台的初始化。控制台初始化时通过调用arch/x86/boot/early_serial_console.c中定义的 console_init 函数实现的。 这个函数首先查看命令行参数是否包含 earlyprintk 选项。如果命令行参数包含该选项，那么函数将分析这个选项的内容。得到控制台将使用的串口信息，然后进行串口的初始化。以下是 earlyprintk 选项可能的取值： serial,0x3f8,115200 serial,ttyS0,115200 ttyS0,115200 当串口初始化成功之后，如果命令行参数包含 debug 选项，我们将看到如下的输出。 12if (cmdline_find_option_bool(&quot;debug&quot;)) puts(&quot;early console in setup code\\n&quot;); puts 函数定义在tty.c。这个函数只是简单的调用 putchar 函数将输入字符串中的内容按字节输出。下面让我们来看看 putchar函数的实现： 12345678910void __attribute__((section(&quot;.inittext&quot;))) putchar(int ch)&#123; if (ch == &#x27;\\n&#x27;) putchar(&#x27;\\r&#x27;); bios_putchar(ch); if (early_serial_base != 0) serial_putchar(ch);&#125; __attribute__((section(&quot;.inittext&quot;))) 说明这段代码将被放入 .inittext 代码段。关于 .inittext 代码段的定义你可以在 setup.ld中找到。 如果需要输出的字符是 \\n ，那么 putchar 函数将调用自己首先输出一个字符 \\r。接下来，就调用 bios_putchar 函数将字符输出到显示器（使用bios int10中断）： 1234567891011static void __attribute__((section(&quot;.inittext&quot;))) bios_putchar(int ch)&#123; struct biosregs ireg; initregs(&amp;ireg); ireg.bx = 0x0007; ireg.cx = 0x0001; ireg.ah = 0x0e; ireg.al = ch; intcall(0x10, &amp;ireg, NULL);&#125; 在上面的代码中 initreg 函数接受一个 biosregs 结构的地址作为输入参数，该函数首先调用 memset 函数将 biosregs 结构体所有成员清0。 123456memset(reg, 0, sizeof *reg); reg-&gt;eflags |= X86_EFLAGS_CF; reg-&gt;ds = ds(); reg-&gt;es = ds(); reg-&gt;fs = fs(); reg-&gt;gs = gs(); 下面让我们来看看memset函数的实现 : 1234567891011121314GLOBAL(memset) pushw %di movw %ax, %di movzbl %dl, %eax imull $0x01010101,%eax pushw %cx shrw $2, %cx rep; stosl popw %cx andw $3, %cx rep; stosb popw %di retlENDPROC(memset) 首先你会发现，memset 函数和 memcpy 函数一样使用了 fastcall 调用规则，因此函数的参数是通过 ax，dx 以及 cx 寄存器传入函数内部的。 就像memcpy函数一样，memset 函数一开始将 di 寄存器入栈，然后将 biosregs 结构的地址从 ax 寄存器拷贝到di寄存器。接下来，使用 movzbl 指令将 dl 寄存器的内容拷贝到 ax 寄存器的低字节，到这里 ax 寄存器就包含了需要拷贝到 di 寄存器所指向的内存的值。 接下来的 imull 指令将 eax 寄存器的值乘上 0x01010101。这么做的原因是代码每次将尝试拷贝4个字节内存的内容。下面让我们来看一个具体的例子，假设我们需要将 0x7 这个数值放到内存中，在执行 imull 指令之前，eax 寄存器的值是 0x7，在 imull 指令被执行之后，eax 寄存器的内容变成了 0x07070707（4个字节的 0x7）。在 imull 指令之后，代码使用 rep; stosl 指令将 eax 寄存器的内容拷贝到 es:di 指向的内存。 在 bisoregs 结构体被 initregs 函数正确填充之后，bios_putchar 调用中断 0x10 在显示器上输出一个字符。接下来 putchar 函数检查是否初始化了串口，如果串口被初始化了，那么将调用serial_putchar将字符输出到串口。 堆初始化 当堆栈和bss段在header.S中被初始化之后， 内核需要初始化全局堆，全局堆的初始化是通过 init_heap 函数实现的。 代码首先检查内核设置头中的loadflags是否设置了 CAN_USE_HEAP标志。 如果该标记被设置了，那么代码将计算堆栈的结束地址： 123456char *stack_end; //%P1 is (-STACK_SIZE) if (boot_params.hdr.loadflags &amp; CAN_USE_HEAP) &#123; asm(&quot;leal %P1(%%esp),%0&quot; : &quot;=r&quot; (stack_end) : &quot;i&quot; (-STACK_SIZE)); 换言之stack_end = esp - STACK_SIZE. 在计算了堆栈结束地址之后，代码计算了堆的结束地址： 12//heap_end = heap_end_ptr + 512 heap_end = (char *)((size_t)boot_params.hdr.heap_end_ptr + 0x200); 接下来代码判断 heap_end 是否大于 stack_end，如果条件成立，将 stack_end 设置成 heap_end（这么做是因为在大部分系统中全局堆和堆栈是相邻的，但是增长方向是相反的）。 到这里为止，全局堆就被正确初始化了。在全局堆被初始化之后，我们就可以使用 GET_HEAP 方法。 检查CPU类型 在堆栈初始化之后，内核代码通过调用arch/x86/boot/cpu.c提供的 validate_cpu 方法检查CPU级别以确定系统是否能够在当前的CPU上运行。validate_cpu 调用了check_cpu方法得到当前系统的CPU级别，并且和系统预设的最低CPU级别进行比较。如果不满足条件，则不允许系统运行。 12345678/*from cpu.c*/check_cpu(&amp;cpu_level, &amp;req_level, &amp;err_flags);/*after check_cpu call, req_level = req_level defined in cpucheck.c*/if (cpu_level &lt; req_level) &#123; printf(&quot;This kernel requires an %s CPU, &quot;, cpu_name(req_level)); printf(&quot;but only detected an %s CPU.\\n&quot;, cpu_name(cpu_level)); return -1;&#125; 除此之外，check_cpu 方法还做了大量的其他检测和设置工作，下面就简单介绍一些：1）检查cpu标志，如果cpu是64位cpu，那么就设置long mode, 2) 检查CPU的制造商，根据制造商的不同，设置不同的CPU选项。比如对于AMD出厂的cpu，如果不支持 SSE+SSE2，那么就禁止这些选项。 内存分布侦测 接下来，内核调用 detect_memory 方法进行内存侦测，以得到系统当前内存的使用分布。该方法使用多种编程接口，包括 0xe820（获取全部内存分配），0xe801 和 0x88（获取临近内存大小），进行内存分布侦测。在这里我们只介绍arch/x86/boot/memory.c中提供的 detect_memory_e820 方法。 该方法首先调用 initregs 方法初始化 biosregs 数据结构，然后向该数据结构填入 0xe820 编程接口所要求的参数： 12345initregs(&amp;ireg);ireg.ax = 0xe820;ireg.cx = sizeof buf;ireg.edx = SMAP;ireg.di = (size_t)&amp;buf; ax 固定为 0xe820 cx 包含数据缓冲区的大小，该缓冲区将包含系统内存的信息数据 edx 必须是 SMAP 这个魔术数字，就是 0x534d4150 es:di 包含数据缓冲区的地址 ebx 必须为0. 接下来就是通过一个循环来收集内存信息了。每个循环都开始于一个 0x15 中断调用，这个中断调用返回地址分配表中的一项，接着程序将返回的 ebx 设置到 biosregs 数据结构中，然后进行下一次的 0x15 中断调用。那么循环什么时候结束呢？直到 0x15 调用返回的eflags包含标志 X86_EFLAGS_CF: 12intcall(0x15, &amp;ireg, &amp;oreg);ireg.ebx = oreg.ebx; 在循环结束之后，整个内存分配信息将被写入到 e820entry 数组中，这个数组的每个元素包含下面3个信息: 内存段的起始地址 内存段的大小 内存段的类型（类型可以是reserved, usable等等)。 你可以在 dmesg 输出中看到这个数组的内容： 1234567[ 0.000000] e820: BIOS-provided physical RAM map:[ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable[ 0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved[ 0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved[ 0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000003ffdffff] usable[ 0.000000] BIOS-e820: [mem 0x000000003ffe0000-0x000000003fffffff] reserved[ 0.000000] BIOS-e820: [mem 0x00000000fffc0000-0x00000000ffffffff] reserved 键盘初始化 接下来内核调用keyboard_init()方法进行键盘初始化操作。 首先，方法调用initregs初始化寄存器结构，然后调用0x16中断来获取键盘状态。 1234initregs(&amp;ireg);ireg.ah = 0x02; /* Get keyboard status */intcall(0x16, &amp;ireg, &amp;oreg);boot_params.kbd_status = oreg.al; 在获取了键盘状态之后，代码再次调用0x16中断来设置键盘的按键检测频率。 12ireg.ax = 0x0305; /* Set keyboard repeat rate */intcall(0x16, &amp;ireg, NULL); 显示模式初始化和进入保护模式 就像我们前面所说的，我们将从 set_video 函数开始我们这章的内容，你可以在 arch/x86/boot/video.c找到这个函数的定义。 这个函数首先从 boot_params.hdr 数据结构获取显示模式设置： 1u16 mode = boot_params.hdr.vid_mode; 至于 boot_params.hdr 数据结构中的内容，是通过 copy_boot_params 函数实现的 ，boot_params.hdr 中的 vid_mode 是引导程序必须填入的字段。你可以在 kernel boot protocol 文档中找到关于 vid_mode 的详细信息： 123Offset Proto Name Meaning/Size01FA/2 ALL vid_mode Video mode control 而在 linux kernel boot protocol 文档中定义了如何通过命令行参数的方式为 vid_mode 字段传入相应的值： 12345678**** SPECIAL COMMAND LINE OPTIONSvga=&lt;mode&gt; &lt;mode&gt; here is either an integer (in C notation, either decimal, octal, or hexadecimal) or one of the strings &quot;normal&quot; (meaning 0xFFFF), &quot;ext&quot; (meaning 0xFFFE) or &quot;ask&quot; (meaning 0xFFFD). This value should be entered into the vid_mode field, as it is used by the kernel before the command line is parsed. 根据上面的描述，我们可以通过将 vga 选项写入 grub 或者写到引导程序的配置文件，从而让内核命令行得到相应的显示模式设置信息。这个选项可以接受不同类型的值来表示相同的意思。比如你可以传入 0XFFFD 或者 ask，这2个值都表示需要显示一个菜单让用户选择想要的显示模式。下面的链接就给出了这个菜单： 通过这个菜单，用户可以选择想要进入的显示模式。不过在我们进一步了解显示模式的设置过程之前，让我们先回头了解一些重要的概念。 内核数据类型 Type char short int long u8 u16 u32 u64 Size 1 2 4 8 1 2 4 8 堆操作API 在 set_video 函数将 vid_mod 的值设置完成之后，将调用 RESET_HEAP 宏将 HEAP 头指向 _end 符号。RESET_HEAP 宏定义在 boot.h： 1#define RESET_HEAP() ((void *)( HEAP = _end )) 这个宏只是简单的将 HEAP 头设置到 _end 标号。在 boot.h 中通过 extern char _end[]; 来引用（从这里可以看出，在内核初始化的时候堆和栈是共享内存空间的）： 下面一个是 GET_HEAP 宏： 12#define GET_HEAP(type, n) \\ ((type *)__get_heap(sizeof(type),__alignof__(type),(n))) 可以看出这个宏调用了 __get_heap 函数来进行内存的分配。__get_heap 需要下面3个参数来进行内存分配操作： 某个数据类型所占用的字节数 __alignof__(type) 返回对于请求的数据类型需要怎样的对齐方式 ( gcc 提供的一个功能 ） n 需要分配多少个对应数据类型的对象 下面是 __get_heap 函数的实现： 123456789static inline char *__get_heap(size_t s, size_t a, size_t n)&#123; char *tmp; HEAP = (char *)(((size_t)HEAP+(a-1)) &amp; ~(a-1)); tmp = HEAP; HEAP += s*n; return tmp;&#125; 现在让我们来了解这个函数是如何工作的。 这个函数首先根据对齐方式要求（参数 a ）调整 HEAP 的值，然后将 HEAP 值赋值给一个临时变量 tmp。接下来根据需要分配的对象的个数（参数 n ），预留出所需要的内存，然后将 tmp 返回给调用端。 最后一个关于 HEAP 的操作是： 1234static inline bool heap_free(size_t n)&#123; return (int)(heap_end - HEAP) &gt;= (int)n;&#125; 这个函数简单做了一个减法 heap_end - HEAP，如果相减的结果大于请求的内存，那么就返回真，否则返回假。 设置显示模式 在我们分析了内核数据类型以及和 HEAP 相关的操作之后，让我们回来继续分析显示模式的初始化。在 RESET_HEAP() 函数被调用之后，set_video 函数接着调用 store_mode_params 函数将对应显示模式的相关参数写入 boot_params.screen_info 字段。这个字段的结构定义可以在 include/uapi/linux/screen_info.h中找到。 store_mode_params 函数将调用 store_cursor_position 函数将当前屏幕上光标的位置保存起来。下面让我们来看 store_cursor_poistion 函数是如何实现的。 首先函数初始化一个类型为 biosregs 的变量，将其中的 AH 寄存器内容设置成 0x3，然后调用 0x10 BIOS 中断。当中断调用返回之后，DL 和 DH 寄存器分别包含了当前光标的行和列信息。接着，这2个信息将被保存到 boot_params.screen_info 字段的 orig_x 和 orig_y字段。 在 store_cursor_position 函数执行完毕之后，store_mode_params 函数将调用 store_video_mode 函数将当前使用的显示模式保存到 boot_params.screen_info.orig_video_mode。 接下來 store_mode_params 函数将根据当前显示模式的设定，给 video_segment 变量设置正确的值（实际上就是设置显示内存的起始地址）。在 BIOS 将控制权转移到引导扇区的时候，显示内存地址和显示模式的对应关系如下表所示： 120xB000:0x0000 32 Kb Monochrome Text Video Memory0xB800:0x0000 32 Kb Color Text Video Memory 根据上表，如果当前显示模式是 MDA, HGC 或者单色 VGA 模式，那么 video_sgement 的值将被设置成 0xB000；如果当前显示模式是彩色模式，那么 video_segment 的值将被设置成 0xB800。在这之后，store_mode_params 函数将保存字体大小信息到 boot_params.screen_info.orig_video_points： 1234//保存字体大小信息set_fs(0);font_size = rdfs16(0x485);boot_params.screen_info.orig_video_points = font_size; 这段代码首先调用 set_fs 函数（在 boot.h中定义了许多类似的函数进行寄存器操作）将数字 0 放入 FS 寄存器。接着从内存地址 0x485 处获取字体大小信息并保存到 boot_params.screen_info.orig_video_points。 12x = rdfs16(0x44a);y = (adapter == ADAPTER_CGA) ? 25 : rdfs8(0x484)+1; 接下来代码将从地址 0x44a 处获得屏幕列信息，从地址 0x484 处获得屏幕行信息，并将它们保存到 boot_params.screen_info.orig_video_cols 和 boot_params.screen_info.orig_video_lines。到这里，store_mode_params 的执行就结束了。 接下来，set_video 函数将调用 save_screen 函数将当前屏幕上的所有信息保存到 HEAP 中。这个函数首先获得当前屏幕的所有信息（包括屏幕大小，当前光标位置，屏幕上的字符信息），并且保存到 saved_screen 结构体中。这个结构体的定义如下所示： 12345static struct saved_screen &#123; int x, y; int curx, cury; u16 *data;&#125; saved; 接下来函数将检查 HEAP 中是否有足够的空间保存这个结构体的数据： 12if (!heap_free(saved.x*saved.y*sizeof(u16)+512)) return; 如果 HEAP 有足够的空间，代码将在 HEAP 中分配相应的空间并且将 saved_screen 保存到 HEAP。 接下来 set_video 函数将调用 probe_cards(0)（这个函数定义在 arch/x86/boot/video-mode.c）。 这个函数简单遍历所有的显卡，并通过调用驱动程序设置显卡所支持的显示模式： 12345678for (card = video_cards; card &lt; video_cards_end; card++) &#123; if (card-&gt;unsafe == unsafe) &#123; if (card-&gt;probe) card-&gt;nmodes = card-&gt;probe(); else card-&gt;nmodes = 0; &#125;&#125; 如果你仔细看上面的代码，你会发现 video_cards 这个变量并没有被声明，那么程序怎么能够正常编译执行呢？实际上很简单，它指向了一个在 arch/x86/boot/setup.ld 中定义的叫做 .videocards 的内存段： 12345.videocards : &#123; video_cards = .; *(.videocards) video_cards_end = .; &#125; 那么这段内存里面存放的数据是什么呢，下面我们就来详细分析。在内核初始化代码中，对于每个支持的显示模式都是使用下面的代码进行定义的： 12345static __videocard video_vga = &#123; .card_name = &quot;VGA&quot;, .probe = vga_probe, .set_mode = vga_set_mode,&#125;; __videocard 是一个宏定义，如下所示： 1#define __videocard struct card_info __attribute__((used,section(&quot;.videocards&quot;))) 因此 __videocard 是一个 card_info 结构，这个结构定义如下： 12345678910struct card_info &#123; const char *card_name; int (*set_mode)(struct mode_info *mode); int (*probe)(void); struct mode_info *modes; int nmodes; int unsafe; u16 xmode_first; u16 xmode_n;&#125;; 在 .videocards 内存段实际上存放的就是所有被内核初始化代码定义的 card_info 结构（可以看成是一个数组），所以 probe_cards 函数可以使用 video_cards，通过循环遍历所有的 card_info。 在 probe_cards 执行完成之后，我们终于进入 set_video 函数的主循环了。在这个循环中，如果 vid_mode=ask，那么将显示一个菜单让用户选择想要的显示模式，然后代码将根据用户的选择或者 vid_mod 的值 ，通过调用 set_mode 函数来设置正确的显示模式。如果设置成功，循环结束，否则显示菜单让用户选择显示模式，继续进行设置显示模式的尝试。 12345678910for (;;) &#123; if (mode == ASK_VGA) mode = mode_menu(); if (!set_mode(mode)) break; printf(&quot;Undefined video mode number: %x\\n&quot;, mode); mode = ASK_VGA; &#125; 你可以在 video-mode.c 中找到 set_mode 函数的定义。这个函数只接受一个参数，这个参数是对应的显示模式的数字表示（这个数字来自于显示模式选择菜单，或者从内核命令行参数获得）。 set_mode 函数首先检查传入的 mode 参数，然后调用 raw_set_mode 函数。而后者将遍历内核知道的所有 card_info 信息，如果发现某张显卡支持传入的模式，这调用 card_info 结构中保存的 set_mode 函数地址进行显卡显示模式的设置。以 video_vga 这个 card_info 结构来说，保存在其中的 set_mode 函数就指向了 vga_set_mode 函数。下面的代码就是 vga_set_mode 函数的实现，这个函数根据输入的 vga 显示模式，调用不同的函数完成显示模式的设置： 12345678910111213141516171819202122232425262728293031static int vga_set_mode(struct mode_info *mode)&#123; vga_set_basic_mode(); force_x = mode-&gt;x; force_y = mode-&gt;y; switch (mode-&gt;mode) &#123; case VIDEO_80x25: break; case VIDEO_8POINT: vga_set_8font(); break; case VIDEO_80x43: vga_set_80x43(); break; case VIDEO_80x28: vga_set_14font(); break; case VIDEO_80x30: vga_set_80x30(); break; case VIDEO_80x34: vga_set_80x34(); break; case VIDEO_80x60: vga_set_80x60(); break; &#125; return 0;&#125; 在上面的代码中，每个 vga_set*** 函数只是简单调用 0x10 BIOS 中断来进行显示模式的设置。 在显卡的显示模式被正确设置之后，这个最终的显示模式被写回 boot_params.hdr.vid_mode。 接下来 set_video 函数将调用 vesa_store_edid 函数， 这个函数只是简单的将 EDID (Extended Display Identification Data) 写入内存，以便于内核访问。最后， set_video 将调用 do_restore 函数将前面保存的当前屏幕信息还原到屏幕上。 到这里为止，显示模式的设置完成，接下来我们可以切换到保护模式了。 在切换到保护模式之前的最后准备工作。 在进入保护模式之前的最后一个函数调用发生在 main.c中的 go_to_protected_mode 函数，就像这个函数的注释说的，这个函数将进行最后的准备工作然后进入保护模式，下面就让我们来具体看看最后的准备工作是什么，以及系统是如何切换到保护模式的。 go_to_protected_mode 函数本身定义在 arch/x86/boot/pm.c。 这个函数调用了一些其他的函数进行最后的准备工作，下面就让我们来具体看看这些函数。 go_to_protected_mode 函数首先调用的是 realmode_switch_hook 函数，后者如果发现 realmode_switch hook， 那么将调用它并禁止 NMI 中断，反之将直接禁止 NMI 中断。只有当 bootloader 运行在宿主环境下（比如在 DOS 下运行 ）， hook 才会被使用。你可以在 boot protocol (see ADVANCED BOOT LOADER HOOKS) 中详细了解 hook 函数的信息。 12345678910111213141516/* * Invoke the realmode switch hook if present; otherwise * disable all interrupts. */static void realmode_switch_hook(void)&#123; if (boot_params.hdr.realmode_swtch) &#123; asm volatile(&quot;lcallw *%0&quot; : : &quot;m&quot; (boot_params.hdr.realmode_swtch) : &quot;eax&quot;, &quot;ebx&quot;, &quot;ecx&quot;, &quot;edx&quot;); &#125; else &#123; asm volatile(&quot;cli&quot;); outb(0x80, 0x70); /* Disable NMI */ io_delay(); &#125;&#125; realmode_switch 指向了一个16 位实模式代码地址（远跳转指针），这个16位代码将禁止 NMI 中断。所以在上述代码中，如果 realmode_swtch hook 存在，代码是用了 lcallw 指令进行远函数调用。在我的环境中，因为不存在这个 hook ，所以代码是直接进入 else 部分进行了 NMI(Non-Maskable Interrupt)的禁止：代码首先调用 cli 汇编指令清除了中断标志 IF，这条指令执行之后，外部中断就被禁止了，紧接着的下一行代码就禁止了 NMI 中断。 这里简单介绍一下中断。中断是由硬件或者软件产生的，当中断产生的时候， CPU 将得到通知。这个时候， CPU 将停止当前指令的执行，保存当前代码的环境，然后将控制权移交到中断处理程序。当中断处理程序完成之后，将恢复中断之前的运行环境，从而被中断的代码将继续运行。 NMI 中断是一类特殊的中断，往往预示着系统发生了不可恢复的错误，所以在正常运行的操作系统中，NMI 中断是不会被禁止的，但是在进入保护模式之前，由于特殊需求，代码禁止了这类中断。 现在让我们回到上面的代码，在 NMI 中断被禁止之后（通过写 0x80 进 CMOS 地址寄存器 0x70 ），函数接着调用了 io_delay 函数进行了短暂的延时以等待 I/O 操作完成。下面就是 io_delay 函数的实现： 12345static inline void io_delay(void)&#123; const u16 DELAY_PORT = 0x80; asm volatile(&quot;outb %%al,%0&quot; : : &quot;dN&quot; (DELAY_PORT));&#125; 对 I/O 端口 0x80 写入任何的字节都将得到 1 ms 的延时。在上面的代码中，代码将 al 寄存器中的值写到了这个端口。在这个 io_delay 调用完成之后， realmode_switch_hook 函数就完成了所有工作，下面让我们进入下一个函数。 下一个函数调用是 enable_a20，这个函数使能 A20 line，你可以在 arch/x86/boot/a20.c找到这个函数的定义，这个函数会尝试使用不同的方式来使能 A20 地址线。首先这个函数将调用 a20_test_short（该函数将调用 a20_test 函数） 来检测 A20 地址线是否已经被激活了: 123456789101112131415161718192021static int a20_test(int loops)&#123; int ok = 0; int saved, ctr; set_fs(0x0000); set_gs(0xffff); saved = ctr = rdfs32(A20_TEST_ADDR); while (loops--) &#123; wrfs32(++ctr, A20_TEST_ADDR); io_delay(); /* Serialize and make delay constant */ ok = rdgs32(A20_TEST_ADDR+0x10) ^ ctr; if (ok) break; &#125; wrfs32(saved, A20_TEST_ADDR); return ok;&#125; 这个函数首先将 0x0000 放入 FS 寄存器，将 0xffff 放入 GS 寄存器。然后通过 rdfs32 函数调用，将 A20_TEST_ADDR 内存地址的内容放入 saved 和 ctr 变量。 接下来我们使用 wrfs32 函数将更新过的 ctr 的值写入 fs:gs ，接着延时 1ms， 然后从 GS:A20_TEST_ADDR+0x10 读取内容，如果该地址内容不为0，那么 A20 已经被激活。如果 A20 没有被激活，代码将尝试使用多种方法进行 A20 地址激活。其中的一种方法就是调用 BIOS 0X15 中断激活 A20 地址线。 如果 enabled_a20 函数调用失败，显示一个错误消息并且调用 die 函数结束操作系统运行。die 函数定义在 arch/x86/boot/header.S 1234die: hlt jmp die .size die, .-die A20 地址线被激活之后，reset_coprocessor 函数被调用： 12outb(0, 0xf0);outb(0, 0xf1); 这个函数非常简单，通过将 0 写入 I/O 端口 0xf0 和 0xf1 以复位数字协处理器。 接下来 mask_all_interrupts 函数将被调用： 12outb(0xff, 0xa1); /* Mask all interrupts on the secondary PIC */outb(0xfb, 0x21); /* Mask all but cascade on the primary PIC */ 这个函数调用屏蔽了从中断控制器 (注：中断控制器的原文是 Programmable Interrupt Controller) 的所有中断，和主中断控制器上除IRQ2以外的所有中断（IRQ2是主中断控制器上的级联中断，所有从中断控制器的中断将通过这个级联中断报告给 CPU ）。 到这里位置，我们就完成了所有的准备工作，下面我们就将正式开始从实模式转换到保护模式。 设置中断描述符表 现在内核将调用 setup_idt 方法来设置中断描述符表（ IDT ）： 12345static void setup_idt(void)&#123; static const struct gdt_ptr null_idt = &#123;0, 0&#125;; asm volatile(&quot;lidtl %0&quot; : : &quot;m&quot; (null_idt));&#125; 上面的代码使用 lidtl 指令将 null_idt 所指向的中断描述符表引入寄存器 IDT。由于 null_idt 没有设定中断描述符表的长度（长度为 0 ），所以这段指令执行之后，实际上没有任何中断调用被设置成功（所有中断调用都是空的）。null_idt 是一个 gdt_ptr 结构的数据，这个结构的定义如下所示： 1234struct gdt_ptr &#123; u16 len; u32 ptr;&#125; __attribute__((packed)); 在上面的定义中，我们可以看到上面这个结构包含一个 16 bit 的长度字段，和一个 32 bit 的指针字段。__attribute__((packed)) 意味着这个结构就只包含 48 bit 信息（没有字节对齐优化）。 设置全局描述符表 在设置完中断描述符表之后，我们将使用 setup_gdt 函数来设置全局描述符表。在 setup_gdt 函数中，使用 boot_gdt 数组定义了需要引入 GDTR 寄存器的段描述符信息： 123456static const u64 boot_gdt[] __attribute__((aligned(16))) = &#123; [GDT_ENTRY_BOOT_CS] = GDT_ENTRY(0xc09b, 0, 0xfffff), [GDT_ENTRY_BOOT_DS] = GDT_ENTRY(0xc093, 0, 0xfffff), [GDT_ENTRY_BOOT_TSS] = GDT_ENTRY(0x0089, 4096, 103),&#125;; 在上面的 boot_gdt 数组中，我们定义了代码，数据和 TSS 段(Task State Segment, 任务状态段)的段描述符，因为我们并没有设置任何的中断调用（记得上面说的 null_idt吗？），所以 TSS 段并不会被使用到。TSS 段存在的唯一目的就是让 Intel 处理器能够正确进入保护模式。下面让我们详细了解一下 boot_gdt 这个数组，首先，这个数组被 __attribute__((aligned(16))) 修饰，这就意味着这个数组将以 16 字节为单位对齐。 因为在 boot_gdt 的定义中， GDT_ENTRY_BOOT_CS = 2，所以在数组中有2个空项，第一项是一个空的描述符，第二项在代码中没有使用。在没有 align 16 之前，整个结构占用了（8*5=40）个字节，加了 align 16 之后，结构就占用了 48 字节 。 上面代码中出现的 GDT_ENTRY 是一个宏定义，这个宏接受 3 个参数（标志，基地址，段长度）来产生段描述符结构。让我们来具体分析上面数组中的代码段描述符（ GDT_ENTRY_BOOT_CS ）来看看这个宏是如何工作的，对于这个段，GDT_ENTRY 接受了下面 3 个参数： 基地址 - 0 段长度 - 0xfffff 标志 - 0xc09b 上面这些数字表明，这个段的基地址是 0， 段长度是 0xfffff （ 1 MB ），而标志字段展开之后是下面的二进制数据： 11100 0000 1001 1011 这些二进制数据的具体含义如下: 1 - (G) 这里为 1，表示段的实际长度是 0xfffff * 4kb = 4GB 1 - (D) 表示这个段是一个32位段 0 - (L) 这个代码段没有运行在 long mode 0 - (AVL) Linux 没有使用 0000 - 段长度的4个位 1 - (P) 段已经位于内存中 00 - (DPL) - 段优先级为0 1 - (S) 说明这个段是一个代码或者数据段 101 - 段类型为可执行/可读 1 - 段可访问 你也可以阅读 Intel® 64 and IA-32 Architectures Software Developer’s Manuals 3A获取全部信息。 在定义了数组之后，代码将获取 GDT 的长度： 1gdt.len = sizeof(boot_gdt)-1; 接下来是将 GDT 的地址放入 gdt.ptr 中： 1gdt.ptr = (u32)&amp;boot_gdt + (ds() &lt;&lt; 4); 这里的地址计算很简单，因为我们还在实模式，所以就是 （ ds &lt;&lt; 4 + 数组起始地址）。 最后通过执行 lgdtl 指令将 GDT 信息写入 GDTR 寄存器： 1asm volatile(&quot;lgdtl %0&quot; : : &quot;m&quot; (gdt)); 切换进入保护模式 go_to_protected_mode 函数在完成 IDT, GDT 初始化，并禁止了 NMI 中断之后，将调用 protected_mode_jump 函数完成从实模式到保护模式的跳转： 1protected_mode_jump(boot_params.hdr.code32_start, (u32)&amp;boot_params + (ds() &lt;&lt; 4)); protected_mode_jump 函数定义在 arch/x86/boot/pmjump.S，它接受下面2个参数: 保护模式代码的入口 boot_params 结构的地址 第一个参数保存在 eax 寄存器，而第二个参数保存在 edx 寄存器。 代码首先在 boot_params 地址放入 esi 寄存器，然后将 cs 寄存器内容放入 bx 寄存器，接着执行 bx &lt;&lt; 4 + 标号为2的代码的地址，这样一来 bx 寄存器就包含了标号为2的代码的地址。接下来代码将把数据段索引放入 cx 寄存器，将 TSS 段索引放入 di 寄存器： 12movw $__BOOT_DS, %cxmovw $__BOOT_TSS, %di 就像前面我们看到的 GDT_ENTRY_BOOT_CS 的值为2，每个段描述符都是 8 字节，所以 cx 寄存器的值将是 2*8 = 16，di 寄存器的值将是 4*8 =32 接下来，我们通过设置 CR0 寄存器相应的位使 CPU 进入保护模式： 123movl %cr0, %edxorb $X86_CR0_PE, %dlmovl %edx, %cr0 在进入保护模式之后，通过一个长跳转进入 32 位代码： 123 .byte 0x66, 0xea2: .long in_pm32 .word __BOOT_CS ;(GDT_ENTRY_BOOT_CS*8) = 16，段描述符表索引 这段代码中 0x66 操作符前缀允许我们混合执行 16 位和 32 位代码 0xea - 跳转指令的操作符 in_pm32 跳转地址偏移 __BOOT_CS 代码段描述符索引 在执行了这个跳转命令之后，我们就在保护模式下执行代码了： 12.code32.section &quot;.text32&quot;,&quot;ax&quot; 保护模式代码的第一步就是重置所有的段寄存器（除了 CS 寄存器）: 123456GLOBAL(in_pm32)movl %ecx, %dsmovl %ecx, %esmovl %ecx, %fsmovl %ecx, %gsmovl %ecx, %ss 还记得我们在实模式代码中将 $__BOOT_DS （数据段描述符索引）放入了 cx 寄存器，所以上面的代码设置所有段寄存器（除了 CS 寄存器）指向数据段。接下来代码将所有的通用寄存器清 0 ： 12345xorl %ecx, %ecxxorl %edx, %edxxorl %ebx, %ebxxorl %ebp, %ebpxorl %edi, %edi 最后使用长跳转跳入正在的 32 位代码（通过参数传入的地址） 1jmpl *%eax ; 到这里，我们就进入了保护模式开始执行代码了 切换到64位模式 32位入口点 回忆一下， eax 寄存器包含了 32 位入口点的地址。我们可以在 x86 linux 内核引导协议 中找到相关内容： 1When using bzImage, the protected-mode kernel was relocated to 0x100000 我们可以在汇编源码 arch/x86/boot/compressed/head_64.S 中找到 32 位入口点的定义。 1234567__HEAD .code32ENTRY(startup_32)............ENDPROC(startup_32) 首先，为什么目录名叫做 被压缩的 (compressed) ？实际上 bzimage 是由 vmlinux + 头文件 + 内核启动代码 被 gzip 压缩之后获得的。所以， head_64.S 的主要目的就是做好进入长模式的准备之后进入长模式，进入以后再解压内核。 在 arch/x86/boot/compressed 目录下有两个文件： head_32.S head_64.S 让我们看一下 arch/x86/boot/compressed/Makefile。在那里我们可以看到以下目标： 123vmlinux-objs-y := $(obj)/vmlinux.lds $(obj)/head_$(BITS).o $(obj)/misc.o \\ $(obj)/string.o $(obj)/cmdline.o \\ $(obj)/piggy.o $(obj)/cpuflags.o 注意 $(obj)/head_$(BITS).o 。这意味着我们将会选择基于 $(BITS) 所设置的文件执行链接操作，即 head_32.o 或者 head_64.o。$(BITS) 在 arch/x86/Makefile之中根据 .config 文件另外定义： 123456789ifeq ($(CONFIG_X86_32),y) BITS := 32 ... ...else BITS := 64 ... ...endif 必要时加载内存段寄存器 正如上面阐述的，我们先从 arch/x86/boot/compressed/head_64.S这个汇编文件开始。首先我们看到了在 startup_32 之前的特殊段属性定义： 123 __HEAD .code32ENTRY(startup_32) 这个 __HEAD 是一个定义在头文件 include/linux/init.h 中的宏，展开后就是下面这个段的定义： 1#define __HEAD .section &quot;.head.text&quot;,&quot;ax&quot; 其拥有 .head.text 的命名和 ax 标记。在这里，这些标记告诉我们这个段是可执行的或者换种说法，包含了代码。我们可以在 arch/x86/boot/compressed/vmlinux.lds.S这个链接脚本里找到这个段的定义： 12345678SECTIONS&#123; . = 0; .head.text : &#123; _head = . ; HEAD_TEXT _ehead = . ; &#125; 如果你不熟悉 GNU LD 这个链接脚本语言的语法，你可以在这个文档中找到更多信息。简单来说，这个 . 符号是一个链接器的特殊变量 - 位置计数器。其被赋值为相对于该段的偏移。在这里，我们将位置计数器赋值为0，这意味着我们的代码被链接到内存的 0 偏移处。此外，我们可以从注释里找到更多信息： 1Be careful parts of head_64.S assume startup_32 is at address 0. 好了，现在我们知道我们在哪里了，接下来就是深入 startup_32 函数的最佳时机。 在 startup_32 函数的开始，我们可以看到 cld 指令将标志寄存器的 DF （方向标志）位清空。当方向标志被清空，所有的串操作指令像stos， scas等等将会增加索引寄存器 esi 或者 edi 的值。我们需要清空方向标志是因为接下来我们会使用汇编的串操作指令来做为页表腾出空间等工作。 在我们清空 DF 标志后，下一步就是从内核加载头中的 loadflags 字段来检查 KEEP_SEGMENTS 标志。这些标记在 linux 的引导协议文档中有描述： 123456 1Bit 6 (write): KEEP_SEGMENTS Protocol: 2.07+ - If 0, reload the segment registers in the 32bit entry point. - If 1, do not reload the segment registers in the 32bit entry point. Assume that %cs %ds %ss %es are all set to flat segments witha base of 0 (or the equivalent for their environment). 所以，如果 KEEP_SEGMENTS 位在 loadflags 中没有被设置，我们需要重置 ds , ss 和 es 段寄存器到一个基地址为 0 的普通段中。如下： 12345678 testb $(1 &lt;&lt; 6), BP_loadflags(%esi)jnz 1fclimovl $(__BOOT_DS), %eaxmovl %eax, %dsmovl %eax, %esmovl %eax, %ss 记住 __BOOT_DS 是 0x18 （位于全局描述符表中数据段的索引）。如果设置了 KEEP_SEGMENTS ，我们就跳转到最近的 1f 标签，或者当没有 1f 标签，则用 __BOOT_DS 更新段寄存器。这非常简单，但是这是一个有趣的操作。我们在 arch/x86/boot/pmjump.S 中切换到保护模式的时候已经更新了这些段寄存器。那么为什么我们还要去关心这些段寄存器的值呢？答案很简单，Linux 内核也有32位的引导协议，如果一个引导程序之前使用32位协议引导内核，那么在 startup_32 之前的代码就会被忽略。在这种情况下 startup_32 将会变成引导程序之后的第一个入口点，不保证段寄存器会不会处于未知状态。 在我们检查了 KEEP_SEGMENTS 标记并且给段寄存器设置了正确的值之后，下一步就是计算我们代码的加载和编译运行之间的位置偏差了。记住 setup.ld.S 包含了以下定义：在 .head.text 段的开始 . = 0 。这意味着这一段代码被编译成从 0 地址运行。我们可以在 objdump 工具的输出中看到： 12345678arch/x86/boot/compressed/vmlinux: file format elf64-x86-64Disassembly of section .head.text:0000000000000000 &lt;startup_32&gt;: 0: fc cld 1: f6 86 11 02 00 00 40 testb $0x40,0x211(%rsi) objdump 工具告诉我们 startup_32 的地址是 0 。但实际上并不是。我们当前的目标是获知我们实际上在哪里。在长模式下，这非常简单，因为其支持 rip 相对寻址，但是我们当前处于保护模式下。我们将会使用一个常用的方法来确定 startup_32 的地址。我们需要定义一个标签并且跳转到它，然后把栈顶抛出到一个寄存器中： 12call labellabel: pop %reg 在这之后，那个寄存器将会包含标签的地址，让我们看看在 Linux 内核中类似的寻找 startup_32 地址的代码： 1234 leal (BP_scratch+4)(%esi), %esp call 1f1: popl %ebp subl $1b, %ebp esi 寄存器包含了bootparams 结构的地址，这个结构在我们切换到保护模式之前已经被填充了。bootparams 这个结构体包含了一个特殊的字段 scratch ，其偏移量为 0x1e4 。这个 4 字节的区域将会成为 call 指令的临时栈。我们把 scratch 的地址加 4 存入 esp 寄存器。我们之所以在 BP_scratch 基础上加 4 是因为，如之前所说的，这将成为一个临时的栈，而在 x86_64 架构下，栈是自顶向下生长的。所以我们的栈指针就会指向栈顶。接下来我们就可以看到我上面描述的过程。我们跳转到 1f 标签并且把该标签的地址放入 ebp 寄存器，因为在执行 call 指令之后我们把返回地址放到了栈顶。那么，目前我们拥有 1f 标签的地址，也能够很容易得到 startup_32 的地址。我们只需要把我们从栈里得到的地址减去标签的地址： 12345678910111213startup_32 (0x0) +-----------------------+ | | | | | | | | | | | | | | | |1f (0x0 + 1f offset) +-----------------------+ %ebp - 实际物理地址 | | | | +-----------------------+ startup_32 被链接为在 0x0 地址运行，这意味着 1f 的地址为 0x0 + 1f 的偏移量 。实际上偏移量大概是 0x22 字节。 ebp 寄存器包含了 1f 标签的实际物理地址。所以如果我们从 ebp 中减去 1f ，我们就会得到 startup_32 的实际物理地址。Linux 内核的引导协议描述了保护模式下的内核基地址是 0x100000 。我们可以用 gdb 来验证。 startup_32 的地址是 0x100000 。在我们知道了 startup_32 的地址之后，我们可以开始准备切换到长模式了。 栈的建立和 CPU 的确认 如果不知道 startup_32 标签的地址，我们就无法建立栈。我们可以把栈看作是一个数组，并且栈指针寄存器 esp 必须指向数组的底部。当然我们可以在自己的代码里定义一个数组，但是我们需要知道其真实地址来正确配置栈指针。让我们看一下代码： 123 movl $boot_stack_end, %eaxaddl %ebp, %eaxmovl %eax, %esp boots_stack_end 标签被定义在同一个汇编文件 arch/x86/boot/compressed/head_64.S中，位于 .bss 段： 1234567 .bss .balign 4boot_heap: .fill BOOT_HEAP_SIZE, 1, 0boot_stack: .fill BOOT_STACK_SIZE, 1, 0boot_stack_end: 首先，我们把 boot_stack_end 放到 eax 寄存器中。那么 eax 寄存器将包含 boot_stack_end 链接后的地址或者说 0x0 + boot_stack_end 。为了得到 boot_stack_end 的实际地址，我们需要加上 startup_32 的实际地址。回忆一下，前面我们找到了这个地址并且把它存到了 ebp 寄存器中。最后，eax 寄存器将会包含 boot_stack_end 的实际地址，我们只需要将其加到栈指针上。 在外面建立了栈之后，下一步是 CPU 的确认。既然我们将要切换到 长模式 ，我们需要检查 CPU 是否支持 长模式 和 SSE。我们将会在跳转到 verify_cpu 函数之后执行 123call verify_cputestl %eax, %eaxjnz no_longmode 这个函数定义在 arch/x86/kernel/verify_cpu.S 中，只是包含了几个对 cpuid 指令的调用。该指令用于获取处理器的信息。在我们的情况下，它检查了对 长模式 和 SSE 的支持，通过 eax 寄存器返回0表示成功，1表示失败。 如果 eax 的值不是 0 ，我们就跳转到 no_longmode 标签，用 hlt 指令停止 CPU ，期间不会发生硬件中断： 1234no_longmode:1: hlt jmp 1b 如果 eax 的值为0，万事大吉，我们可以继续。 计算重定位地址 下一步是在必要的时候计算解压缩之后的地址。首先，我们需要知道内核重定位的意义。我们已经知道 Linux 内核的32位入口点地址位于 0x100000 。但是那是一个32位的入口。默认的内核基地址由内核配置项 CONFIG_PHYSICAL_START 的值所确定，其默认值为 0x1000000 或 16 MB 。这里的主要问题是如果内核崩溃了，内核开发者需要一个配置于不同地址加载的 救援内核 来进行 kdump。Linux 内核提供了特殊的配置选项以解决此问题 - CONFIG_RELOCATABLE 。我们可以在内核文档中找到： 123456This builds a kernel image that retains relocation informationso it can be loaded someplace besides the default 1MB.Note: If CONFIG_RELOCATABLE=y, then the kernel runs from the addressit has been loaded at and the compile time physical address(CONFIG_PHYSICAL_START) is used as the minimum location. 简单来说，这意味着相同配置下的 Linux 内核可以从不同地址被启动。这是通过将程序以 位置无关代码 的形式编译来达到的。如果我们参考 /arch/x86/boot/compressed/Makefile，我们将会看到解压器的确是用 -fPIC 标记编译的： 1KBUILD_CFLAGS += -fno-strict-aliasing -fPIC 当我们使用位置无关代码时，一段代码的地址是由一个控制地址加上程序计数器计算得到的。我们可以从任意一个地址加载使用这种方式寻址的代码。这就是为什么我们需要获得 startup_32 的实际地址。现在让我们回到 Linux 内核代码。我们目前的目标是计算出内核解压的地址。这个地址的计算取决于内核配置项 CONFIG_RELOCATABLE 。让我们看代码： 12345678910111213#ifdef CONFIG_RELOCATABLE movl %ebp, %ebx movl BP_kernel_alignment(%esi), %eax decl %eax addl %eax, %ebx notl %eax andl %eax, %ebx cmpl $LOAD_PHYSICAL_ADDR, %ebx jge 1f#endif movl $LOAD_PHYSICAL_ADDR, %ebx1: addl $z_extract_offset, %ebx 记住 ebp 寄存器的值就是 startup_32 标签的物理地址。如果在内核配置中 CONFIG_RELOCATABLE 内核配置项开启，我们就把这个地址放到 ebx 寄存器中，对齐到 2M 的整数倍 ，然后和 LOAD_PHYSICAL_ADDR 的值比较。 LOAD_PHYSICAL_ADDR 宏定义在头文件 arch/x86/include/asm/boot.h中，如下： 123#define LOAD_PHYSICAL_ADDR ((CONFIG_PHYSICAL_START \\ + (CONFIG_PHYSICAL_ALIGN - 1)) \\ &amp; ~(CONFIG_PHYSICAL_ALIGN - 1)) 我们可以看到该宏只是展开成对齐的 CONFIG_PHYSICAL_ALIGN 值，其表示了内核加载位置的物理地址。在比较了 LOAD_PHYSICAL_ADDR 和 ebx 的值之后，我们给 startup_32 加上偏移来获得解压内核镜像的地址。如果 CONFIG_RELOCATABLE 选项在内核配置时没有开启，我们就直接将默认的地址加上 z_extract_offset 。 在前面的操作之后，ebp 包含了我们加载时的地址，ebx 被设为内核解压缩的目标地址。 进入长模式前的准备工作 在我们得到了重定位内核镜像的基地址之后，我们需要做切换到64位模式之前的最后准备。首先，我们需要更新全局描述符表： 123 leal gdt(%ebp), %eaxmovl %eax, gdt+2(%ebp)lgdt gdt(%ebp) 在这里我们把 ebp 寄存器加上 gdt 的偏移存到 eax 寄存器。接下来我们把这个地址放到 ebp 加上 gdt+2 偏移的位置上，并且用 lgdt 指令载入 全局描述符表 。为了理解这个神奇的 gdt 偏移量，我们需要关注 全局描述符表 的定义。我们可以在同一个源文件中找到其定义： 1234567891011 .datagdt: .word gdt_end - gdt .long gdt .word 0 .quad 0x0000000000000000 /* NULL descriptor */ .quad 0x00af9a000000ffff /* __KERNEL_CS */ .quad 0x00cf92000000ffff /* __KERNEL_DS */ .quad 0x0080890000000000 /* TS descriptor */ .quad 0x0000000000000000 /* TS continued */gdt_end: 我们可以看到其位于 .data 段，并且包含了5个描述符： null 、内核代码段、内核数据段和其他两个任务描述符。将描述符改为 CS.L = 1 CS.D = 0 从而在 64 位模式下执行。我们可以看到， gdt 的定义从两个字节开始： gdt_end - gdt ，代表了 gdt 表的最后一个字节，或者说表的范围。接下来的4个字节包含了 gdt 的基地址。记住 全局描述符表 保存在 48位 GDTR-全局描述符表寄存器 中，由两个部分组成： 全局描述符表的大小 (16位） 全局描述符表的基址 (32位) 所以，我们把 gdt 的地址放到 eax 寄存器，然后存到 .long gdt 或者 gdt+2。现在我们已经建立了 GDTR 寄存器的结构，并且可以用 lgdt 指令载入 全局描述符表 了。 在我们载入 全局描述符表 之后，我们必须启用 PAE 模式。方法是将 cr4 寄存器的值传入 eax ，将第5位置1，然后再写回 cr4 。 123 movl %cr4, %eaxorl $X86_CR4_PAE, %eaxmovl %eax, %cr4 现在我们已经接近完成进入64位模式前的所有准备工作了。最后一步是建立页表. 长模式 长模式是 x86_64 系列处理器的原生模式。首先让我们看一看 x86_64 和 x86 的一些区别。 64位 模式提供了一些新特性，比如： 从 r8 到 r15 8个新的通用寄存器，并且所有通用寄存器都是64位的了。 64位指令指针 - RIP ; 新的操作模式 - 长模式; 64位地址和操作数; RIP 相对寻址 . 长模式是一个传统保护模式的扩展，其由两个子模式构成： 64位模式 兼容模式 为了切换到 64位 模式，我们需要完成以下操作： 启用 PAE; 建立页表并且将顶级页表的地址放入 cr3 寄存器; 启用 EFER.LME ; 启用分页; 我们已经通过设置 cr4 控制寄存器中的 PAE 位启动 PAE 了。 初期页表初始化 Linux 内核使用 4级 页表，通常我们会建立6个页表： 1 个 PML4 或称为 4级页映射 表，包含 1 个项； 1 个 PDP 或称为 页目录指针 表，包含 4 个项； 4 个 页目录表，一共包含 2048 个项； 让我们看看其实现方式。首先我们在内存中为页表清理一块缓存。每个表都是 4096 字节，所以我们需要 24 KB 的空间： 1234 leal pgtable(%ebx), %edixorl %eax, %eaxmovl $((4096*6)/4), %ecxrep stosl 我们把和 ebx 相关的 pgtable 的地址放到 edi 寄存器中，清空 eax 寄存器，并将 ecx 赋值为 6144 。 rep stosl 指令将会把 eax 的值写到 edi 指向的地址，然后给 edi 加 4 ， ecx 减 4 ，重复直到 ecx 小于等于 0 。所以我们才把 6144 赋值给 ecx 。 pgtable 定义在 arch/x86/boot/compressed/head_64.S的最后： 1234 .section &quot;.pgtable&quot;,&quot;a&quot;,@nobits .balign 4096pgtable: .fill 6*4096, 1, 0 我们可以看到，其位于 .pgtable 段，大小为 24KB 。 在我们为 pgtable 分配了空间之后，我们可以开始构建顶级页表 - PML4 ： 123 leal pgtable + 0(%ebx), %edileal 0x1007 (%edi), %eaxmovl %eax, 0(%edi) 还是在这里，我们把和 ebx 相关的，或者说和 startup_32 相关的 pgtable 的地址放到 edi 寄存器。接下来我们把相对此地址偏移 0x1007 的地址放到 eax 寄存器中。 0x1007 是 PML4 的大小 4096 加上 7 。这里的 7 代表了 PML4 的项标记。在我们这里，这些标记是 PRESENT+RW+USER 。在最后我们把第一个 PDP（页目录指针） 项的地址写到 PML4 中。 在接下来的一步，我们将会在 页目录指针（PDP） 表（3级页表）建立 4 个带有 PRESENT+RW+USE 标记的 Page Directory （2级页表） 项： 12345678 leal pgtable + 0x1000(%ebx), %edi leal 0x1007(%edi), %eax movl $4, %ecx1: movl %eax, 0x00(%edi) addl $0x00001000, %eax addl $8, %edi decl %ecx jnz 1b 我们把 3 级页目录指针表的基地址（从 pgtable 表偏移 4096 或者 0x1000 ）放到 edi ，把第一个 2 级页目录指针表的首项的地址放到 eax 寄存器。把 4 赋值给 ecx 寄存器，其将会作为接下来循环的计数器，然后将第一个页目录指针项写到 edi 指向的地址。之后， edi 将会包含带有标记 0x7 的第一个页目录指针项的地址。接下来我们就计算后面的几个页目录指针项的地址，每个占 8 字节，把地址赋值给 eax ，然后回到循环开头将其写入 edi 所在地址。建立页表结构的最后一步就是建立 2048 个 2MB 页的页表项。 12345678 leal pgtable + 0x2000(%ebx), %edi movl $0x00000183, %eax movl $2048, %ecx1: movl %eax, 0(%edi) addl $0x00200000, %eax addl $8, %edi decl %ecx jnz 1b 在这里我们做的几乎和上面一样，所有的表项都带着标记 - $0x00000183 - PRESENT + WRITE + MBZ 。最后我们将会拥有 2048 个 2MB 大的页，或者说：一个4G页表，现在我们可以把高级页表 PML4 的地址放到 cr3 寄存器中了： 12 leal pgtable(%ebx), %eaxmovl %eax, %cr3 这样就全部结束了。所有的准备工作都已经完成，我们可以开始看如何切换到长模式了。 切换到长模式 首先我们需要设置 MSR 中的 EFER.LME 标记为 0xC0000080 ： 1234 movl $MSR_EFER, %ecxrdmsrbtsl $_EFER_LME, %eaxwrmsr 在这里我们把 MSR_EFER 标记（在 arch/x86/include/uapi/asm/msr-index.h 中定义）放到 ecx 寄存器中，然后调用 rdmsr 指令读取 MSR 寄存器。在 rdmsr 执行之后，我们将会获得 edx:eax 中的结果值，其取决于 ecx 的值。我们通过 btsl 指令检查 EFER_LME 位，并且通过 wrmsr 指令将 eax 的数据写入 MSR 寄存器。 下一步我们将内核段代码地址入栈（我们在 GDT 中定义了），然后将 startup_64 的地址导入 eax 。 12 pushl $__KERNEL_CSleal startup_64(%ebp), %eax 在这之后我们把这个地址入栈然后通过设置 cr0 寄存器中的 PG 和 PE 启用分页： 12 movl $(X86_CR0_PG | X86_CR0_PE), %eaxmovl %eax, %cr0 然后执行： 1lret 记住前一步我们已经将 startup_64 函数的地址入栈，在 lret 指令之后，CPU 取出了其地址跳转到那里 这些步骤之后我们最后来到了64位模式： 123456.code64 .org 0x200ENTRY(startup_64)............ 内核解压 我们停在了跳转到64位入口点——startup_64的跳转之前，它在源文件 arch/x86/boot/compressed/head_64.S里面。 12345678910 pushl $__KERNEL_CSleal startup_64(%ebp), %eax.........pushl %eax.........lret 由于我们加载了新的全局描述符表并且在其他模式有CPU的模式转换（在我们这里是64位模式），我们可以在startup_64的开头看到数据段的建立： 123456789 .code64 .org 0x200ENTRY(startup_64) xorl %eax, %eax movl %eax, %ds movl %eax, %es movl %eax, %ss movl %eax, %fs movl %eax, %gs 除cs之外的段寄存器在我们进入长模式时已经重置。 下一步是计算内核编译时的位置和它被加载的位置的差： 123456789101112131415#ifdef CONFIG_RELOCATABLE leaq startup_32(%rip), %rbp movl BP_kernel_alignment(%rsi), %eax decl %eax addq %rax, %rbp notq %rax andq %rax, %rbp cmpq $LOAD_PHYSICAL_ADDR, %rbp jge 1f#endif movq $LOAD_PHYSICAL_ADDR, %rbp1: movl BP_init_size(%rsi), %ebx subl $_end, %ebx addq %rbp, %rbx rbp包含了解压后内核的起始地址，在这段代码执行之后rbx会包含用于解压的重定位内核代码的地址。我们已经在startup_32看到类似的代码，但是我们需要再做这个计算，因为引导加载器可以用64位引导协议，而startup_32在这种情况下不会执行。 下一步，我们可以看到栈指针的设置和标志寄存器的重置： 1234 leaq boot_stack_end(%rbx), %rsppushq $0popfq 如上所述，rbx寄存器包含了内核解压代码的起始地址，我们把这个地址的boot_stack_entry偏移地址相加放到表示栈顶指针的rsp寄存器。在这一步之后，栈就是正确的。你可以在汇编源码文件 arch/x86/boot/compressed/head_64.S的末尾找到boot_stack_end的定义： 1234567 .bss .balign 4boot_heap: .fill BOOT_HEAP_SIZE, 1, 0boot_stack: .fill BOOT_STACK_SIZE, 1, 0boot_stack_end: 它在.bss节的末尾，就在.pgtable前面。如果你查看 arch/x86/boot/compressed/vmlinux.lds.S 链接脚本，你会找到.bss和.pgtable的定义。 由于我们设置了栈，在我们计算了解压了的内核的重定位地址后，我们可以复制压缩了的内核到以上地址。 123456789 pushq %rsileaq (_bss-8)(%rip), %rsileaq (_bss-8)(%rbx), %rdimovq $_bss, %rcxshrq $3, %rcxstdrep movsqcldpopq %rsi 首先我们把rsi压进栈。我们需要保存rsi的值，因为这个寄存器现在存放指向boot_params的指针，这是包含引导相关数据的实模式结构体。在代码的结尾，我们会重新恢复指向boot_params的指针到rsi. 接下来两个leaq指令用_bss - 8偏移和rip和rbx计算有效地址并存放到rsi和rdi. 我们为什么要计算这些地址？实际上，压缩了的代码镜像存放在这份复制了的代码（从startup_32到当前的代码）和解压了的代码之间。你可以通过查看链接脚本 arch/x86/boot/compressed/vmlinux.lds.S 验证 123456789101112131415. = 0; .head.text : &#123; _head = . ; HEAD_TEXT _ehead = . ; &#125; .rodata..compressed : &#123; *(.rodata..compressed) &#125; .text : &#123; _text = .; /* Text */ *(.text) *(.text.*) _etext = . ; &#125; 注意.head.text节包含了startup_32. 你可以从之前的部分回忆起它： 123456 __HEAD .code32ENTRY(startup_32)......... .text节包含解压代码： 123456789 .textrelocated:........./* * Do the decompression, and jump to the new kernel.. */... .rodata..compressed包含了压缩了的内核镜像。所以rsi包含_bss - 8的绝对地址，rdi包含_bss - 8的重定位的相对地址。在我们把这些地址放入寄存器时，我们把_bss的地址放到了rcx寄存器。正如你在vmlinux.lds.S链接脚本中看到了一样，它和设置/内核代码一起在所有节的末尾。现在我们可以开始用movsq指令每次8字节地从rsi到rdi复制代码。 注意在数据复制前有std指令：它设置DF标志，意味着rsi和rdi会递减。换句话说，我们会从后往前复制这些字节。最后，我们用cld指令清除DF标志，并恢复boot_params到rsi. 现在我们有.text节的重定位后的地址，我们可以跳到那里： 12 leaq relocated(%rbx), %raxjmp *%rax 在内核解压前的最后准备 在上一段我们看到了.text节从relocated标签开始。它做的第一件事是清空.bss节： 123456 xorl %eax, %eaxleaq _bss(%rip), %rdileaq _ebss(%rip), %rcxsubq %rdi, %rcxshrq $3, %rcxrep stosq 我们要初始化.bss节，因为我们很快要跳转到C代码。这里我们就清空eax，把_bss的地址放到rdi，把_ebss放到rcx，然后用rep stosq填零。 最后，我们可以调用extract_kernel函数： 123456789 pushq %rsimovq %rsi, %rdileaq boot_heap(%rip), %rsileaq input_data(%rip), %rdxmovl $z_input_len, %ecxmovq %rbp, %r8movq $z_output_len, %r9call extract_kernelpopq %rsi 我们再一次设置rdi为指向boot_params结构体的指针并把它保存到栈中。同时我们设置rsi指向用于内核解压的区域。最后一步是准备extract_kernel的参数并调用这个解压内核的函数。extract_kernel函数在 arch/x86/boot/compressed/misc.c源文件定义并有六个参数： rmode - 指向 boot_params 结构体的指针，boot_params被引导加载器填充或在早期内核初始化时填充 heap - 指向早期启动堆的起始地址 boot_heap 的指针 input_data - 指向压缩的内核，即 arch/x86/boot/compressed/vmlinux.bin.bz2 的指针 input_len - 压缩的内核的大小 output - 解压后内核的起始地址 output_len - 解压后内核的大小 所有参数根据 System V Application Binary Interface 通过寄存器传递。我们已经完成了所有的准备工作，现在我们可以看内核解压的过程。 内核解压 就像我们在之前的段落中看到了那样，extract_kernel函数在源文件 arch/x86/boot/compressed/misc.c定义并有六个参数。正如我们在之前的部分看到的，这个函数从图形/控制台初始化开始。我们要再次做这件事，因为我们不知道我们是不是从实模式开始，或者是使用了引导加载器，或者引导加载器用了32位还是64位启动协议。 在最早的初始化步骤后，我们保存空闲内存的起始和末尾地址。 12free_mem_ptr = heap;free_mem_end_ptr = heap + BOOT_HEAP_SIZE; 在这里 heap 是我们在 arch/x86/boot/compressed/head_64.S 得到的 extract_kernel 函数的第二个参数： 1leaq boot_heap(%rip), %rsi 如上所述，boot_heap定义为： 12boot_heap: .fill BOOT_HEAP_SIZE, 1, 0 在这里BOOT_HEAP_SIZE是一个展开为0x10000(对bzip2内核是0x400000)的宏，代表堆的大小。 在堆指针初始化后，下一步是从 arch/x86/boot/compressed/kaslr.c 调用choose_random_location函数。我们可以从函数名猜到，它选择内核镜像解压到的内存地址。看起来很奇怪，我们要寻找甚至是选择内核解压的地址，但是Linux内核支持kASLR，为了安全，它允许解压内核到随机的地址。 现在我们回头看 misc.c. 在获得内核镜像的地址后，需要有一些检查以确保获得的随机地址是正确对齐的，并且地址没有错误： 1234567891011121314151617if ((unsigned long)output &amp; (MIN_KERNEL_ALIGN - 1)) error(&quot;Destination physical address inappropriately aligned&quot;);if (virt_addr &amp; (MIN_KERNEL_ALIGN - 1)) error(&quot;Destination virtual address inappropriately aligned&quot;);if (heap &gt; 0x3fffffffffffUL) error(&quot;Destination address too large&quot;);if (virt_addr + max(output_len, kernel_total_size) &gt; KERNEL_IMAGE_SIZE) error(&quot;Destination virtual address is beyond the kernel mapping area&quot;);if ((unsigned long)output != LOAD_PHYSICAL_ADDR) error(&quot;Destination address does not match LOAD_PHYSICAL_ADDR&quot;);if (virt_addr != LOAD_PHYSICAL_ADDR) error(&quot;Destination virtual address changed when not relocatable&quot;); 在所有这些检查后，我们可以看到熟悉的消息： 1Decompressing Linux... 然后调用解压内核的__decompress函数： 1__decompress(input_data, input_len, NULL, NULL, output, output_len, NULL, error); __decompress函数的实现取决于在内核编译期间选择什么压缩算法： 1234567891011121314151617181920212223#ifdef CONFIG_KERNEL_GZIP#include &quot;../../../../lib/decompress_inflate.c&quot;#endif#ifdef CONFIG_KERNEL_BZIP2#include &quot;../../../../lib/decompress_bunzip2.c&quot;#endif#ifdef CONFIG_KERNEL_LZMA#include &quot;../../../../lib/decompress_unlzma.c&quot;#endif#ifdef CONFIG_KERNEL_XZ#include &quot;../../../../lib/decompress_unxz.c&quot;#endif#ifdef CONFIG_KERNEL_LZO#include &quot;../../../../lib/decompress_unlzo.c&quot;#endif#ifdef CONFIG_KERNEL_LZ4#include &quot;../../../../lib/decompress_unlz4.c&quot;#endif 在内核解压之后，最后两个函数是parse_elf和handle_relocations.这些函数的主要用途是把解压后的内核移动到正确的位置。事实上，解压过程会原地解压，我们还是要把内核移动到正确的地址。我们已经知道，内核镜像是一个ELF可执行文件，所以parse_elf的主要目标是移动可加载的段到正确的地址。我们可以在readelf的输出看到可加载的段： 1234567891011121314151617readelf -l vmlinuxElf file type is EXEC (Executable file)Entry point 0x1000000There are 5 program headers, starting at offset 64Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flags Align LOAD 0x0000000000200000 0xffffffff81000000 0x0000000001000000 0x0000000000893000 0x0000000000893000 R E 200000 LOAD 0x0000000000a93000 0xffffffff81893000 0x0000000001893000 0x000000000016d000 0x000000000016d000 RW 200000 LOAD 0x0000000000c00000 0x0000000000000000 0x0000000001a00000 0x00000000000152d8 0x00000000000152d8 RW 200000 LOAD 0x0000000000c16000 0xffffffff81a16000 0x0000000001a16000 0x0000000000138000 0x000000000029b000 RWE 200000 parse_elf函数的目标是加载这些段到从choose_random_location函数得到的output地址。这个函数从检查ELF签名标志开始： 123456789101112Elf64_Ehdr ehdr;Elf64_Phdr *phdrs, *phdr;memcpy(&amp;ehdr, output, sizeof(ehdr));if (ehdr.e_ident[EI_MAG0] != ELFMAG0 || ehdr.e_ident[EI_MAG1] != ELFMAG1 || ehdr.e_ident[EI_MAG2] != ELFMAG2 || ehdr.e_ident[EI_MAG3] != ELFMAG3) &#123; error(&quot;Kernel is not a valid ELF file&quot;); return;&#125; 如果是无效的，它会打印一条错误消息并停机。如果我们得到一个有效的ELF文件，我们从给定的ELF文件遍历所有程序头，并用正确的地址复制所有可加载的段到输出缓冲区： 1234567891011121314151617for (i = 0; i &lt; ehdr.e_phnum; i++) &#123; phdr = &amp;phdrs[i]; switch (phdr-&gt;p_type) &#123; case PT_LOAD:#ifdef CONFIG_RELOCATABLE dest = output; dest += (phdr-&gt;p_paddr - LOAD_PHYSICAL_ADDR);#else dest = (void *)(phdr-&gt;p_paddr);#endif memmove(dest, output + phdr-&gt;p_offset, phdr-&gt;p_filesz); break; default: break; &#125; &#125; 这就是全部的工作。 从现在开始，所有可加载的段都在正确的位置。 在parse_elf函数之后是调用handle_relocations函数。这个函数的实现依赖于CONFIG_X86_NEED_RELOCS内核配置选项，如果它被启用，这个函数调整内核镜像的地址，只有在内核配置时启用了CONFIG_RANDOMIZE_BASE配置选项才会调用。handle_relocations函数的实现足够简单。这个函数从基准内核加载地址的值减掉LOAD_PHYSICAL_ADDR的值，从而我们获得内核链接后要加载的地址和实际加载地址的差值。在这之后我们可以进行内核重定位，因为我们知道内核加载的实际地址、它被链接的运行的地址和内核镜像末尾的重定位表。 在内核重定位后，我们从extract_kernel回来，到 arch/x86/boot/compressed/head_64.S 内核的地址在rax寄存器，我们跳到那里： 1jmp *%rax 就是这样。现在我们就在内核里！ 内核地址随机化 你可能还记得，Linux内核的入口点是 main.c 的start_kernel函数，它在LOAD_PHYSICAL_ADDR地址开始执行。这个地址依赖于CONFIG_PHYSICAL_START内核配置选项，默认为0x1000000: 12345678config PHYSICAL_START hex &quot;Physical address where the kernel is loaded&quot; if (EXPERT || CRASH_DUMP) default &quot;0x1000000&quot; ---help--- This gives the physical address where the kernel is loaded. ... ... ... 这个选项在内核配置时可以修改，但是加载地址可以选择为一个随机值。为此，CONFIG_RANDOMIZE_BASE内核配置选项在内核配置时应该启用。在这种情况下，Linux内核镜像解压和加载的物理地址会被随机化。 页表的初始化 在内核解压器要开始找随机的内核解压和加载地址之前，应该初始化恒等映射（identity mapped,虚拟地址和物理地址相同）页表。如果引导加载器使用16位或32位引导协议，那么我们已经有了页表。但在任何情况下，如果内核解压器选择它们之外的内存区域，我们需要新的页。这就是为什么我们需要建立新的恒等映射页表。是的，建立恒等映射页表是随机化加载地址的最早的步骤之一。 内核解压器的入口点——extract_kernel函数。随机化从调用这个函数开始： 123456void choose_random_location(unsigned long input, unsigned long input_size, unsigned long *output, unsigned long output_size, unsigned long *virt_addr)&#123;&#125; 你可以看到，这个函数有五个参数： input; input_size; output; output_isze; virt_addr. 让我们试着理解一下这些参数是什么。第一个input参数来自源文件 arch/x86/boot/compressed/misc.c 里的extract_kernel函数： 1234567891011121314151617asmlinkage __visible void *extract_kernel(void *rmode, memptr heap, unsigned char *input_data, unsigned long input_len, unsigned char *output, unsigned long output_len)&#123; ... ... ... choose_random_location((unsigned long)input_data, input_len, (unsigned long *)&amp;output, max(output_len, kernel_total_size), &amp;virt_addr); ... ... ...&#125; 这个参数由 arch/x86/boot/compressed/head_64.S的汇编代码传递： 1leaq input_data(%rip), %rdx input_data由 mkpiggy.c程序生成。如果你亲手编译过Linux内核源码，你会找到这个程序生成的文件，它应该位于 linux/arch/x86/boot/compressed/piggy.S. 在我这里，这个文件是这样的： 123456789.section &quot;.rodata..compressed&quot;,&quot;a&quot;,@progbits.globl z_input_lenz_input_len = 6988196.globl z_output_lenz_output_len = 29207032.globl input_data, input_data_endinput_data:.incbin &quot;arch/x86/boot/compressed/vmlinux.bin.gz&quot;input_data_end: 你能看到它有四个全局符号。前两个z_input_len和z_output_len是压缩的和解压后的vmlinux.bin.gz的大小。第三个是我们的input_data，你可以看到，它指向二进制格式（去掉所有调试符号、注释和重定位信息）的Linux内核镜像。最后的input_data_end指向压缩的Linux镜像的末尾。 所以我们choose_random_location函数的第一个参数是指向嵌入在piggy.o目标文件的压缩的内核镜像的指针。 choose_random_location函数的第二个参数是我们刚刚看到的z_input_len. choose_random_location函数的第三和第四个参数分别是解压后的内核镜像的位置和长度。放置解压后内核的地址来自 arch/x86/boot/compressed/head_64.S，并且它是startup_32对齐到 2MB 边界的地址。解压后的内核的大小来自同样的piggy.S，并且它是z_output_len. choose_random_location函数的最后一个参数是内核加载地址的虚拟地址。我们可以看到，它和默认的物理加载地址相同： 1unsigned long virt_addr = LOAD_PHYSICAL_ADDR; 它依赖于内核配置 123#define LOAD_PHYSICAL_ADDR ((CONFIG_PHYSICAL_START \\ + (CONFIG_PHYSICAL_ALIGN - 1)) \\ &amp; ~(CONFIG_PHYSICAL_ALIGN - 1)) 现在，由于我们考虑choose_random_location函数的参数，让我们看看它的实现。这个函数从检查内核命令行的nokaslr选项开始： 1234if (cmdline_find_option_bool(&quot;nokaslr&quot;)) &#123; warn(&quot;KASLR disabled: &#x27;nokaslr&#x27; on cmdline.&quot;); return;&#125; 如果有这个选项，那么我们就退出choose_random_location函数，并且内核的加载地址不会随机化。相关的命令行选项可以在内核文档找到： 1234567kaslr/nokaslr [X86]Enable/disable kernel and module base offset ASLR(Address Space Layout Randomization) if built intothe kernel. When CONFIG_HIBERNATION is selected,kASLR is disabled by default. When kASLR is enabled,hibernation will be disabled. 假设我们没有把nokaslr传到内核命令行，并且CONFIG_RANDOMIZE_BASE启用了内核配置选项。 下一步是以下函数的调用： 1initialize_identity_maps(); 它在 arch/x86/boot/compressed/pagetable.c源码文件定义。这个函数从初始化mapping_info,x86_mapping_info结构体的一个实例开始。 1234mapping_info.alloc_pgt_page = alloc_pgt_page;mapping_info.context = &amp;pgt_data;mapping_info.page_flag = __PAGE_KERNEL_LARGE_EXEC | sev_me_mask;mapping_info.kernpg_flag = _KERNPG_TABLE | sev_me_mask; x86_mapping_info结构体在 arch/x86/include/asm/init.h头文件定义： 12345678struct x86_mapping_info &#123; void *(*alloc_pgt_page)(void *); void *context; unsigned long page_flag; unsigned long offset; bool direct_gbpages; unsigned long kernpg_flag;&#125;; 这个结构体提供了关于内存映射的信息。你可能还记得，在前面的部分，我们已经建立了初始的从0到4G的页表。现在我们可能需要访问4G以上的内存来在随机的位置加载内核。所以，initialize_identity_maps函数初始化一个内存区域，它用于可能需要的新页表。首先，让我们尝试查看x86_mapping_info结构体的定义。 alloc_pgt_page是一个会在为一个页表项分配空间时调用的回调函数。context域是一个用于跟踪已分配页表的alloc_pgt_data结构体的实例。page_flag和kernpg_flag是页标志。第一个代表PMD或PUD表项的标志。第二个kernpg_flag域代表会在之后被覆盖的内核页的标志。direct_gbpages域代表对大页的支持。最后的offset域代表内核虚拟地址到PMD级物理地址的偏移。 alloc_pgt_page回调函数检查有一个新页的空间，从缓冲区分配新页并返回新页的地址： 12entry = pages-&gt;pgt_buf + pages-&gt;pgt_buf_offset;pages-&gt;pgt_buf_offset += PAGE_SIZE; 缓冲区在此结构体中： 12345struct alloc_pgt_data &#123; unsigned char *pgt_buf; unsigned long pgt_buf_size; unsigned long pgt_buf_offset;&#125;; initialize_identity_maps函数最后的目标是初始化pgdt_buf_size和pgt_buf_offset. 由于我们只是在初始化阶段，initialize_identity_maps函数设置pgt_buf_offset为0: 1pgt_data.pgt_buf_offset = 0; 而pgt_data.pgt_buf_size会根据引导加载器所用的引导协议（64位或32位）被设置为77824或69632. pgt_data.pgt_buf也是一样。如果引导加载器在startup_32引导内核，pgdt_data.pgdt_buf会指向已经在 arch/x86/boot/compressed/head_64.S初始化的页表的末尾： 1pgt_data.pgt_buf = _pgtable + BOOT_INIT_PGT_SIZE; 其中_pgtable指向这个页表 _pgtable的开头。另一方面，如果引导加载器用64位引导协议并在startup_64加载内核，早期页表应该由引导加载器建立，并且_pgtable会被重写： 1pgt_data.pgt_buf = _pgtable 在新页表的缓冲区被初始化之下，我们回到choose_random_location函数。 避开保留的内存范围 在恒等映射页表相关的数据被初始化之后，我们可以开始选择放置解压后内核的随机位置。但是正如你猜的那样，我们不能选择任意地址。在内存的范围中，有一些保留的地址。这些地址被重要的东西占用，如initrd, 内核命令行等等。这个函数： 1mem_avoid_init(input, input_size, *output); 会帮我们做这件事。所有不安全的内存区域会收集到： 123456struct mem_vector &#123; unsigned long long start; unsigned long long size;&#125;;static struct mem_vector mem_avoid[MEM_AVOID_MAX]; 数组。其中MEM_AVOID_MAX来自枚举类型mem_avoid_index, 它代表不同类型的保留内存区域： 123456789enum mem_avoid_index &#123; MEM_AVOID_ZO_RANGE = 0, MEM_AVOID_INITRD, MEM_AVOID_CMDLINE, MEM_AVOID_BOOTPARAMS, MEM_AVOID_MEMMAP_BEGIN, MEM_AVOID_MEMMAP_END = MEM_AVOID_MEMMAP_BEGIN + MAX_MEMMAP_REGIONS - 1, MEM_AVOID_MAX,&#125;; 它们都定义在源文件 arch/x86/boot/compressed/kaslr.c 中。 让我们看看mem_avoid_init函数的实现。这个函数的主要目标是在mem_avoid数组存放关于被mem_avoid_index枚举类型描述的保留内存区域的信息，并且在我们新的恒等映射缓冲区为这样的区域创建新页。mem_avoid_index函数的几个部分很相似，但是先看看其中一个： 1234mem_avoid[MEM_AVOID_ZO_RANGE].start = input;mem_avoid[MEM_AVOID_ZO_RANGE].size = (output + init_size) - input;add_identity_map(mem_avoid[MEM_AVOID_ZO_RANGE].start, mem_avoid[MEM_AVOID_ZO_RANGE].size); mem_avoid_init函数的开头尝试避免用于当前内核解压的内存区域。我们用这个区域的起始地址和大小填写mem_avoid数组的一项，并调用add_identity_map函数，它会为这个区域建立恒等映射页。add_identity_map函数在源文件 arch/x86/boot/compressed/kaslr.c定义： 123456789101112void add_identity_map(unsigned long start, unsigned long size)&#123; unsigned long end = start + size; start = round_down(start, PMD_SIZE); end = round_up(end, PMD_SIZE); if (start &gt;= end) return; kernel_ident_mapping_init(&amp;mapping_info, (pgd_t *)top_level_pgt, start, end);&#125; 你可以看到，它对齐内存到 2MB 边界并检查给定的起始地址和终止地址。 最后它调用kernel_ident_mapping_init函数，它在源文件 arch/x86/mm/ident_map.c中，并传入以上初始化好的mapping_info实例、顶层页表的地址和建立新的恒等映射的内存区域的地址。 kernel_ident_mapping_init函数为新页设置默认的标志，如果它们没有被给出： 12if (!info-&gt;kernpg_flag) info-&gt;kernpg_flag = _KERNPG_TABLE; 并且开始建立新的2MB (因为mapping_info.page_flag中的PSE位) 给定地址相关的页表项（五级页表中的PGD -&gt; P4D -&gt; PUD -&gt; PMD或者四级页表中的PGD -&gt; PUD -&gt; PMD）。 123456789101112for (; addr &lt; end; addr = next) &#123; p4d_t *p4d; next = (addr &amp; PGDIR_MASK) + PGDIR_SIZE; if (next &gt; end) next = end; p4d = (p4d_t *)info-&gt;alloc_pgt_page(info-&gt;context); result = ident_p4d_init(info, p4d, addr, next); return result;&#125; 首先我们找给定地址在 页全局目录 的下一项，如果它大于给定的内存区域的末地址end，我们把它设为end.之后，我们用之前看过的x86_mapping_info回调函数分配一个新页，然后调用ident_p4d_init函数。ident_p4d_init函数做同样的事情，但是用于低层的页目录 (p4d -&gt; pud -&gt; pmd). 和保留地址相关的新页表项已经在我们的页表中。这不是mem_avoid_init函数的末尾，但是其他部分类似。它建立用于 initrd、内核命令行等数据的页。 现在我们可以回到choose_random_location函数。 物理地址随机化 在保留内存区域存储在mem_avoid数组并且为它们建立了恒等映射页之后，我们选择最小可用的地址作为解压内核的随机内存区域 1min_addr = min(*output, 512UL &lt;&lt; 20); 你可以看到，它应该小于512MB. 选择这个512MB的值只是避免低内存区域中未知的东西。 下一步是选择随机的物理和虚拟地址来加载内核。首先是物理地址： 1random_addr = find_random_phys_addr(min_addr, output_size); find_random_phys_addr函数在kaslr.c源文件中定义： 1234567891011static unsigned long find_random_phys_addr(unsigned long minimum, unsigned long image_size)&#123; minimum = ALIGN(minimum, CONFIG_PHYSICAL_ALIGN); if (process_efi_entries(minimum, image_size)) return slots_fetch_random(); process_e820_entries(minimum, image_size); return slots_fetch_random();&#125; process_efi_entries函数的主要目标是在整个可用的内存找到所有的合适的内存区域来加载内核。如果内核没有在支持EFI的系统中编译和运行，我们继续在e820区域中找这样的内存区域。所有找到的内存区域会存储在 12345678struct slot_area &#123; unsigned long addr; int num;&#125;;#define MAX_SLOT_AREA 100static struct slot_area slot_areas[MAX_SLOT_AREA]; 数组中。内核解压器应该选择这个数组随机的索引，并且它会是内核解压的随机位置。这个选择会被slots_fetch_random函数执行。slots_fetch_random函数的主要目标是通过kaslr_get_random_long函数从slot_areas数组选择随机的内存范围： 1slot = kaslr_get_random_long(&quot;Physical&quot;) % slot_max; kaslr_get_random_long函数在源文件 arch/x86/lib/kaslr.c中定义，它返回一个随机数。注意这个随机数会通过不同的方式得到，取决于内核配置、系统机会（基于时间戳计数器的随机数、rdrand等等）。 这就是随机内存范围的选择方法。 虚拟地址随机化 在内核解压器选择了随机内存区域后，新的恒等映射页会为这个区域按需建立： 123456random_addr = find_random_phys_addr(min_addr, output_size);if (*output != random_addr) &#123; add_identity_map(random_addr, output_size); *output = random_addr;&#125; 这时，output会存放内核将会解压的一个内存区域的基地址。但是现在，正如你还记得的那样，我们只是随机化了物理地址。在x86_64架构，虚拟地址也应该被随机化： 1234if (IS_ENABLED(CONFIG_X86_64)) random_addr = find_random_virt_addr(LOAD_PHYSICAL_ADDR, output_size);*virt_addr = random_addr; 正如你所看到的，对于非x86_64架构，随机化的虚拟地址和随机化的物理地址相同。find_random_virt_addr函数计算可以保存内存镜像的虚拟内存范围的数量并且调用我们在尝试找到随机的物理地址的时候，之前已经看到的kaslr_get_random_long函数。 这时，我们同时有了用于解压内核的随机化的物理(*output)和虚拟(*virt_addr)基地址。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blackforest1990.github.io/categories/Linux/"}],"tags":[{"name":"Kernel","slug":"Kernel","permalink":"https://blackforest1990.github.io/tags/Kernel/"}]},{"title":"main memory","slug":"mainmemory","date":"2024-03-19T03:49:14.000Z","updated":"2024-03-20T03:50:35.076Z","comments":true,"path":"2024/03/19/mainmemory/","link":"","permalink":"https://blackforest1990.github.io/2024/03/19/mainmemory/","excerpt":"","text":"背景 内存是现代计算机系统运行的核心。内存由大量的字节组成，每个字节都有自己的地址。CPU根据程序计数器的值从内存中获取指令。这些指令可能会导致从特定的内存地址进行额外的加载和存储。 我们从讨论与管理内存相关的几个问题开始讨论：基本硬件、符号内存地址与实际物理地址的绑定，以及逻辑地址和物理地址之间的区别。以讨论动态链接和共享库来结束这个blog。 基本硬件 主存储器和内置于处理器中的寄存器是 CPU 可直接访问的唯一通用存储器。有一些机器指令以内存地址作为参数，但没有以磁盘地址作为参数的指令。因此，任何正在执行的指令以及指令正在使用的任何数据都必须位于这些直接访问存储设备之一中。如果数据不在内存中，则必须将其移至内存，然后 CPU 才能对其进行操作。 内置于 CPU 中的寄存器通常在 CPU 时钟周期内可访问。大多数 CPU 可以以每个时钟周期一个或多个操作的速率解码指令并对寄存器内容执行简单操作。而主存储器却不能这样，它是通过内存总线进行访问的。完成内存访问可能需要许多 CPU 时钟周期。在这种情况下，处理器通常需要停顿，因为它没有完成正在执行的指令所需的数据。由于内存访问的频率，这种情况是不可容忍的。解决的方法是在 CPU 和主存储器之间添加快速存储器，通常在 CPU 芯片上以实现快速访问。为了管理内置于 CPU 中的高速缓存，硬件会自动加速内存访问，无需任何操作系统控制。 我们不仅关注访问物理内存的相对速度，还必须确保正确操作。为了正确运行系统，我们必须保护操作系统免受用户进程的访问。在多用户系统中，我们还必须保护用户进程免受彼此的影响。这种保护必须由硬件提供，因为操作系统通常不会介入 CPU 和其内存访问之间（由于所产生的性能损失）。 首先，我们需要确保每个进程都拥有单独的内存空间。为了分隔内存空间，我们需要确定进程可以访问的合法地址范围，并确保进程只能访问这些合法地址。我们可以通过使用两个寄存器来提供这种保护，通常是一个基址寄存器和一个限界寄存器。基址寄存器保存最小的合法物理内存地址；限界寄存器指定了地址范围的大小。例如，如果基址寄存器保存了 300040，而限界寄存器是 120900，那么程序可以合法地访问从 300040 到 420939（包括边界）的所有地址。 通过让 CPU 硬件将用户模式下生成的每个地址与寄存器进行比较来实现对内存空间的保护。任何在用户模式下执行的程序企图访问操作系统内存或其他用户内存的尝试都会导致陷阱（trap）到操作系统，操作系统会将该尝试视为致命错误。这种方案可以防止用户程序修改操作系统或其他用户的代码或数据结构。 base和limit寄存器只能由操作系统加载，而操作系统使用特殊的特权指令进行加载，特权指令只能在内核模式下执行。该方案允许操作系统更改寄存器的值但不允许用户程序去修改。 在内核模式下执行的操作系统对操作系统内存和用户内存都具有无限制的访问权限。这项规定允许操作系统将用户程序加载到用户内存中，以便在出现错误时将这些程序卸载，访问和修改系统调用的参数，与用户内存进行 I/O 操作，并提供许多其他服务。例如，考虑一个多处理系统的操作系统必须执行上下文切换，在将下一个进程的上下文从主内存加载到寄存器之前，将一个进程的状态从寄存器存储到主内存中。 地址绑定 通常，一个程序以二进制可执行文件的形式存在于磁盘上。要执行该程序，必须将其加载到内存中，并放置到一个进程内。根据所使用的内存管理方式，在执行过程中，该进程可能会在磁盘和内存之间移动。等待被加载到内存以供执行的磁盘上的进程形成输入队列。正常的单任务处理过程是从输入队列中选择一个进程，并将该进程加载到内存中。随着进程的执行，它会从内存中访问指令和数据。最终，进程终止，其内存空间被声明为空闲。 大多数系统允许用户进驻驻留在物理内存的任何部分。因此，尽管计算机的地址空间可能从00000开始，但用户进程的第一个地址不一定是00000。 在大多数情况下，用户程序在执行前经历几个步骤：在这些步骤中，地址可以用不同方式表示。源程序中的地址通常是符号的(count), 编译器通常讲这些符号地址绑定到可重定位地址。(例如“从这个模块开头的14字节”)。链接器又将可重定向地址绑定到绝对地址(例如74014)。每个绑定都是从一个地址空间到另一个地址空间的映射。 传统上，指令和数据与内存地址的绑定可以在沿途的任何步骤中完成： 编译时：如果在编译时知道进程将驻留在内存中的位置，则可以生成绝对代码。 加载时：如果在编译时不知道进程将驻留在内存中的位置，则编译器必须生成可重定向代码。在这种情况下，最终的绑定被延迟到加载时。如果起始地址发生变化，我们只需要重新加载用户代码以将此更改的值纳入其中。 执行时：如果进程在执行过程中可以从一个内存段移动到另一个内存段，则绑定必须延迟到运行时。为了使这种方案正常工作，必须有特殊的硬件可用。大多数通用操作系统采用这种方案。 逻辑空间与物理空间 CPU生成的地址通常成为逻辑地址，而被内存单元看到的地址–即加载到内存地址寄存器中的地址–通常被称为物理地址。 编译时和加载时地址绑定方法生成相同的逻辑地址和物理地址。然而，执行时地址绑定方案导致不同的逻辑地址和物理地址。在这种情况下，我们通常将逻辑地址称为虚拟地址。在本文中，我们将逻辑地址和虚拟地址互换使用。由程序生成的所有逻辑地址的集合称为逻辑地址空间。与这些逻辑地址对应的所有物理地址的集合称为物理地址空间。 虚拟地址到物理地址的运行时映射是由称为内存管理单元（MMU）的硬件设备完成的。目前，我们将用一个简单的 MMU 方案来说明这种映射。现在，基址寄存器被称为重定位寄存器。在将用户进程生成的每个地址发送到内存时，重定位寄存器中的值会被添加到该地址上。例如，如果基址是 14000，那么用户试图访问位置 0 将被动态重定位到位置 14000；访问位置 346 将被映射到位置 14346。 用户程序永远不会看到真实的物理地址。程序可以创建一个指向位置 346 的指针，将其存储在内存中，对其进行操作，并将其与其他地址进行比较——所有这些都是作为数字 346。只有当它被用作内存地址（例如在间接加载或存储中）时，它才相对于基址寄存器进行重定位。用户程序处理逻辑地址。内存映射硬件将逻辑地址转换为物理地址。被引用的内存地址的最终位置直到引用被执行时才确定。 现在我们有两种不同类型的地址：逻辑地址（在范围 0 到 max 内）和物理地址（对于基址值 R，范围为 R + 0 到 R + max）。用户程序仅生成逻辑地址，并认为进程在位置 0 到 max 运行。然而，在使用这些逻辑地址之前，它们必须映射到物理地址。将逻辑地址空间绑定到单独的物理地址空间的概念是正确的内存管理的核心。 动态加载 到目前为止，在我们的讨论中，为了执行进程，进程的整个程序和所有数据都必须在物理内存中。因此，进程的大小被限制为物理内存的大小。为了获得更好的内存空间利用率，我们可以使用动态加载。使用动态加载，直到调用时才加载例程。所有例程以可重定位的加载格式保存在磁盘上。主程序被加载到内存中并执行。当一个例程需要调用另一个例程时，调用例程首先检查另一个例程是否已加载。如果没有，就调用可重定位链接加载器将所需的例程加载到内存中，并更新程序的地址表以反映此更改。然后控制传递给新加载的例程。 动态加载的优点是只有在需要时才加载例程。当需要大量代码来处理不经常发生的情况时，例如错误例程时，这种方法特别有用。在这种情况下，虽然总程序大小可能很大，但使用的部分（因此加载的部分）可能要小得多。 动态加载不需要操作系统的特殊支持。将程序设计为利用这种方法是用户的责任。然而，操作系统可能通过提供库例程来实现动态加载，从而帮助程序员。 动态链接和共享库 动态链接库是系统库，当用户程序运行时，它们会被链接到用户程序。一些操作系统仅支持静态链接，其中系统库被视为任何其他对象模块，并由加载程序合并到二进制程序图像中。相比之下，动态链接类似于动态加载。在这里，链接而不是加载被推迟到执行时期。此功能通常与系统库一起使用，例如语言子例程库。如果没有此功能，系统上的每个程序都必须在可执行映像中包含其语言库的副本。这个要求浪费了磁盘空间和主内存。 使用动态链接，每个库例程引用都包含在映像中的一个存根stub。存根是一小段代码，指示如何定位适当的内存驻留库例程，或者如何加载库，如果例程尚未存在的话。当存根执行时，它会检查所需的例程是否已经在内存中。如果没有，程序将例程加载到内存中。无论哪种方式，存根都将自身替换为例程的地址并执行该例程。因此，下一次达到特定的代码段时，库例程将直接执行，动态链接不会产生任何成本。在这个方案下，使用语言库的所有进程只执行一份库代码的副本。 **这个特性可以扩展到库的更新（如修复错误）。一个库可以被一个新版本替换，所有引用该库的程序将自动使用新版本。**如果没有动态链接，所有这些程序都需要重新链接才能访问新的库。为了防止程序意外执行新的不兼容版本的库，版本信息被包含在程序和库中。一个库的多个版本可能被加载到内存中，每个程序使用它的版本信息来决定使用哪个库副本。因此，只有使用新库版本编译的程序才会受到其中包含的任何不兼容更改的影响。在安装新库之前链接的其他程序将继续使用旧库。这个系统也被称为共享库。 与动态加载不同，动态链接和共享库通常需要操作系统的帮助。如果内存中的进程相互保护，则操作系统是唯一能够检查所需例程是否在另一个进程的内存空间中或允许多个进程访问相同内存地址的实体。 交换 一个进程必须在内存中执行。然而，一个进程可以暂时交换到后备存储中去，然后再次被带回内存继续执行。交换使得所有进程的总物理地址空间可以超过系统的真实内存，从而增加了多道程序设计的程度。 标准交换 标准交换涉及将进程在主内存和后备存储之间移动。后备存储通常是一个快速磁盘。它必须足够大，以容纳所有用户的所有内存镜像的副本，并且必须提供对这些内存镜像的直接访问。系统维护一个准备队列，其中包含所有内存镜像位于后备存储或内存中且准备运行的进程。每当 CPU 调度程序决定执行一个进程时，它就会调用调度程序。调度程序检查队列中的下一个进程是否在内存中。如果不是，并且没有空闲的内存区域，调度程序就会将当前在内存中的一个进程换出，并换入所需的进程。然后，它重新加载寄存器并将控制转移到所选的进程。 交换时间的主要开销在传输上。标准的交换在现代操作系统中不再使用。它需要太长的交换时间，提供的执行时间太少，无法成为合理的内存管理解决方案。然而，修改过的交换版本在许多系统上都可以找到，包括 UNIX、Linux 和 Windows。在一个常见的变体中，交换通常被禁用，但如果空闲内存的量（供操作系统或进程使用的未使用内存）低于一个阈值时，交换将开始。当空闲内存的量增加时，交换将停止。另一个变体涉及交换进程的部分而不是整个进程，以减少交换时间。通常，这些修改过的交换形式与虚拟内存一起使用。 移动系统交换 尽管大多数 PC 和服务器的操作系统支持某种修改过的交换版本，但移动系统通常不支持任何形式的交换。移动设备通常使用闪存而不是更宽敞的硬盘作为它们的持久存储。由此产生的空间限制是移动操作系统设计者避免交换的原因之一。其他原因包括闪存在变得不可靠之前所能容忍的有限写入次数，以及这些设备中主内存与闪存之间的低吞吐量。 当空闲内存下降到一定阈值以下时，苹果的iOS不使用交换，而是要求应用程序自愿释放分配的内存。只读数据（例如代码）从系统中删除，并在需要时从闪存中重新加载。已修改的数据（例如堆栈）永远不会被删除。然而，任何未能释放足够内存的应用程序可能会被操作系统终止。 Android 不支持交换，并采用了类似 iOS 的策略。如果内存不足，它可能会终止一个进程。然而，在终止进程之前，Android 会将其应用程序状态写入闪存，以便可以快速重新启动。 由于这些限制，移动系统的开发人员必须谨慎地分配和释放内存，以确保他们的应用程序不会使用过多的内存或遭受内存泄漏的问题。请注意，iOS 和 Android 都支持分页，因此它们确实具有内存管理能力。 连续内存分配 主内存必须容纳操作系统和各种用户进程。因此，我们需要以尽可能高效的方式分配主内存。本节将解释一种早期的方法，连续内存分配。 内存通常分为两个分区：一个用于驻留操作系统，另一个用于用户进程。我们可以将操作系统放置在低内存或高内存中。影响此决定的主要因素是中断向量的位置。由于中断向量通常位于低内存中，程序员通常也将操作系统放置在低内存中。 我们通常希望多个用户进程同时驻留在内存中。因此，我们需要考虑如何将可用内存分配给等待被载入内存的进程。在连续内存分配中，每个进程都包含在内存的一个单独部分中，该部分与包含下一个进程的部分相邻。 内存保护 在进一步讨论内存分配之前，我们必须讨论内存保护的问题。通过结合之前讨论过的两个想法，我们可以防止一个进程访问它不拥有的内存。如果我们有一个具有重定位寄存器和一个限制寄存器的系统，我们就可以实现我们的目标。重定位寄存器包含最小物理地址的值；限制寄存器包含逻辑地址的范围（例如，重定位 = 100040，限制 = 74600）。每个逻辑地址必须落在限制寄存器指定的范围内。内存管理单元（MMU）通过将重定位寄存器中的值添加到逻辑地址来动态地映射逻辑地址。这个映射地址被发送到内存。 当CPU调度程序选择一个进程进行执行时，调度程序作为上下文切换的一部分将重定位和限制寄存器加载为正确的值。因为CPU生成的每个地址都会与这些寄存器进行比较，所以我们可以保护操作系统和其他用户的程序和数据，防止该运行进程对其进行修改。 重定位寄存器方案提供了一种有效的方法来允许操作系统的大小动态变化。这种灵活性在许多情况下都是可取的。例如，操作系统包含设备驱动程序的代码和缓冲区空间。如果一个设备驱动程序（或其他操作系统服务）不常用，我们不希望保留代码和数据在内存中，因为我们可能能够将该空间用于其他用途。因此，在程序执行期间使用此代码会改变操作系统的大小。 内存分配 现在我们准备转向内存分配。一种最简单的内存分配方法是将内存分成几个固定大小的分区。每个分区可能包含一个进程。因此，多道程序设计的程度受到分区数量的限制。在这种多分区方法中，当一个分区为空闲时，从输入队列中选择一个进程，并将其加载到空闲分区中。当进程终止时，该分区变为另一个进程可用。这种方法现在已不再使用。接下来描述的方法是固定分区方案的一种泛化（称为MVT）；它主要用于批处理环境。这里提出的许多想法也适用于使用纯分段进行内存管理的分时环境。 在可变分区方案中，操作系统维护一张表，指示内存的哪些部分是可用的，哪些是占用的。最初，所有内存都可用于用户进程，并被视为一个可用内存的大块，一个空闲块。随着你将会看到的，内存最终包含了一组不同大小的空闲块。 当进程进入系统时，它们被放置在一个输入队列中。操作系统考虑每个进程的内存需求以及可用内存空间的数量，以确定哪些进程被分配内存。当一个进程被分配空间时，它被加载到内存中，然后可以竞争CPU时间。当一个进程终止时，它释放其内存，操作系统随后可以用来填充输入队列中的另一个进程。 因此，在任何给定时间，我们都有一个可用块大小的列表和一个输入队列。操作系统可以根据调度算法对输入队列进行排序。内存被分配给进程，直到最终无法满足下一个进程的内存需求为止，也就是说，没有足够大的可用内存块（或空闲块）来容纳该进程。然后，操作系统可以等待直到有足够大的块可用，或者可以跳过输入队列，看看是否可以满足其他一些进程的较小内存需求。 通常情况下，正如前面提到的，可用的内存块包括散布在整个内存中的一组不同大小的空闲块。当一个进程到达并需要内存时，系统会在这组空闲块中搜索一个足够大的空闲块来容纳该进程。如果空闲块太大，它将被分成两部分。一部分分配给到达的进程；另一部分则返回到空闲块集合中。当一个进程终止时，它释放其内存块，然后将其放回到空闲块集合中。如果新的空闲块与其他空闲块相邻，则这些相邻的空闲块将合并成一个更大的空闲块。此时，系统可能需要检查是否有等待内存的进程，以及这些新释放和合并的内存是否能够满足任何一个等待中的进程的需求。 这个过程是一种特殊情况，涉及到如何从一组空闲块中满足大小为n的请求的一般动态存储分配问题。对于这个问题有很多解决方案。首次适应、最佳适应和最坏适应策略是从可用空闲块集合中选择一个空闲块的最常用方法。 • 首次适应。分配第一个足够大的空闲块。搜索可以从空闲块集合的开头开始，也可以从上次首次适应搜索结束的位置开始。一旦找到一个足够大的空闲块，我们就可以停止搜索。 • 最佳适应。分配最小的足够大的空闲块。除非列表按大小排序，否则我们必须搜索整个列表。该策略产生的剩余空闲块最小。 • 最坏适应。分配最大的空闲块。同样，除非列表按大小排序，否则我们必须搜索整个列表。该策略产生的剩余空闲块最大，可能比最佳适应方法产生的较小剩余空闲块更有用。 模拟结果表明，在减少时间和存储利用方面，首次适应和最佳适应都优于最坏适应。在存储利用方面，首次适应和最佳适应都没有明显的优势，但是首次适应通常速度更快。 内存碎片化 首次适应和最佳适应的内存分配策略都存在外部碎片问题。随着进程被加载到内存中并从内存中移除，空闲内存空间被分割成小块。当总内存空间足够满足请求，但可用空间不连续时，就存在外部碎片化：存储被分割成大量小块。这种碎片化问题可能非常严重。在最坏的情况下，我们可能在每两个进程之间都有一块空闲（或浪费的）内存。如果所有这些小块内存都在一个大的空闲块中，我们可能能够运行更多的进程。 我们使用首次适应还是最佳适应策略都会影响碎片化的程度。另一个因素是空闲块的哪一端被分配。无论使用哪种算法，外部碎片化都将是一个问题。根据总内存存储量和平均进程大小的不同，外部碎片化可能是一个轻微或严重的问题。例如，对首次适应进行统计分析显示，即使进行了一些优化，对于给定的N个分配块，另外的0.5N块将被碎片化浪费。也就是说，有三分之一的内存可能无法使用！这一特性被称为50%规则。 内存碎片化不仅可以是外部的，还可以是内部的。考虑一个带有18,464字节空洞的多分区分配方案。假设下一个进程请求18,462字节。如果我们精确地分配所请求的块，就会剩下2字节的空洞。跟踪这个空洞的开销将远远大于空洞本身。避免这个问题的一般方法是将物理内存分成固定大小的块，并根据块大小单位分配内存。采用这种方法，分配给一个进程的内存可能略大于请求的内存。这两个数字之间的差异就是内部碎片化——分区内部未使用的内存。 解决外部碎片化问题的一个方法是压缩。其目标是重新排列内存内容，以便将所有的空闲内存放在一个大的块中。然而，压缩并非总是可行的。如果重定位是静态的，并且在汇编或加载时进行，那么无法进行压缩。只有在重定位是动态的，并且在执行时进行时才可能。如果地址是动态重定位的，则重定位仅需要移动程序和数据，然后更改基址寄存器以反映新的基址。当压缩是可能的时，我们必须确定其成本。最简单的压缩算法是将所有进程移向内存的一端；所有的空洞向另一个方向移动，产生一个大的可用内存空洞。 解决外部碎片化问题的另一个可能方法是允许进程的逻辑地址空间是非连续的，从而允许进程在任何可用的物理内存中分配内存。两种互补的技术实现了这个解决方案：分段和分页。 分段 如果硬件能够提供一种将程序员的视图映射到实际物理内存的内存机制会怎样呢？系统会有更多的自由来管理内存，而程序员将拥有更自然的编程环境。分段segmentation提供了这样一种机制。 基本方法 程序员是否将内存视为一组包含指令和数据的字节的线性数组？大多数程序员会说“不是”。相反，他们更倾向于将内存视为一组可变大小的段，这些段之间没有必要的顺序关系。 在编写程序时，程序员将其视为一个主程序，其中包含一组方法、过程或函数。它还可能包括各种数据结构：对象、数组、堆栈、变量等等。每个模块或数据元素都通过名称引用。程序员会谈论“堆栈”、“数学库”和“主程序”，而不关心这些元素在内存中占据的地址。她不关心堆栈是存储在Sqrt()函数之前还是之后。段的长度各不相同，每个段的长度都是由其在程序中的目的固定定义的。段内的元素由它们相对于段开头的偏移量来标识：程序的第一条语句，堆栈中第七个堆栈帧条目，Sqrt()的第五条指令等等。 分段是一种支持程序员对内存这种视图的内存管理方案。逻辑地址空间是段的集合。每个段都有一个名称和一个长度。地址指定了段的名称和段内的偏移量。因此，程序员通过两个量来指定每个地址：一个段名称和一个偏移量。段被编号并通过段号来引用，而不是通过段名。因此，逻辑地址由一个二元组组成：&lt;段号，偏移量&gt;。 通常，当程序被编译时，编译器会自动构建反映输入程序的段。 代码 全局变量 堆，用于分配内存 每个线程使用的栈 标准C库 在编译时链接的库可能会被分配给单独的段。加载器会获取所有这些段并为它们分配段号。 Segmentation Hardware 尽管程序员现在可以通过二维地址引用程序中的对象，但实际的物理内存仍然是一维字节序列。因此，我们必须定义一种实现，将二维用户定义的地址映射到一维物理地址。这种映射是通过一个段表实现的。段表中的每个条目都有一个段基址segment base和一个段限长segment limit。段基址包含段在内存中的起始物理地址，而段限长指定了段的长度。 段表的使用: 一个逻辑地址由两部分组成：段号s和该段中的偏移量d。段号被用作段表的索引。逻辑地址的偏移量d必须在0到段限长之间。如果不是，则会陷入操作系统（尝试超出段末尾的逻辑寻址）。当偏移量是合法的时，它会加到段基址上，以产生所需字节的物理内存地址。因此，段表实质上是一组基址-限长寄存器对的数组。 举例来说。我们有从0到4编号的五个段。段存储在物理内存中。段表为每个段单独提供一个条目，给出段在物理内存中的起始地址（或基址）和该段的长度（或限长）。例如，段2长400字节，从位置4300开始。因此，对段2的字节53的引用被映射到位置4300 + 53 = 4353。对段3，字节852的引用被映射到3200（段3的基址） + 852 = 4052。对段0的字节1222的引用将导致陷阱到操作系统，因为这个段只有1000字节长。 分页 分段允许物理地址空间是非连续的。分页Paging另一种内存管理方案。分页避免了外部碎片化和压缩的需要，而分段则没有。它还解决了将不同大小的内存块适应到后备存储的问题。在分页引入之前使用的大多数内存管理方案都存在这个问题。问题的出现是因为，当驻留在主存储器中的代码片段或数据需要被换出时，必须在后备存储器中找到空间。后备存储器具有与主内存中讨论的碎片化问题相同的问题，但访问速度要慢得多，因此无法进行压缩。由于其优点，分页在各种形式下被用于大多数操作系统，从大型机到智能手机的操作系统都是如此。分页通过操作系统与计算机硬件的协作实现。 基本方法 实现分页的基本方法包括将物理内存划分为固定大小的块，称为页框frames，将逻辑内存划分为相同大小的块，称为页面pages。当一个进程要执行时，它的页面从其源（文件系统或后备存储）加载到任何可用的内存页框中。后备存储被划分为与内存页框大小相同的固定大小的块，或者是多个页框的聚类。这个相当简单的想法具有很强的功能性和广泛的影响。例如，逻辑地址空间现在完全与物理地址空间分离，因此即使系统的物理内存少于 2的64次方字节，进程也可以拥有逻辑 64 位地址空间。 硬件对分页的支持如下图所示。CPU 生成的每个地址分为两部分：页号page number (p) 和页偏移量page offset (d)。页号用作页表page table的索引。页表包含物理内存中每个页面的基地址。这个基地址与页偏移量相结合，定义了发送到内存单元的物理内存地址。 内存的分页模型如下图所示。 页面大小（与frame大小一样）由硬件定义。页面的大小是 2 的幂，根据计算机体系结构的不同，每个页面的大小可变，介于 512 字节到 1 GB 之间。选择 2 的幂作为页面大小使得将逻辑地址转换为页号和页偏移量特别容易。如果逻辑地址空间的大小为 2^m，而页面大小为 2^n 字节，则逻辑地址的高 m − n 位表示页号，低 n 位表示页偏移量。因此，逻辑地址如下所示： 其中，p 是页表的索引，d 是页内的位移。 作为一个具体的（虽然微小的）例子，考虑下图中的内存。在这里，逻辑地址中，n= 2，m = 4。使用页面大小为4字节和物理内存为32字节（8页），我们展示了如何将程序员对内存的视图映射到物理内存中。逻辑地址0是页面0，偏移0。索引到页表中，我们发现页面0在frame 5中。因此，逻辑地址0映射到物理地址20 [=（5 × 4）+ 0]。逻辑地址3（页面0，偏移3）映射到物理地址23 [=（5 × 4）+ 3]。逻辑地址4是页面1，偏移0；根据页表，页面1映射到帧6。因此，逻辑地址4映射到物理地址24 [=（6 × 4）+ 0]。逻辑地址13映射到物理地址9。 你可能已经注意到，分页本身是一种动态重定位的形式。每个逻辑地址都由分页硬件绑定到某个物理地址。使用分页类似于使用基址（或重定位）寄存器表，每个内存frame对应一个。 当我们使用分页方案时，不存在外部碎片：任何空闲帧都可以分配给需要的进程。然而，我们可能会有一些内部碎片。请注意，frame是作为单元分配的。如果一个进程的内存需求恰好不与页面边界相符，那么最后分配的frame可能不会完全填满。例如，如果页面大小为 2,048 字节，一个大小为 72,766 字节的进程将需要 35 个页面加上 1,086 字节。它将被分配 36 个frame，导致内部碎片为 2,048 − 1,086 = 962 字节。在最坏的情况下，一个进程将需要 n 个页面加上 1 个字节。它将被分配 n + 1 个frame，导致几乎整个frame的内部碎片。 如果进程大小与页面大小无关，我们预期每个进程平均会有一半页面的内部碎片。这个考虑表明小的页面大小是理想的。然而，每个页表项都涉及一定的开销，随着页面大小的增加，这种开销会减少。此外，当传输的数据量较大时，磁盘 I/O 更有效率。通常，随着进程、数据集和主内存的增大，页面大小也在不断增长。如今，页面通常介于 4 KB 到 8 KB 之间，一些系统甚至支持更大的页面大小。一些 CPU 和内核甚至支持多个页面大小。例如，Solaris 使用 8 KB 和 4 MB 的页面大小，具体取决于页面存储的数据。研究人员正在开发支持可变即时页面大小的技术。 通常情况下，在 32 位 CPU 上，每个页表项长为 4 个字节，但这个大小也可能会变化。一个 32 位的页表项可以指向 2^32 个物理页帧中的一个。如果帧大小为 4 KB（2^12），那么具有 4 个字节的页表项的系统可以寻址 2^44 字节（或 16 TB）的物理内存。我们应该注意的是，在分页内存系统中，物理内存的大小与进程的最大逻辑大小是不同的。随着我们进一步探索分页，我们会介绍必须保留在页表项中的其他信息。这些信息减少了用于寻址页帧的可用位数。因此，具有 32 位页表项的系统可能会寻址比可能的最大物理内存更少。32 位 CPU 使用 32 位地址，这意味着给定进程空间只能是 2^32 字节（4 TB）。因此，分页让我们可以使用比 CPU 地址指针长度所能寻址的更大的物理内存。 当一个进程到达系统准备执行时，系统会检查其大小，以页为单位。进程的每一页都需要一个帧。因此，如果进程需要 n 页，那么至少需要 n 个帧在内存中可用。如果有 n 个帧可用，它们将被分配给这个到达的进程。进程的第一页被加载到分配的一个帧中，并且该帧号被放入该进程的页表中。接下来的一页被加载到另一个帧中，其帧号也被放入页表中，依此类推。 分页的一个重要方面是程序员对内存和实际物理内存之间的清晰分离。程序员将内存视为一个单一的空间，仅包含这一个程序。实际上，用户程序分散在物理内存中，物理内存还包含其他程序。程序员对内存的视图与实际物理内存之间的差异是由地址转换硬件进行调和的。逻辑地址被转换为物理地址。这种映射对程序员来说是隐藏的，并由操作系统控制。请注意，根据定义，用户进程无法访问它不拥有的内存。它无法寻址页表之外的内存，而页表只包含进程拥有的那些页面。 由于操作系统正在管理物理内存，它必须了解物理内存的分配细节——哪些帧已分配，哪些帧可用，总共有多少帧等等**。这些信息通常保存在一个称为页帧表frame table的数据结构中**。页帧表为每个物理页面帧保留一个条目，指示该页面帧是否空闲或已分配，如果已分配，则分配给哪个进程或多个进程的哪个页面。 此外，操作系统必须意识到用户进程运行在用户空间，所有逻辑地址必须映射为物理地址。如果用户进行系统调用并提供一个地址作为参数，那么该地址必须映射为正确的物理地址。操作系统为每个进程维护一个页表的副本，就像它维护指令计数器和寄存器内容的副本一样。这个副本用于在操作系统必须手动映射逻辑地址到物理地址时，将逻辑地址转换为物理地址。它也被CPU调度程序用于在分配CPU给一个进程时定义硬件页表。因此，分页增加了上下文切换时间。 硬件支持 每个操作系统都有自己的方法来存储页表。一些操作系统为每个进程分配一个页表。指向页表的指针与进程控制块中的其他寄存器值（如指令计数器）一起存储。当调度程序被告知要启动一个进程时，它必须重新加载用户寄存器，并从存储的用户页表中定义正确的硬件页表值。其他操作系统提供一个或最多几个页表，这样在进行进程上下文切换时，减少了相关开销。 页表的硬件实现可以有几种方式。在最简单的情况下，页表被实现为一组专用寄存器。这些寄存器应该采用非常高速的逻辑构建，以使分页地址转换高效。每次访问内存都必须通过分页映射，因此效率是一个重要考虑因素。CPU调度程序重新加载这些寄存器，就像重新加载其他寄存器一样。加载或修改页表寄存器的指令当然是特权的，这样只有操作系统才能更改内存映射。DEC PDP-11就是这种架构的一个例子。地址由16位组成，页面大小为8 KB。因此，页表由八个条目组成，这些条目存储在快速寄存器中。 如果页表相对较小（例如，256个条目），则使用寄存器来实现页表是令人满意的。然而，大多数当代计算机允许页表非常庞大（例如，100万个条目）。对于这些机器，使用快速寄存器来实现页表是不可行的。相反，页表保存在主存储器中，并且页表基址寄存器（PTBR）指向页表。更改页表仅需要更改这一个寄存器，大大减少了上下文切换时间。 这种方法的问题在于访问用户内存位置所需的时间。如果我们想要访问位置 i，我们必须首先使用页表基址寄存器（PTBR）中的值按照 i 的页号进行索引。这个任务需要一次内存访问。它为我们提供了页框号，将其与页面偏移组合以产生实际地址。然后我们可以访问内存中的所需位置。使用这种方案，访问一个字节需要两次内存访问（一次用于页表条目，一次用于字节）。因此，内存访问速度减慢了一倍。在大多数情况下，这种延迟是无法容忍的。我们可能还不如使用交换！ 解决这个问题的标准方法是使用一种特殊的、小型的、快速查找的硬件缓存，称为translation look-aside buffer（TLB）。TLB 是关联的、高速的内存。TLB 中的每个条目由两部分组成：一个键（或标签）和一个值。当关联内存收到一个条目时，该条目将与所有键同时进行比较。如果找到了该条目，则返回相应的值字段。搜索速度很快；在现代硬件中，TLB 查找是指令流水线的一部分，基本上不会增加性能开销。然而，为了能够在流水线步骤内执行搜索，TLB 必须保持较小。它通常的大小在 32 到 1,024 个条目之间。一些 CPU 实现了单独的指令和数据地址 TLB。这样可以增加可用的 TLB 条目数量，因为这些查找发生在不同的流水线步骤中。我们可以看到在这个发展中 CPU 技术的演变示例：系统从没有 TLB 发展到具有多级 TLB，就像它们具有多级缓存一样。 TLB 与页表结合使用的方式如下。TLB 仅包含少量的页表条目。当 CPU 生成一个逻辑地址时，它的页号被提供给 TLB。如果找到了页号，其帧号将立即可用，并用于访问内存。正如刚才提到的，这些步骤是作为 CPU 内的指令流水线的一部分执行的，与不实现分页的系统相比，不会增加性能开销。 如果页号不在 TLB 中（称为 TLB 未命中），则必须进行对页表的内存引用。根据 CPU 的不同，这可能是通过硬件自动完成，也可能是通过向操作系统发出中断。获取了帧号后，我们可以用它来访问内存。此外，我们将页号和帧号添加到 TLB 中，以便在下一次引用时快速找到它们。如果 TLB 已经装满了条目，则必须选择一个现有条目进行替换。替换策略从最近最少使用（LRU）到循环轮询再到随机都有。一些 CPU 允许操作系统参与 LRU 条目替换，而其他 CPU 则自行处理此事。此外，一些 TLB 允许将某些条目固定下来，意味着它们无法从 TLB 中移除。通常，对关键内核代码的 TLB 条目进行固定。 一些 TLB 在每个 TLB 条目中存储地址空间标识符（ASID）。ASID 唯一标识每个进程，并用于为该进程提供地址空间保护。当 TLB 尝试解析虚拟页号时，它确保当前运行的进程的 ASID 与与虚拟页关联的 ASID 匹配。如果 ASID 不匹配，则将该尝试视为 TLB 未命中。除了提供地址空间保护外，ASID 还允许 TLB 同时包含多个不同进程的条目。如果 TLB 不支持单独的 ASID，则每次选择新的页表（例如，每次上下文切换时），都必须清空（或擦除）TLB，以确保下一个执行的进程不会使用错误的转换信息。否则，TLB 可能包含旧条目，这些条目包含有效的虚拟地址，但从前一个进程留下的物理地址是错误的或无效的。 页面号在 TLB 中被找到的百分比被称为命中率。例如，80% 的命中率意味着我们在 TLB 中找到所需的页面号的次数占总次数的 80%。如果访问内存需要 100 纳秒，那么当页面号在 TLB 中时，映射内存访问需要 100 纳秒。如果我们在 TLB 中找不到页面号，则我们必须先访问页面表和帧号（100 纳秒），然后访问内存中所需的字节（100 纳秒），总共需要 200 纳秒。（我们假设页面表查找只需要一次内存访问，但实际可能需要更多，我们稍后会看到。）为了找到有效的内存访问时间，我们根据其概率进行加权： 有效访问时间 = 0.80 × 100 + 0.20 × 200 = 120 纳秒 正如我们前面所指出的，当今的 CPU 可能提供多级 TLB。因此，在现代 CPU 中计算内存访问时间要比上面的示例复杂得多。例如，Intel Core i7 CPU 具有 128 个条目的 L1 指令 TLB 和 64 个条目的 L1 数据 TLB。在 L1 中发生未命中的情况下，CPU 需要六个周期来检查 L2 中的条目。L2 中的未命中意味着 CPU 必须要么遍历内存中的页面表条目以找到相关的帧地址（可能需要数百个周期），要么中断到操作系统以让其完成这项工作。 TLB 是一种硬件特性，因此似乎对操作系统及其设计者来说并不重要。但设计者需要了解 TLB 的功能和特性，因为这些特性因硬件平台而异。**为了实现最佳操作，针对特定平台的操作系统设计必须根据该平台的 TLB 设计来实现分页。**同样，TLB 设计的更改（例如，在英特尔 CPU 的不同世代之间）可能需要操作系统的分页实现发生变化。 保护 在分页环境中，内存保护是通过与每个页面关联的保护位来实现的。通常，这些位被保存在页表中。 每个比特可以将一个页面定义为读写或只读。每次对内存的引用都通过页表以找到正确的页框号。同时，在计算物理地址的同时，可以检查保护位，以验证是否对只读页面进行了写操作。尝试向只读页面写入会导致硬件trap到操作系统（或内存保护违规）。 通常，每个页表条目都会附加一个额外的位：有效-无效位。当该位被设置为有效时，关联的页面位于进程的逻辑地址空间中，因此是合法的（或有效的）页面。当该位被设置为无效时，页面不在进程的逻辑地址空间中。非法地址通过使用有效-无效位被捕获。操作系统为每个页面设置此位，以允许或禁止对页面的访问。 例如，在一个具有14位地址空间（从0到16383）的系统中，假设我们有一个程序，应该仅使用地址0到10468。假设页面大小为2 KB。页面0、1、2、3、4和5中的地址通过页表正常映射。然而，任何尝试生成页面6或7中的地址都会发现有效-无效位被设置为无效，计算机将陷入操作系统（无效页面引用）。 请注意，这种方案引入了一个问题。因为程序仅扩展到地址10468，所以超出该地址的任何引用都是非法的。然而，对于页面5的引用被分类为有效，因此对地址12287及其以下的访问是有效的。只有从12288到16383的地址是无效的。这个问题是2 KB页面大小的结果，反映了分页的内部碎片问题。 很少有进程使用其整个地址范围。实际上，许多进程只使用它们可用的地址空间的一小部分。在这些情况下，为每个地址范围内的页面创建一个页表将是浪费的。大多数情况下，这个表都是未使用的，但会占用宝贵的内存空间。一**些系统提供了硬件支持，以页表长度寄存器（PTLR）的形式指示页表的大小。**该值与每个逻辑地址进行比较，以验证地址是否在进程的有效范围内。如果这个测试失败，就会导致操作系统发生错误陷阱。 共享页面 分页的一个优点是共享公共代码的可能性。在时间共享环境中，这一考虑尤为重要。考虑一个支持40个用户的系统，每个用户都执行一个文本编辑器。如果文本编辑器包含150 KB的代码和50 KB的数据空间，我们需要8000 KB来支持这40个用户。然而，如果代码是可重入代码（或纯代码），它就可以被共享。在这里，我们看到三个进程共享一个三页的编辑器——每页大小为50 KB（使用大页面大小是为了简化图示）。每个进程都有自己的数据页。 每个进程都有自己的寄存器副本和数据存储，用于保存进程执行所需的数据。两个不同进程的数据当然是不同的。只需在物理内存中保留一个编辑器的副本。每个用户的页表映射到相同的编辑器物理副本，但数据页映射到不同的帧。因此，要支持40个用户，我们只需要一个编辑器副本（150 KB），加上每个用户50 KB的数据空间的40个副本。现在所需的总空间是2150 KB，而不是8000 KB——这是一个显著的节省。其他使用频繁的程序也可以被共享——编译器、窗口系统、运行时库、数据库系统等等。为了可以共享，代码必须是可重入的。共享代码的只读特性不应该依赖代码的正确性；操作系统应该强制执行这个属性。 页表结构 分层页表 大多数现代计算机系统支持大型的逻辑地址空间（从2^32到2^64）。在这样的环境中，页表本身变得过于庞大。例如，考虑一个具有32位逻辑地址空间的系统。如果在这样的系统中，页面大小为4 KB（2^12），那么一个页表最多可以包含100万个条目（2^32/2^12）。假设每个条目由4字节组成，那么每个进程可能需要多达4 MB 的物理地址空间来存储页面表。显然，我们不希望在主存储器中连续分配页面表。解决这个问题的一个简单方法是将页面表分成较小的部分。我们可以通过几种方式实现这种划分。 一种方法是使用双层分页算法，其中页面表本身也被分页。 例如，再次考虑具有32位逻辑地址空间和页面大小为4 KB的系统。逻辑地址被划分为包含20位的页号和包含12位的页偏移量。因为我们对页表进行分页，所以页号进一步被划分为10位的页号和10位的页偏移量。因此，逻辑地址如下： 其中 p1 是外部页面表的索引，p2 是内部页面表中的位移。该体系结构的地址转换方法如下图所示。由于地址转换是从外部页面表向内部进行的，因此该方案也被称为前向映射的页表(foward-mapped page table)。 VAX是数字设备公司（DEC）于1977年至2000年间销售的最受欢迎的小型计算机。VAX架构支持一种变体的双级分页。VAX是一台32位机器，页面大小为512字节。进程的逻辑地址空间被划分为四个相等的部分，每个部分包含2^30字节。每个部分代表进程的逻辑地址空间的不同部分。逻辑地址的前2位高位指定了相应的部分。接下来的21位表示该部分的逻辑页号，最后的9位表示所需页的偏移量。通过以这种方式对页面表进行分区，操作系统可以将分区保留未使用，直到进程需要它们。整个虚拟地址空间的部分经常未被使用，并且多级页面表对这些空间没有条目，大大减少了存储虚拟内存数据结构所需的内存量。 在这个方案中，s表示节号，p是页面表的索引，d是页面内的位移。即使使用了这种方案，对于使用一个部分的VAX进程，一个级别的页面表的大小为2^21位 * 4字节每个条目 = 8 MB。为了进一步减少主存储器的使用，VAX对用户进程的页面表进行了分页。 64位的UltraSPARC需要七级分页，这将导致非常多的内存访问来转换每个逻辑地址。从这个例子中可以看出，对于64位架构来说，分层页表通常被认为是不合适的。 哈希页表 处理大于32位的地址空间的常见方法是使用哈希页表，其中哈希值是虚拟页号。哈希表中的每个条目包含一个链表，其中包含哈希到相同位置的元素（用于处理冲突）。每个元素包含三个字段：（1）虚拟页号，（2）映射的页帧的值，以及（3）指向链表中下一个元素的指针。 该算法的工作原理如下：虚拟地址中的虚拟页号被哈希到哈希表中。将虚拟页号与链表中的第一个元素中的字段1进行比较。如果存在匹配，则使用相应的页帧(字段2)来形成所需的物理地址。如果没有匹配项，则在链表中搜索后续条目以查找匹配的虚拟页号。 这种方案有一种适用于64位地址空间的变体。这种变体使用了聚簇式页表clustered page tables，它类似于散列页表，但哈希表中的每个条目不是指向单个页面，而是指向多个页面（例如16个）。因此，单个页表条目可以存储多个物理页面帧的映射关系。聚簇式页表对于稀疏地址空间特别有用，其中内存引用是不连续的，分散在整个地址空间中。 倒置页表 通常，每个进程都有一个关联的页表。页表中的每个条目对应着进程正在使用的每一页（或者对每个虚拟地址都有一个插槽，无论其有效性如何）。这种表示是自然的，因为进程通过页面的虚拟地址引用页面。然后，操作系统必须将此引用转换为物理内存地址。由于表是按虚拟地址排序的，因此操作系统能够计算出关联的物理地址条目位于表中的位置，并直接使用该值。这种方法的一个缺点是，每个页表可能包含数百万个条目。这些表可能会消耗大量物理内存，仅用于跟踪其他物理内存的使用情况。 为了解决这个问题，我们可以使用倒置页表inverted page table。倒置页表为内存中的每一页（或帧）都有一个条目。每个条目包含存储在该实际内存位置上的页面的虚拟地址，以及拥有该页面的进程的信息。因此，系统中只有一个页表，对于每一页物理内存只有一个条目。倒置页表通常要求在页表的每个条目中存储地址空间标识符，因为该表通常包含映射物理内存的几个不同地址空间。存储地址空间标识符可以确保特定进程的逻辑页面映射到相应的物理页面帧。使用倒置页表的系统包括64位UltraSPARC和PowerPC。 为了说明这种方法，我们描述了IBM RT中使用的倒置页表的简化版本。IBM是第一个使用倒置页表的主要公司，从IBM System 38开始，一直延续到RS/6000和当前的IBM Power CPU。对于IBM RT，系统中的每个虚拟地址都由三个部分组成：&lt;进程ID，页号，偏移量&gt;。每个倒置页表条目都是一对&lt;进程ID，页号&gt;，其中进程ID扮演地址空间标识符的角色。当发生内存引用时，虚拟地址的一部分，即&lt;进程ID，页号&gt;，被呈现给内存子系统。然后在倒置页表中搜索匹配项。如果找到匹配项，比如在条目i处找到了匹配项，那么将生成物理地址&lt;i，偏移量&gt;。如果找不到匹配项，那么就尝试访问非法地址。 尽管这种方案减少了存储每个页表所需的内存量，但当发生页引用时，搜索表所需的时间却增加了。由于倒置页表是按物理地址排序的，但查找是基于虚拟地址进行的，可能需要搜索整个表才能找到匹配项。这种搜索将耗费太长时间。为了缓解这个问题，我们使用哈希表，将搜索限制在一个或最多几个页表条目上。当然，每次访问哈希表都会向过程中添加一个内存引用，因此一个虚拟内存引用至少需要两次真实内存读取——一次用于哈希表条目，一次用于页表。（请注意，在查询哈希表之前首先搜索TLB，这会提供一些性能改进。） 使用倒置页表的系统在实现共享内存时会遇到困难。共享内存通常被实现为多个虚拟地址映射到一个物理地址。这种标准方法不能用于倒置页表；因为每个物理页只有一个虚拟页条目，一个物理页不能有两个共享的虚拟地址**。解决此问题的一个简单技术是允许页表仅包含一个虚拟地址到共享物理地址的映射**。这意味着对未映射的虚拟地址的引用会导致页面错误。 Oracle SPARC Solaris 考虑一个现代的64位CPU和操作系统，它们紧密集成以提供低开销的虚拟内存。在SPARC CPU上运行的Solaris是一个完全64位的操作系统，因此必须解决虚拟内存的问题，而不会用尽所有物理内存来保存多级页表。它的方法有点复杂，但是使用了散列页表有效地解决了这个问题。有两个散列表——一个用于内核，另一个用于所有用户进程。**每个散列表将内存地址从虚拟内存映射到物理内存。每个散列表条目表示一个连续的虚拟内存映射区域，这比为每个页面单独设置散列表条目更有效率。**每个条目都有一个基地址和一个表示条目代表的页面数的跨度。 如果每个地址都需要通过散列表进行搜索，虚拟到物理的转换将需要太长时间，因此CPU实现了一个TLB，用于保存转换表条目（TTE），以进行快速的硬件查找。这些TTE的缓存驻留在转换存储缓冲区（TSB）中，其中包括最近访问的每个页面的条目。当发生虚拟地址引用时，硬件会在TLB中搜索转换。如果没有找到，则硬件将通过内存中的TSB查找导致查找的虚拟地址对应的TTE。这种TLB遍历功能在许多现代CPU中都可以找到。如果在TSB中找到匹配项，CPU将TTE复制到TLB中，内存转换完成。如果在TSB中找不到匹配项，则会中断内核以搜索散列表。然后，内核从相应的散列表创建一个TTE，并将其存储在TSB中，以便由CPU内存管理单元自动加载到TLB中。最后，中断处理程序将控制返回给MMU，完成地址转换，并从主内存中检索所请求的字节或字。 Example: Intel 32 and 64-bit Architectures 英特尔芯片的架构在个人计算机领域主导了数年。16位英特尔8086出现在1970年代末，很快又出现了另一款16位芯片——英特尔8088，后者因为被用于最初的IBM PC而引人注目。8086芯片和8088芯片都基于分段架构。后来，英特尔推出了一系列32位芯片——IA-32，其中包括32位奔腾处理器系列。IA-32架构支持分页和分段。最近，英特尔推出了一系列基于x86-64架构的64位芯片。目前，所有最受欢迎的个人计算机操作系统都运行在英特尔芯片上，包括Windows、Mac OS X和Linux（当然，Linux也运行在其他几种架构上）。值得注意的是，然而，英特尔的主导地位并没有扩展到移动系统，ARM架构目前在移动领域取得了相当大的成功。在本节中，我们将分别讨论IA-32和x86-64架构的地址转换。 IA-32 系统 IA-32系统中的内存管理分为两个组件–分段和分页，工作原理如下：CPU生成逻辑地址，这些地址传递给分段单元。分段单元为每个逻辑地址生成一个线性地址。然后将线性地址传递给分页单元，分页单元再生成主内存中的物理地址。因此，分段和分页单元形成了内存单元(MMU)的等效。 IA-32 分段 IA-32架构允许一个段的大小最大为4 GB，每个进程的最大段数为16K。一个进程的逻辑地址空间被划分为两个部分。第一个部分包含最多8K个对该进程私有的段。第二个部分包含最多8K个在所有进程之间共享的段。关于第一个部分的信息保存在本地描述符表（LDT）中；关于第二个部分的信息保存在全局描述符表（GDT）中。LDT和GDT中的每个条目都由一个包含有关特定段的详细信息的8字节段描述符组成，包括该段的基址和限制。 逻辑地址是一个二元组(selector, offset)，其中selector是一个16位的数字： 其中s表示段号，g指示段是否在GDT或LDT中，p处理保护。偏移量是一个32位的数字，指定了所讨论段内的字节位置。 该计算机有六个段寄存器，允许进程同时访问六个段。它还有六个8字节的微程序寄存器，用于保存来自LDT或GDT的相应描述符。这个缓存使得 Pentium 能够避免在每次内存引用时都从内存中读取描述符。 在 IA-32 上，线性地址是 32 位长，形成如下。段寄存器指向 LDT 或 GDT 中的适当条目。关于所讨论段的基址和限制信息被用来生成线性地址。首先，限制信息用于检查地址的有效性。如果地址无效，则生成内存故障，导致操作系统产生陷阱。如果地址有效，则将偏移量的值加到基址的值上，得到一个32位的线性地址。 IA-32分页 IA-32 架构允许页面大小为 4 KB 或 4 MB。对于 4 KB 页面，IA-32 使用两级分页方案，其中 32 位线性地址的划分如下： 前 10 位高阶位引用了外层页表的一个条目，IA-32 称之为页目录。（CR3 寄存器指向当前进程的页目录。）页目录条目指向一个内部页表，该表由线性地址中最内层的 10 位内容进行索引。最后，低阶位 0 到 11 指的是在页表中指向的 4 KB 页中的偏移量。 为了提高物理内存的使用效率，IA-32 页表可以被交换到磁盘上。在这种情况下，页目录条目中使用一个无效位来指示该条目指向的表是在内存中还是在磁盘上。如果表在磁盘上，操作系统可以使用其余的 31 位来指定表的磁盘位置。然后可以根据需要将表调入内存中。 随着软件开发人员开始发现 32 位架构的 4GB 内存限制，英特尔采用了页地址扩展page address extension（PAE），允许 32 位处理器访问超过 4GB 的物理地址空间。PAE 支持引入的根本区别是，分页从两级方案变为三级方案，其中顶部两位指向一个页目录指针表。 PAE还将页目录和页表条目的大小从32位增加到64位，这使得页表和页面框的基地址从20位扩展到24位。结合12位偏移量，为IA-32增加PAE支持将地址空间扩展到36位，支持最多64GB的物理内存。需要注意的是，操作系统必须支持使用PAE。Linux和英特尔的Mac OS X都支持PAE。但是，即使启用了PAE，Windows桌面操作系统的32位版本仍然仅支持4GB物理内存。 x86-64 在历史上，AMD经常基于英特尔的架构开发芯片，但现在情况却有所不同，因为英特尔采用了AMD的x86-64架构。在讨论这一架构时，我们将使用更通用的术语x86-64，而不是商业名称AMD64和Intel 64。 64位地址空间的支持可以提供令人惊讶的2^64字节的可寻址内存，这个数字超过了16万亿。然而，尽管64位系统理论上可以寻址这么多内存，但在当前设计中，用于地址表示的位数远远少于64位。x86-64架构目前提供了48位虚拟地址，并使用四级分页层次支持4 KB、2 MB或1 GB的页面大小。由于这种寻址方案可以使用PAE，虚拟地址的大小为48位，但支持52位物理地址。 Example: ARM Architecture 尽管英特尔芯片在个人计算机市场上占据主导地位已经超过30年，但智能手机和平板电脑等移动设备的芯片通常采用32位ARM处理器。有趣的是，英特尔既设计又制造芯片，而ARM只负责设计芯片，然后将其设计授权给芯片制造商。苹果已经为其iPhone和iPad移动设备授权了ARM设计，而一些基于安卓的智能手机也使用ARM处理器。 32位ARM架构支持以下页面大小： 4KB和16KB页面 1MB和16MB页面(termed sections) 所使用的分页系统取决于是引用页面还是段。1MB和16MB段使用单级分页；4KB和16KB页面使用双级分页。ARM MMU的地址转换如下图所示 ARM架构还支持两级TLB。外层有两个micro TLB，一个用于数据，另一个用于指令。micro TLB也支持ASIDs。内层是一个主TLB。地址转换从micro TLB级别开始。如果发生未命中，则会检查主TLB。如果两个TLB都未命中，则必须在硬件中执行页表遍历。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"内存管理","slug":"内存管理","permalink":"https://blackforest1990.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"}]},{"title":"deadlock","slug":"deadlock","date":"2024-02-19T06:10:57.000Z","updated":"2024-02-19T07:16:09.737Z","comments":true,"path":"2024/02/19/deadlock/","link":"","permalink":"https://blackforest1990.github.io/2024/02/19/deadlock/","excerpt":"","text":"进程死锁算法 死锁问题 死锁问题 例子1：拥有2个磁盘驱动器的系统 假设有两个进程，P1和P2，每个进程持有一个磁盘驱动器，并且它们各自等待另一个进程持有的磁盘驱动器。这种情况创建了一个循环等待，因为P1正在等待P2持有的资源，反之亦然。 ​ ● P1持有磁盘驱动器1，等待磁盘驱动器2。 ​ ● P2持有磁盘驱动器2，等待磁盘驱动器1。 在这种情况下，循环等待条件得到满足，如果其他死锁条件也得到满足，就可能发生死锁。 例子2：信号量A和B，初始值为1 假设有两个信号量A和B，它们的初始值都为1。信号量通常用于控制对资源的访问。如果进程被设计为按照特定顺序获取这两个信号量，并以相反的顺序释放它们，可以防止死锁。然而，如果进程不遵循这个协议，就可能发生死锁。 ​ ● 进程P1获取信号量A，然后获取信号量B。 ​ ● 进程P2获取信号量B，然后获取信号量A。 如果P1持有信号量A并等待信号量B，而P2持有信号量B并等待信号量A，就会产生循环等待条件。 这两个例子强调了仔细管理资源的重要性，确保进程以受控的方式请求和释放资源，以避免循环等待和其他死锁条件。预防性的措施，比如仔细分配资源和使用同步机制，有助于减轻系统中死锁的风险。 桥梁交叉问题 桥梁交叉问题是一个经典的死锁场景，用于说明在资源争用的情况下可能发生的问题。这个例子描述了一座只允许单向交通的桥梁，桥上的每个部分都可以看作是一个资源。以下是该问题的一些关键要点： ​ 1. 交通只能单向： 只允许从一个方向通过桥梁，这意味着车辆只能按照特定的方向移动。 ​ 2. 桥上的每个部分是一个资源： 桥上的每个部分都可以被看作是一个独占资源，一次只能由一个车辆使用。 ​ 3. 死锁的解决： 如果发生死锁，可以通过一个车辆后退（放弃资源并回滚）来解决。也就是说，为了打破死锁，系统可以迫使某些车辆回到桥的起点，释放它们占用的资源。 ​ 4. 可能需要多个车辆后退： 如果死锁涉及多个车辆，可能需要多个车辆后退才能解除死锁。 ​ 5. 可能发生饥饿： 在解决死锁的过程中，某些车辆可能被迫多次后退，导致它们无法继续前进，从而可能发生饥饿问题。 在这个场景中，死锁可能发生在多个车辆试图同时通过桥梁的时候，由于资源争夺，它们彼此等待对方释放资源而无法继续前进。解决死锁的方法是引入一些机制，例如强制某些车辆后退，以打破循环等待并释放资源。 这个例子反映了在并发系统中管理资源的复杂性，以确保系统能够避免死锁和饥饿问题。 十字路口问题 在一个有四个停车标志的十字路口，通行权的规则通常旨在防止死锁，确保交通流畅。来自《加利福尼亚驾驶：道路规则和交通法规》的摘录提供了对规则的简明解释： 轮流通行： 驾驶员轮流通过十字路口，顺序是按照他们到达十字路口边缘的顺序。 同时到达： 如果两辆或更多车辆同时到达路口，让车辆靠右的原则生效，按照顺时针的次序通行。换句话说，靠你右边的驾驶员有通行权，先行驶。 特殊情况和谨慎驾驶： 摘录中承认这种方案可能不是绝对可靠的，特别是当四辆车同时到达的情况。在这种情况下，驾驶员需要保持警惕，密切关注，也许与其他驾驶员沟通，协调行车动作，确定谁先通过。 这些通行权规则有助于管理十字路口的交通，降低冲突或死锁的可能性。然而，正如提到的，驾驶员应该保持警惕，并准备适应特定情况，特别是在多辆车同时到达的情况下。沟通、眼神交流和耐心是顺利而安全地通过十字路口的关键因素。 System Model 死锁条件 互斥条件（Mutual Exclusion）： 同一时刻只有一个进程能够使用资源。这意味着资源是排它性的，不能被多个进程同时持有或使用。 持有等待条件（Hold and Wait）： 一个进程持有至少一个资源，并在等待获取其他被其他进程持有的资源。这就创造了一种资源占用和等待的情况，可能导致死锁。 无抢占条件（No Preemption）： 一个资源只能被持有它的进程主动释放，而且只有在该进程完成任务之后才能释放。系统不能强制性地从进程手中取走资源。 循环等待条件（Circular Wait）： 存在一个进程集合 &#123;P0, P1, …, P0&#125;，其中 P0 正在等待被 P1 持有的资源，P1 正在等待被 P2 持有的资源，以此类推，Pn-1 正在等待被 Pn 持有的资源，而 P0 正在等待被 P0 持有的资源。这形成了一个循环等待的环。 这四个条件的同时存在会导致死锁的发生。因此，在设计和管理系统时，通常需要采取措施来防止这些条件同时满足，以减少死锁的风险。这可能涉及到资源分配策略、死锁检测和恢复机制等。 处理死锁的方法 处理死锁的方法主要分为以下三种： 确保系统永远不会进入死锁状态： ​ ● 方法： 通过设计系统或引入合适的算法，以确保系统不会满足死锁发生的四个必要条件。这可能涉及合理的资源分配、资源请求顺序规定等。然而，这样的方法可能会导致系统资源利用率下降，因为它们可能需要较为保守的资源分配策略。 允许系统进入死锁状态，然后进行恢复： ​ ● 方法： 如果死锁已经发生，可以通过检测死锁的存在，选择性地终止一些进程，释放资源，并允许其他进程继续执行。这需要死锁检测和恢复机制，以及相应的策略来决定哪些进程应该被终止。 忽略问题，假装系统中不存在死锁： ​ ● 方法： 一些操作系统采用这种方法，即忽略死锁问题，假装死锁从不发生。这通常是因为死锁发生的频率相对较低，或者实施其他死锁处理方法可能导致系统过于保守，影响性能。虽然这是一种简单的方法，但也可能导致死锁时系统无法预测的行为。 每种方法都有其优缺点，选择取决于系统的需求、性能要求以及开发人员对死锁问题的处理偏好。在设计系统时，通常需要综合考虑这些方法，以找到适合特定场景的平衡点。 Deadlock Prevention (预防) 限制请求的方式： ​ ● 方法： 通过对进程请求资源的方式进行限制，可以防止死锁的发生。这可能包括对资源请求的顺序、数量或其他约束的限制。 互斥（Mutual Exclusion）： ​ ● 情况： 对于可共享的资源，互斥不是必需的；但对于不可共享的资源，互斥是必需的。 ​ ● 解释： 对于可共享的资源，多个进程可以同时持有，而对于不可共享的资源，只能由一个进程持有。这是死锁预防的一种方法。 持有和等待（Hold and Wait）： ​ ● 方法： 确保每当进程请求资源时，它不持有任何其他资源。 ​ ● 方式： 要求进程在开始执行之前请求并分配其所有资源，或者只允许进程在没有任何资源时请求资源（在请求任何其他资源之前释放所有当前资源）。 ​ ● 问题： 这可能导致资源利用率较低，且可能导致饥饿问题。 无抢占（No Preemption）： ​ ● 方法： 如果一个进程持有一些资源并请求另一个不能立即分配的资源，那么释放所有当前持有的资源。 ​ ● 操作： 被抢占的资源被添加到进程正在等待的资源列表中，只有在进程能够重新获得其旧资源以及新请求的资源时，才会重新启动该进程。 循环等待（Circular Wait）： ​ ● 方法： 强制对所有资源类型进行总排序，并要求每个进程按递增顺序枚举请求资源。 ​ ● 示例： 对于三种资源类型（磁带驱动器、磁盘驱动器和打印机），给定的排序如下： ​ i. F(tape drive) = 1 ​ ii. F(disk drive) = 5 ​ iii. F(printer) = 12 这些方法共同旨在通过限制资源的获取方式来防止死锁的发生。在设计系统时，选择适当的死锁预防策略可能取决于系统的性质和需求。 Deadlock Avoidance (避免) **死锁避免（Deadlock Avoidance）**是一种需要系统具有额外的先验信息的方法。其中，最简单且最有用的模型要求每个进程声明可能需要的每种类型资源的最大数量。死锁避免算法动态地检查资源分配状态，以确保永远不会发生循环等待条件。资源分配状态由可用资源数量、已分配资源数量以及进程对资源的最大需求定义。 具体而言，这个模型的主要特点包括： 先验信息： 每个进程在运行之前声明其可能需要的每种资源类型的最大数量。这是系统在运行时用于进行死锁避免的关键信息。 资源分配状态： 资源分配状态通过以下方面来定义： ​ ● 可用资源数量： 表示当前系统中未被任何进程占用的资源数量。 ​ ● 已分配资源数量： 表示已经被分配给进程的资源数量。 ​ ● 进程对资源的最大需求： 表示每个进程可能在未来请求的最大资源数量。 动态检查： 死锁避免算法会动态地检查资源分配状态，以确保不会形成循环等待条件。这意味着系统会根据当前的资源分配状态来判断是否允许某个进程继续请求资源，以防止死锁的发生。 总体而言，死锁避免方法试图在运行时动态地管理资源分配，以确保系统不会陷入死锁状态。这需要对资源需求和分配状态进行仔细的监测和控制。 Safe State 一个系统处于“安全状态（Safe State）”是指当一个进程请求一个可用资源时，系统必须决定是否立即分配该资源将系统置于安全状态。系统被认为处于安全状态，如果存在系统中所有进程的一个序列 &lt;P1, P2, …, Pn&gt;，使得对于每个进程 Pi，Pi 目前仍然可以请求的资源可以通过当前可用资源 + 所有 Pj 持有的资源来满足，其中 j &lt; i。 具体来说： ​ ● 如果进程 Pi 的资源需求当前不可用，那么 Pi 可以等待，直到所有 Pj 完成。 ​ ● 当 Pj 完成时，Pi 可以获取所需的资源，执行任务，释放分配的资源，然后终止。 ​ ● 当 Pi 终止时，Pi+1 可以获取其所需的资源，以此类推。 这种安全状态的概念是为了确保在系统进行资源分配时，不会出现死锁。通过仔细管理资源分配，系统可以保持在一个能够支持进程继续执行的状态，而不会进入死锁状态。在实际实现中，系统可能需要使用一些算法和数据结构来动态地维护和检查安全状态。 死锁避免算法 死锁避免算法通常分为两种情况，具体取决于资源类型的实例数： 单一实例的资源类型： ​ ● 算法： 在这种情况下，通常使用资源分配图（Resource Allocation Graph）来进行死锁避免。资源分配图是一个图形化表示，其中包含进程和资源之间的关系，通过请求边和分配边表示。通过分析图中的循环等待条件，可以预防死锁的发生。 多个实例的资源类型： ​ ● 算法： 在这种情况下，通常使用银行家算法（Banker’s Algorithm）。银行家算法是一种死锁避免的动态资源分配算法，它基于预先提供的信息（每个进程的最大资源需求）来判断在分配资源的情况下系统是否保持在安全状态。 ​ ● 原理： 银行家算法维护一个安全序列，该序列表示进程可以按顺序获取资源而不导致死锁。当一个进程请求资源时，系统模拟分配资源并检查是否仍然存在安全序列。如果存在，系统将实际分配资源；否则，请求将被推迟，直到满足安全性条件。 这两种算法分别适用于不同情况下的死锁避免。资源分配图适用于单一实例的资源类型，而银行家算法适用于多个实例的资源类型，其中有额外的信息可用于决策资源分配。选择适当的算法取决于系统的需求、资源类型和性能要求。 资源分配图方案（Resource-Allocation Graph Scheme） 通常用于表示系统中的资源分配情况和进程之间的关系。以下是关于资源分配图方案中一些关键元素的说明： 声明边（Claim Edge）： ​ ● 表示为 Pi → Rj，其中 Pi 表示进程，Rj 表示资源。 ​ ● 用虚线表示。 ​ ● 意味着进程 Pj 可能会请求资源 Rj。 请求边（Request Edge）： ​ ● 当进程请求资源时，声明边（Claim Edge）会转换为请求边。 ​ ● 表示为 Pi → Rj 的实线边。 ​ ● 表示进程 Pi请求资源 Rj。 分配边（Assignment Edge）： ​ ● 当资源被分配给进程时，请求边会转换为分配边。 ​ ● 表示为 Rj → Pi 的实线边。 ​ ● 表示资源 Rj 已被分配给进程 Pi。 释放资源： ​ ● 当进程释放资源时，分配边会重新转换为声明边（Claim Edge）。 ​ ● 表示资源 Rj 又可供其他进程请求。 资源的先验声明： ​ ● 系统中资源必须在先验声明（a priori）的情况下进行。 ​ ● 意味着在系统运行之前，资源的分配关系需要被预先声明。 这种资源分配图方案通常用于可视化和分析系统中的资源分配情况，特别是在处理死锁的上下文中。通过观察图中的边的转换，可以更好地理解进程之间的资源请求、分配和释放关系，从而有助于死锁的预防和管理。 银行家算法 银行家算法是一种用于死锁避免的动态资源分配算法。以下是银行家算法的基本假设： 多个实例： ​ ● 系统中存在多个资源实例，即每种类型的资源有多个实例可用。 先验声明最大需求： ​ ● 每个进程在运行之前必须先声明对每种类型资源的最大需求量。这是系统在运行时判断资源分配是否安全的关键信息。 请求资源可能需要等待： ​ ● 当一个进程请求资源时，如果系统当前无法满足其需求，进程可能需要等待，直到资源可用。 进程获取所有资源后有限时间内释放： ​ ● 当一个进程获得了其所需的所有资源后，它必须在有限的时间内释放这些资源。这确保了资源不会永远被某个进程占用，有助于避免死锁。 基于这些假设，银行家算法通过动态地检查每个进程的资源请求，判断系统是否能够保持安全状态，即是否存在一个安全序列，以确保没有死锁发生。算法基于以下原则： ● 安全状态判断： 如果进程请求资源后系统仍然能够找到一个安全序列，那么系统允许分配资源；否则，请求将被推迟，直到系统能够找到安全序列。 银行家算法的目标是防止死锁，同时尽可能满足进程的资源请求。这种方法需要维护有关进程和资源状态的信息，并根据这些信息做出动态决策，以确保系统的安全性。 数据结构 银行家算法需要维护一些关键的数据结构来进行动态的资源分配和死锁避免。以下是银行家算法中使用的主要数据结构： Available（可用资源向量）： ​ ● 类型：向量（Vector）。 ​ ● 长度：m，表示系统中每种资源类型的实例数。 ​ ● 意义：如果 Available[j] = k，则表示系统中有k个资源实例可用于资源类型Rj。 Max（最大需求矩阵）： ​ ● 类型：矩阵。 ​ ● 大小：n x m，其中n是进程数，m是资源类型数。 ​ ● 意义：如果 Max[i, j] = k，则表示进程Pi最多可能请求资源类型Rj的k个实例。 Allocation（分配矩阵）： ​ ● 类型：矩阵。 ​ ● 大小：n x m。 ​ ● 意义：如果 Allocation[i, j] = k，则表示进程Pi当前被分配了k个资源类型Rj的实例。 Need（需求矩阵）： ​ ● 类型：矩阵。 ​ ● 大小：n x m。 ​ ● 意义：如果 Need[i, j] = k，则表示进程Pi可能需要额外的k个资源类型Rj来完成其任务。 ​ ● 计算方式：Need[i, j] = Max[i, j] - Allocation[i, j]。 这些数据结构通过动态地跟踪系统中的资源状态和进程的需求，使银行家算法能够在运行时进行资源分配决策。通过比较 Need 和 Available 的值，算法可以判断是否可以满足某个进程的资源请求，以及该分配是否安全，从而避免死锁。 安全性算法 这是安全性算法的基本步骤，用于判断系统是否处于安全状态。这个算法用于银行家算法中，确保在动态分配资源的过程中系统不会陷入死锁。以下是步骤的详细解释： ​ 1. 初始化： ​ ● 让 Work 和 Finish 分别是长度为 m 和 n 的向量，其中 m 是资源类型的数量，n 是进程的数量。 ​ ● 初始化 Work 为 Available 向量的副本。 ​ ● 初始化 Finish[i] 为 false，表示没有进程已经完成。 ​ 2. 查找满足条件的进程 i： ​ ● 找到一个进程 i，使得： ​ ○ (a) Finish[i] = false（该进程尚未完成）。 ​ ○ (b) Need[i] ≤ Work（该进程的需求可以被当前可用资源满足）。 ​ ● 如果不存在这样的 i，则转到步骤 4。 ​ 3. 更新 Work 和 Finish： ​ ● 更新 Work 为 Work + Allocation[i]（释放资源给进程 i）。 ​ ● 将 Finish[i] 设置为 true，表示进程 i 已经完成。 ​ ● 返回到步骤 2，继续查找下一个满足条件的进程。 ​ 4. 检查所有进程是否都完成： ​ ● 如果 Finish[i] == true 对于所有 i（即，所有进程都已完成），则系统处于安全状态。 这个算法通过动态地模拟资源分配和释放的过程，以及进程的状态变化，来判断系统是否保持在一个安全状态。如果所有进程都能够完成并释放资源，那么系统被认为是安全的。这有助于避免潜在的死锁情况。 Deadlock Detection 等待图（wait-for graph） 在每个资源类型只有单个实例的情况下，可以使用等待图（wait-for graph）来检测死锁。以下是一些关键的步骤： 等待图的构建： ​ ● 节点： 图的节点表示进程。每个进程都是一个节点。 ​ ● 边： 如果进程 Pi 正在等待进程 Pj，那么在等待图中就有一条边 Pi → Pj。 定期检测循环： ​ ● 周期性地调用一个算法，该算法在等待图中搜索循环。如果存在循环，说明存在死锁。 循环检测算法： ​ ● 使用图论算法（例如深度优先搜索）来检测等待图中是否存在循环。如果找到了一个循环，那么系统处于死锁状态。 复杂度注意事项： ​ ● 检测图中循环的算法通常具有较高的时间复杂度，一般为 n^2，其中 n 是图中的节点数。这是因为通常需要遍历图的所有边和节点，寻找可能的循环。 使用等待图的方法是周期性地检查系统是否存在死锁。如果检测到循环，系统可以采取相应的措施来解决死锁，例如中断一个或多个进程，以破坏循环并解除死锁状态。 需要注意的是，尽管等待图是一种直观的死锁检测方法，但在大型系统中，由于其高时间复杂度，可能会导致性能问题。因此，在设计系统时，需要权衡使用这种方法的代价和效益。 深度优先搜索（Depth-First Search，DFS） 深度优先搜索（Depth-First Search，DFS）在死锁检测上的应用。在这里，等待图中的路径表示进程之间的等待关系，例如 P1 → P2 表示进程 P1 正在等待进程 P2。 DFS 是一种图遍历算法，它从起始节点开始，尽可能深入图中每个分支，直到达到最深的节点，然后回溯并探索其他分支。在死锁检测中，DFS 可以用于检查等待图中是否存在循环，从而判断系统是否处于死锁状态。 从您提供的示例中，以下是一些可能的路径： ​ 1. P1 → P2 → P3 → P4 → P1 ​ 2. P1 → P2 → P4 → P1 ​ 3. P1 → P2 → P5 ​ 4. P2 → P3 → P4 → P1 → P2 这些路径代表了等待图中的一些可能的循环。如果 DFS 在遍历等待图的过程中找到了任何形成循环的路径，就可以得出系统存在死锁的结论。 在实际应用中，DFS 可以用于周期性地检测等待图中的循环，从而提前发现潜在的死锁情况并采取相应的措施，例如中断某些进程以解除死锁。 Recovery from Deadlock 进程终止（Process Termination） 在死锁发生后，进程终止（Process Termination）是另一种可行的恢复机制。以下是使用进程终止来解决死锁的一些考虑因素和策略： 终止所有死锁进程： ​ ● 最直接的方法是终止所有死锁进程，释放它们占用的资源。这样可以消除死锁，但代价可能很高，因为它可能中断正在执行的进程。 逐个终止进程： ​ ● 逐个终止死锁进程，直到死锁循环被消除。这种方法更加灵活，可以选择性地中断进程，以最小化影响。 选择终止的顺序： ​ ● 在逐个终止进程的策略中，选择终止的顺序是关键的。以下是一些可能的选择标准： ​ ○ 进程优先级：选择优先级最低的进程。 ​ ○ 计算时间：选择计算时间最长或最短的进程。 ​ ○ 已使用资源：选择已使用资源最多或最少的进程。 ​ ○ 需要资源：选择需要资源最多或最少的进程。 ​ ○ 需要终止的进程数：选择需要终止的进程数最少的进程。 ​ ○ 交互性：考虑进程是交互式还是批处理。 实时性和用户体验： ​ ● 对于实时系统或交互式系统，需要权衡终止进程的紧急性和用户体验。有时可能需要中断对用户体验影响较小的进程。 资源回收： ​ ● 终止进程后，需要回收其使用的资源，以确保它们可供其他进程使用。 选择正确的终止策略和顺序取决于系统的特定要求、性能目标和用户体验。通常需要综合考虑多个因素，以最小化对系统整体性能和用户的影响。 资源抢占（Resource Preemption） 在死锁发生后，**资源剥夺（Resource Preemption）**是一种可能的恢复机制。这包括选择受害者进程，回滚其状态，然后重新启动。以下是相关的步骤和考虑因素： 选择受害者： ​ ● 选择哪个进程成为“受害者”是一个关键决策。通常，选择的标准是最小化代价。 ​ ● 代价可以包括回滚（Rollback）所需的成本，以及对系统整体性能的影响。 回滚操作： ​ ● 回滚是将进程的状态返回到先前的安全状态的过程。 ​ ● 回滚可能涉及释放已分配的资源，将进程状态还原到以前的状态，以及可能的清理工作。 成本因素： ​ ● 考虑到成本因素，选择受害者时需要综合考虑多个方面。可能的成本因素包括：回滚的步骤数。 ​ ○ 已分配资源的数量。 ​ ○ 对系统性能的影响。 ​ ○ 是否存在死锁的频率。 饥饿问题： ​ ● 在资源剥夺的过程中，存在一种可能性，即同一进程可能会一次又一次地成为受害者，导致饥饿问题。 ​ ● 为了解决这个问题，可以引入一些机制，例如在选择受害者时考虑其上一次回滚的次数，以确保资源剥夺是公平的。 实时性： ​ ● 对于实时系统，资源剥夺可能会影响任务的截止日期。因此，需要权衡实时性和系统状态的一致性。 资源剥夺是一种复杂的恢复机制，需要根据系统的需求和性能要求做出谨慎的决策。在实践中，选择合适的受害者和回滚策略是死锁处理中的关键问题。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"如何让英语成为你的工作语言","slug":"如何让英语成为你的工作语言","date":"2024-01-18T02:37:02.000Z","updated":"2024-01-19T02:26:34.766Z","comments":true,"path":"2024/01/18/如何让英语成为你的工作语言/","link":"","permalink":"https://blackforest1990.github.io/2024/01/18/%E5%A6%82%E4%BD%95%E8%AE%A9%E8%8B%B1%E8%AF%AD%E6%88%90%E4%B8%BA%E4%BD%A0%E7%9A%84%E5%B7%A5%E4%BD%9C%E8%AF%AD%E8%A8%80/","excerpt":"","text":"​ 很多时候，使用英文去沟通能成为一个不错的优势，因为国内的内卷和资方过于强势，去跨国公司和外企现在看来是一个不错的选择，如何组织会议，沟通客户和本地员工，是一门很重要的课程。本人做一个在跨国公司7年的老员工，去过很多国家，做过一些英文交流，水平也不高，在这里提出一些自己的想法，共同提高，共同进步。 看美剧不如聊两句 ​ 很多时候从应试教育出来的人，很难把自己变成做应用的人，也即分数思维如何向解决问题思维转变呢，我记得我第一次出差时，去的国家是阿曼的马斯喀特，是阿曼电信聊Uportal的业务，当时准备了很久，说的时候头脑一篇空白，两个服务的兄弟很好，帮助了我很多，后来我就经常跟本地员工去聊业务，最后渐渐英文也流利了起来。 后来我想一想为什么这样的方式能练好英文，第一，固定topic， 第二，你在技术上有优势，心理上有把握， 第三，内容你熟悉，只是转换成英文去说，我认为如果能够采取这样的方式去练习英文，可能进步速度会快一点。 workshop如何hold住全场 ​ 工作中，很多时候你需要去讲你写的slide，作为一名产品经理，给客户提出你的解决方案是最重要的，如何去做呢，首先材料不要求多，一般正式的proposal要求最多20页，按照2min一页的速度去讲解，材料不要求面面俱到，但写在上面的需要正确和能够合理解释。跟客户私下传递也建议打印一份一页key point，写出方案的优劣点。下图为客户与总部专家workshop中交流情况。 ​ 行业上的词汇其实你应该要牢记掌握，如果你技术上的词汇说的不够地道，会降低会议的演示效果，比如我作为一个通信产品经理，以下词汇其实是经常会见到的： 大部分电信的解决方案胶片都有八股文可以炒，先阐述客户集团和子网的数字化战略，然后分析现网情况，再根据现网情况提出菊厂的方案，再阐述菊厂的战略和投入，在说明应用case，如果能做到这样简明清晰，那么就是一篇很好地方案八股文了：） 客户关系呢？ ​ 客户关系是第一生产力，良好的客户关系能够帮助你在商业上获取信息，影响客户的决策链，如何跟客户成为朋友呢？ ​ 首先你首先要在工作上支持你的客户，如果你活干的不好，你大概跟客户成为不了朋友，如果犯了大错，可能还会使客户倒向友商，所以优秀的工作是一切的前提，然后可以请客户吃吃饭，可以在吃饭的时候谈一些工作上的topic，了解一下客户的兴趣爱好，后面，可以进一步跟客户去打高尔夫啊，网球之类的，注意，每一次跟客户交往，你都要准备几个话题，几个无关痛痒的和一些工作上你需要传递的，这样客户跟你在一起既轻松又可以实现突破。 ​ 最后，如果客户邀请你去他家了，这是一个重大的里程碑，这代表你的客户关系达到亲密的等级，在客户家里要注意客户的宗教信仰，人种，文化背景，教育背景，需要得体而同时活泼，这个时候才是客户关系的第一步，所以良好的英文沟通，让客户信任你跟你聊得来，是非常重要滴。 商务谈判如何做 有一本书叫逆势谈判说的很好，谈判不是辩论，不需要巧舌如簧，谈判是寻求合作，沟通能力很重要，所以商务谈判中的英文，需要精准，而不需要很流利，适当留白让大家去思考反而更有助益。 谈判是冲突各方通过设定立场、提出建议、做出取舍,交换价值,并最终达成共识的过程。谈判更多关注的是如何能实现自己的目标，谈判者并不会纠结于谁对谁错。所以在谈判中，关注的不应该是人，而是在于条款，如何通过博弈或者协商规划出一条对于双方有利的条款，非常重要，所以商务谈判中的英语最好做到精准，职业，非情绪化。 总结 最重要的要去说 提前准备topic 客户关系要一步一步做 商务谈判务求精准","categories":[{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"工作语言","slug":"工作语言","permalink":"https://blackforest1990.github.io/tags/%E5%B7%A5%E4%BD%9C%E8%AF%AD%E8%A8%80/"}]},{"title":"如何建立销售铁军","slug":"如何建立一所销售铁军","date":"2024-01-18T02:36:40.000Z","updated":"2024-01-23T14:55:34.769Z","comments":true,"path":"2024/01/18/如何建立一所销售铁军/","link":"","permalink":"https://blackforest1990.github.io/2024/01/18/%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E4%B8%80%E6%89%80%E9%94%80%E5%94%AE%E9%93%81%E5%86%9B/","excerpt":"","text":"本人在华为做过3年的解决方案产品经理，在一家小型环保公司做过市场总监，即做过销售也做过销售leader，如何通过管理手段，锤炼出能打胜仗的铁军呢。 招合适的人 ​ 首先明确你的项目定位，ToB还是ToC的，是IT行业还是传统行业？人才梯度是怎样的，高级销售员是否需要自带资源？如何验证这些资源是否可靠？如果做一个IT行业的ToB的项目，恐怕他的技术能力和对生态的理解是很重要的，如果是做一个环保行业的ToG项目，恐怕他的政策理解和资源能力是很重要的。对于一个销售员如何判断他是否具备自主销售能力？恐怕重点要考察的是他对某一领域的拓展路径，以及遇到种种困难的处理方式。 如下是一些常见的评估销售员的要素： 销售技能： 包括沟通技巧、谈判技能、客户关系管理等方面的能力。 产品知识： 销售人员需要深入了解所销售的产品或服务，以便能够有效地向客户传达价值。 目标达成能力： 评估销售人员是否能够实现销售目标，并在规定时间内完成销售任务。 客户服务： 考察销售人员对客户需求的理解和满足客户期望的能力。 团队合作： 对于团队中的销售人员，评估他们在团队中的合作和协作能力。 销售策略： 能否制定有效的销售策略，并根据市场需求做出调整。 问题解决： 销售人员是否能够迅速而有效地解决客户和销售过程中出现的问题。 市场洞察： 是否具备对市场趋势和竞争情况的敏感性。 这些要素是否对于你的项目都重要？恐怕评估的时候要通过权重的方式去考虑。 制度比拍脑袋重要 ​ 好的销售队伍就是一只好的部队，令行禁止，能打硬仗，赏罚分明，这些都取决于制度建设，可以分为项目维度和客户群维度分开考核，比如菊厂的LTC，立项以后，卷积相关人员，基于项目角度去管理一只队伍，基于项目定期召开项目分析会，同时团队成员的项目奖金包也跟项目贡献相符。基于大客户群的管理，双方有稳固的关系，这个时候就可以成立销售团队去维护这个客户群，梳理出需要突破的关键客户，做团队运作，奖金包也跟这个客户群的销售收入与回款挂钩，格局突破与销售增长还可以定向加倍激励，总之，对于销售制度，激励是一切的基础。 “Leads to Cash”（潜在客户到现金）通常是指从潜在客户（Leads）的阶段开始，通过销售和业务过程，最终将这些潜在客户转化为实际的销售收入或现金。这涉及整个销售和营销生命周期的过程。 以下是“Leads to Cash”过程的一般步骤： 潜在客户（Leads）生成： 通过各种营销活动（如广告、社交媒体、市场活动等），公司吸引并获取潜在客户的信息。 潜在客户资格验证： 对潜在客户的信息进行验证和资格审查，以确保他们是有兴趣的、符合目标市场的潜在客户。 销售线索转化： 将合格的潜在客户转化为销售线索，开始进行个性化的销售活动。 销售活动和谈判： 销售团队与潜在客户互动，进行谈判、演示产品或服务，并解答他们的疑虑。 销售订单生成： 当潜在客户决定购买时，生成销售订单，明确交易的条款和条件。 交付产品或服务： 提供所销售的产品或服务，确保客户满意。 发票和付款： 发送发票，并确保客户按照协议的条件进行支付。 现金收款： 收到客户的支付，并将其转化为公司的现金收入。 整个过程涉及多个团队和部门，包括市场营销、销售、客户服务和财务等。通过有效的“Leads to Cash”流程，企业可以最大程度地优化他们的销售生命周期，提高客户满意度，并实现更好的财务绩效。 ​ 同样，惩罚措施重要吗，很重要，但是不必写入制度，制度上的制订要有灰度，不能轻易去惩罚员工，可以基于事情去批评员工，但是惩罚会使员工流失，不建议。 ​ 通过制度去流动和评判员工，而不是通过领导好恶，销售工作中，业绩是最重要的，同时，项目分析会和总结也需要纳入整体系统评价中，保证销售数据纳入系统。 执行力胜过一切，能抗住强大压力吗？ ​ 很多时候，销售是执行力的艺术，市场是瞬息万变的，强大的执行力和百折不挠的勇气让这支军队更强大，这也是评价销售人员能否晋升的评价因素，一个销售，水平低不怕，但是主管的要求执行不下去，恐怕也是要被淘汰的。回想我的职业生涯，如果能在非洲扛过强大的压力，努力提升自己，现在人生的境况会不同吗？ ​ 对于一线的客户关系，我感觉遗憾还是蛮大的，没有能跟客户在私下里有交往，Rohitash和Dogals我认为我当时真的是有机会发展成我自己的客户，他们对我的印象都挺好，但是当时我并没有主动去做客户关系，这主要还是执行力不够。在一线，我也没有真正去面对压力，总是逃避，造成了压力越来越大，最终崩溃，逃跑，这都是人生的血泪史。汉高于荥阳成皋，光武于昆阳河北，魏武于濮阳官渡，直面压力，才能获得最宝贵的东西。 总结 强大的销售铁军三要素： 合适的人 好的制度 优秀的执行力","categories":[{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"销售管理","slug":"销售管理","permalink":"https://blackforest1990.github.io/tags/%E9%94%80%E5%94%AE%E7%AE%A1%E7%90%86/"}]},{"title":"电信运营商产品经理复盘","slug":"电信运营商产品经理复盘","date":"2024-01-18T02:36:22.000Z","updated":"2024-01-18T06:07:38.928Z","comments":true,"path":"2024/01/18/电信运营商产品经理复盘/","link":"","permalink":"https://blackforest1990.github.io/2024/01/18/%E7%94%B5%E4%BF%A1%E8%BF%90%E8%90%A5%E5%95%86%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E5%A4%8D%E7%9B%98/","excerpt":"","text":"作为一名菊厂曾经的运营商一线产品经理，对于电信运营商的采购流程有一定了解，做为一个高可靠性，重资产项目，如何推进项目，如何立项，投标，如何影响客户决策链，笔者根据自己的项目经验，现做如下总结归纳。 电信运营商组织架构 电信运营商，即网络架设商，比如中国的移动，联通，电信就是此类运营商。根据传输方式，分为固定运营商和移动运营商。 运营商多采用集团-&gt;子网模式经营，因为运营商多在不同国家运营，所以会在每个国家建立子网子公司，由集团统一管理，下图为中国电信的集团与子网。 笔者曾经深度参与过MTN的项目，子网为乌干达，他的集团和子网架构是这样的。 子网话语权是根据收入来的，比如南非子网，他的CEO的权力就很大，集团也不能过多干涉，但是乌干达这个收入少的子网，很多战略上就不能跟基团相悖，基团会在每年Q1确定子网的预算，由子网做预算分析，确定下一年的Capex和Opex，200万美金以内的扩容项目可以在子网执行，搬迁和新建项目都要向基团汇报。 项目组织架构匹配 电信运营商菊厂主要是采用铁三角（AR+SR+FR）管理客户群的，AR为客户关系，SR为解决方案，FR为服务，主要采取层级一一对应。 子网： CEO/CTO -&gt; 系统部部长， CTO/网络Director -&gt; 客户经理和， 工程Director/CTO -&gt; PM/PD/TD, 网络Director -&gt;产品经理 集团也是同样，大T系统部需要按照层级一一对应。 客户网络Director客户画像 Dogals： 本地黑人，穆斯林，英国曼彻斯特大学硕士毕业，在MTN工作12年，为网络规划部 Director，精通核心网 IPcore IPran，团队8人（3人wireless + 3人固定 +2 人核心网），性格上比较绅士，基本上不会投诉，偏向菊厂，关系较好，能够正常出入客户办公室，可以周末去客户家里聚餐，邀请客户打高尔夫。 项目运作 菊厂对于电信项目有传统的三板斧，主要是指邀请客户参观公司总部、邀请客户参加峰会和展会，并与华为的高层形成互动，建立样板点并邀请客户参观，对于Uganda MTN搬迁现网CS，华为的电信云化解决方案，符合MTN集团CTO的数字化转型战略，同时Uganda MTN为经营比较好的子网，客户不吝于技术上的投入，本项目为经典的技术牵引商业。乌干达子网的经营数据如下： “EBITDA” 是 “Earnings Before Interest, Taxes, Depreciation, and Amortization” 的缩写，即利息、税项、折旧和摊销前利润。EBITDA是一种财务指标，用于衡量公司的经营绩效和盈利能力。 该项目具体运作方法如下： 项目 描述 概述 乌干达MTN语音网络设备老旧，已经面临转型，同时语音收入占总体收入一半以上，客户同样要求业务继承性以及交付的稳定性 战略 集团的数字化转型战略为不可逆的行为，现网EOL的设备都会走云化路线，菊厂内部立项，形成代表处-地区部-机关人员构成，确定职责，汇报方式，定期开项目分析会。 邀请客户参观公司总部 邀请子网客户CTO回国，通过跟总部研发和交付人员的密集交流，CTO打消的疑虑，认可了传统电信设备走云化转型的价值，同时对于稳定搬迁核心语音网络有了技术信心 引导发标 子网CTO向南非集团CTO汇报，发标搬迁现网CS网络，通过技术屏蔽了N和Z厂家 方案价值 现网PS云平台扩容，核心网一朵云，管理方便，考虑未来Uganda语音仍在增长，会节省Opex和Capex 标后澄清 跟子网CTO提出扩容割接方案，通过现网PS扩容，多采购一批硬件，用于CS，基于PS的PO启动CS交付，割接了10%的流量上来，现网运行稳定 商务谈判 子网正式中标，集团汇报过会，商务谈判中，对于搬迁部分给与折扣，后续扩容没有影响，考虑硬件部分已经在PS PO中执行完毕，主要优惠部分为客户现网用户license，所以本项目商务较为优秀 客户决策链如下 子网CTO -&gt; 集团CTO -&gt; 子网制作标书-&gt; 集团过会 -&gt; 子网发标 -&gt; 中标 -&gt; 商务谈判 -&gt; 子网采购流程 -&gt; PO 总结 战略永远是最重要的 客户的经营永远是第一驱动力 永远向着议标去努力，如果不行，合同条款是生命线","categories":[{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"电信运营商","slug":"电信运营商","permalink":"https://blackforest1990.github.io/tags/%E7%94%B5%E4%BF%A1%E8%BF%90%E8%90%A5%E5%95%86/"}]},{"title":"政府的生意怎么做","slug":"政府的生意怎么做","date":"2024-01-18T02:36:03.000Z","updated":"2024-01-22T12:01:28.434Z","comments":true,"path":"2024/01/18/政府的生意怎么做/","link":"","permalink":"https://blackforest1990.github.io/2024/01/18/%E6%94%BF%E5%BA%9C%E7%9A%84%E7%94%9F%E6%84%8F%E6%80%8E%E4%B9%88%E5%81%9A/","excerpt":"","text":"​ 很多时候，政府的生意都很难做，冗长的付款流程和期限，复杂的客户关系，回款风险较大，笔者在从事ToG的环保公司工作了一年，对于ToG领域有一定的了解，现在对于这个领域做出总结，希望能够帮到大家。 政府组织架构 ​ 首先确定项目的类型，比如笔者接触了一年的污水处理项目，国内污水处理项目基本分为三种模式，政府自投自建自营，收益为公共事业基金收益；特许经营模式，社会资本购买污水处理项目的特许经营权并获得污水处理厂的物权，特许经营期满移交回政府实施机构或政府指定机构；PPP模式，其中较为典型的模式是社会资本建设污水处理工程，并实施运营，期间获得使用者付费，收回建设投资并获取收收益。 ​ 根据项目性质，确定政府组织结构如下： 中央层级： 国家发展和改革委员会（NDRC，国家发改委）： 负责经济和社会发展规划，可能涉及项目的规划和资金安排。 生态环境部（原环境保护部）： 主管全国范围内的环境保护工作，负责污水治理政策的制定和实施。 水利部： 负责水资源管理，可能与污水处理项目中的水质、水量等相关事务有关。 住房和城乡建设部： 在城市污水治理项目中可能参与城市基础设施和规划。 地方层级： 省级生态环境厅/局： 在省级范围内负责环境保护和污水治理工作。 地市级生态环境局： 在地市级别进行具体的环境监管和治理工作，包括污水处理。 水务局： 在地方层级，水务局可能负责水资源管理和水环境保护，与污水处理相关。 城市规划和建设部门： 负责城市基础设施规划和建设，包括城市污水处理设施的规划和建设。 相关协会和研究机构： 中国城市水务协会： 作为一个行业协会，可能参与行业内的交流、合作和标准制定。 中国环境科学研究院等研究机构： 参与环境科学研究，为政府决策提供科学依据。 以上是主要的污水处理方面的组织架构，各级政府机构在负责本级别事务的同时，也执行中央政府的政策和指令。协调各级政府和相关机构之间的合作至关重要，以确保污水治理工作的高效实施。请注意，政府组织架构可能会根据时代和政策的变化而调整。 政府采购流程 政府采购是指政府机关、事业单位、公有制企事业单位以及其他使用财政资金的组织，为了履行职责，通过市场行为获取特定商品和服务的过程。 江苏省财政厅政府采购处日前发布了6张操作流程图，包括公开招标操作流程图、竞争性谈判操作流程图、单一来源操作流程图、邀请招标操作流程图、询价操作流程图、竞争性磋商操作流程图。 这几种招标方式有一些关键区别，主要体现在供应商的选择方式、投标过程以及采购合同的达成方式上。以下是它们的主要区别： 公开招标： 供应商选择： 所有合格的供应商都可以参与，公开竞争。 投标过程： 公开发布招标公告，潜在供应商可以获取招标文件并提交投标文件。 合同达成： 通过公开评标，中标供应商与采购方签订正式合同。 竞争性谈判： 供应商选择： 需要事先确定一些资格供应商，然后邀请其参与谈判。 投标过程： 邀请特定的供应商参与竞争性谈判，谈判过程中商议合同条款。 合同达成： 最终确定中标供应商，签订正式合同。 单一来源： 供应商选择： 仅选择一个供应商，通常因为特殊原因，如技术专业性、专有权等。 投标过程： 直接与选定的供应商进行谈判和洽谈。 合同达成： 签订正式合同。 邀请招标： 供应商选择： 从已有供应商资格名单中邀请特定的供应商参与招标。 投标过程： 邀请特定供应商参与，发放邀请函，供应商提交投标文件。 合同达成： 通过评审确定中标供应商，签订正式合同。 询价： 供应商选择： 向多个供应商发出询价函，通常是小额采购。 投标过程： 供应商提交报价，采购方评审并选择最合适的报价。 合同达成： 签订正式合同。 竞争性磋商： 供应商选择： 事先确定一些资格供应商，然后邀请其参与磋商。 投标过程： 邀请特定的供应商参与，进行竞争性磋商，商议合同条款。 合同达成： 最终确定中标供应商，签订正式合同。 这些方式的选择通常由采购项目的性质、法规规定以及采购方的需求决定。 客户关系 根据做了一年政府部门的项目，我对于B2G的项目有一定的了解，基本逻辑也是按照B2B的方式去做，建立线索，验证线索，线索向机会点转化，客户群管理，投标管理，商务管理，合同管理，与2B最大的差别，2G的项目要求更稳定一些，周期也比2C的项目长，所以项目的稳定性更重要一些。 相比直接硬怼耗精力在“最终客户”上，不如先摸清楚外围生意网络，找到靠谱“中间人”，对于政府资源，总是周围有一圈人的，比如揭阳污水项目，林总在项目中起到的作用就非常大，但是这种模式，基本上销售毛利率不会很高，因为对于你来说始终产生不了关键客户关系。所以总体来说，ToG项目很多时候是在做经销商管理。 对于经销商要建立清晰的合作协议： 制定明确的合作协议，确保协议中包含了双方的权利和责任，包括价格政策、销售目标、付款条款等。合作协议应该能够阐明各方的期望。一旦建立关系，要充分放权，充分沟通，同时制订合理的激励计划和技术支持，形成良性循环。 在建立好完善的经销模式后，商业模式有了正反馈，再考虑去做2G的大项目，即EPC模式：EPC（Engineering Procurement Construction），是指承包方受业主委托，按照合同约定对工程建设项目的设计、采购、施工等实行全过程或若干阶段的总承包。并对其所承包工程的质量、安全、费用和进度进行负责。对于这种模式，交付与付款周期都是大挑战，虽然项目规模大，但是收尾困难，项目初期需要对于招标方的资质，付款能力，领导情况做深入评估，人员高度卷积，客户关系深度管理，该项目虽然盈利水平不高，但是对于公司在行业内的口碑都有正向作用，但是如果不能合理评估项目困难，盲目卷入，可能会造成巨大的灾难性亏损。 ToG客户关系分析 TOG政务市场面对的用户主要是是公务员、事业编人员，这个人群大多是高知人群，从学历、社会地位、经济收入、家庭背景都占有相当的优势。由于在政府工作，习惯政令行事、层级森严、派系林立的环境，工作任务固定、重复性高、失误率低的要求。用户主要可以分为以下4类： 决策人员：树立标杆、体现政绩； 管理人员：做好监管、规避风险； 业务人员：流程规范、操作流畅； 广大群众：减少环节、方便快捷； TOG产品的利益相关方通常比较多，且各自需求不一致。对于乙方公司来讲，决策者的需求当然是最重要的，为了更好表达各干系人的需求，通过下面用户心理需求模型来说明，越往上的需求有优先级别越高。 1. 可靠性 可靠性是政务市场的基本要求，包括：项目能够顺利完成；可靠运维服务；数据安全。另外，政府客户对数据安全可靠特别重视，通常在政务内网，政务外网部署，保障数据安全。如果考虑上云部署，要做好保障数据安全的解决方案。 2. 风险控制 政府工作会对工作进行溯源问责，例如办理过程的记录，系统的操作记录，在发生争议或问题的时候，能够快速定位原因。因此，在产品设计过程要关注风险控制。 3. 体现政绩 驱动决策者来做某个项目，都是基于政策和考评要求。发达地区政府追求标杆项目，其他地区追求地方特色，因此定制化需求就比较多。政府之间经常会有考察学习的活动，经常通过可视化大屏展示政绩成果。大屏展示体现了地方特色，很难做到标准化。 4. 效率提升 效能办会对人员每月进行工作量统计，绩效考核，这个时候如何提升效率就显得尤为重要。 5. 更好的体验 所有的角色都需要更好的体验，但使用系统最多的业务人员和人民群众对体验要求最甚。 不像TOC产品那样，基于收集大量用户数据建立画像，TOG产品更多通过行业分析、客户调研来构建客户画像。通过构建客户画像，有助于精准营销，针对性行业客户特点开展售前营销，也有助于项目需求调研前期的背景分析。 TOG的客户，还具有以下特点： 1. 用户非客户 TOG的客户和某类用户可能之间只存在服务与被服务，监管与被监管的关系。 2. 客户往往是行业专家 客户通常是某个政府行业领域的专家，比如城管局，药监局，交管局等等。这就需要我们去拜访客户之前，对行业有着充分的理解，了解产品需要解决该行业什么痛点，才能够有机会取得客户的信任。 3. 潜在客户难以触达，特别是决策者 因为决策者往往是领导，而除非有特别的关系，或者是客户主动找到我们，领导往往不太容易见到。因此这就更需要我们在关键节点之前，做好充分准备。 4. 客户决策流程较长 政府端通常有一个决策流程，我们能够接触到的，一般是执行者，而执行者往往需要向领导汇报，如果关系到位，可以在关键节点见到决策者，这个时候就需要把握机会，汇报关键内容，争取决策者的信任。预算也是非常关键的因素，弄清楚政府部门在这方面的预算，是至关重要的，预算直接关系着决策的结果。 通过客户画像提供了客户特征信息，了解项目背景。还要了解用户建设该项目的动机和背后存在的痛点的关键环节，基于不同的动机，客户对系统的要求也是很大差别的。 政务用户需求通常是由上而下，上级领导掌握着全部的需求决策权。通过用户心理需求模型，在调研过程要抓住主要核心人物（决策人员）的需求，直接决定着需求调研的成功与否。想尽办法获取最有用的信息，利用现场访谈、观察、查阅资料等方法，可以采用直接了解和曲线了解需求。","categories":[{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"与虎谋皮","slug":"与虎谋皮","permalink":"https://blackforest1990.github.io/tags/%E4%B8%8E%E8%99%8E%E8%B0%8B%E7%9A%AE/"}]},{"title":"云计算架构设计总结","slug":"云计算架构设计","date":"2024-01-18T02:35:45.000Z","updated":"2024-03-21T03:05:01.701Z","comments":true,"path":"2024/01/18/云计算架构设计/","link":"","permalink":"https://blackforest1990.github.io/2024/01/18/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"什么是云计算？美国国家标准与技术协会（NIST）对此有这样一个权威和经典的定义： “所谓云计算，就是这样一种模式，该模式允许用户通过无所不在的、便捷的、按需获得的网络接入到一个可动态配置的共享计算资源池（其中包括了网络设备、服务器、存储、应用以及业务），并且以最小的管理代价或者业务提供者交互复杂度即可实现这些可配置计算资源的快速发放与发布。” 云计算的核心可以用五大基本特征、三种服务模式以及四类部署模式来概括。五大基本特征是：按需获得的自助服务，广泛的网络接入、资源池化、快捷的弹性伸缩以及可计量的服务。三种服务模式为：云基础设施即服务（IaaS），云平台即服务（PaaS），以及云软件即服务（SaaS）。四类部署模式可以划分为：专有云（私有云）、行业云、公有云，以及混合云。 云计算把一个固定CAPEX投入的IT项目变成一个PAYU 的OPEX项目, 因为成本和方案的优势，云计算伴随着企业数字化转型，被越来越看重，如何去做企业云化的架构设计，被讨论的越来越频繁，架构不应该只是考虑技术上的可行性，同时要考虑能否快速迁移，能否进行人员培训提高企业的DevOps能力，能否随着数字化转型成功给企业带来正向收益，这些要素同样需要纳入架构的设计中。无论IT架构如何螺旋式演进，客户价值和驱动力都体现在： 更低的TCO； 更高的业务部署与生命周期管理效率； 更优的业务性能与用户体验。 云计算的商业动力与技术趋势 用户痛点 在传统IT体系架构下，当前企业基础设施建设与运维所面临的核心痛点问题可以总结概括为如下几点。 平均资源利用率及能耗效率低下：各个IT基础设施单部件的选型、数量以及不同部件的组网连接方案均取决于企业IT收集的各业务部门对于IT核心业务处理量需求的预测和规划。同时所有企业IT应用软件、数据库以及中间件软件均采用独占计算、存储和网络资源的烟囱式部署。软件应用与硬件唯一捆绑，不同应用之间无法动态、高效共享相同的计算与存储资源。加之按照摩尔定律不断翻番增长的CPU计算能力已大大超出应用软件对计算资源利用率的同步能力，导致企业IT的平均资源利用率始终处于低于20%的水平。 新业务上线测试周期长，效率低下：企业任何一项新业务上线，从最基础的硬件平台开始，向上逐层延伸至操作系统、中间件、数据库、CRM/ERP/HRM/PDM/Email/UC等各类业务关键软件堆栈，均需要投入IT专业化团队，进行软件安装、调试、功能与性能验证测试、网络配置及修改调整，然后经过若干轮测试、故障及性能稳定性测试定位及重配置和调整之后，才能最终达到期望正式上线运行的成熟度水准。这个过程一般需要长达2至3个月的时间。 资源储备及弹性伸缩能力不足，不具备应对企业IT突发业务高峰处理的能力：针对特定垂直行业短时间内突发性的高流量、高密度业务需求（比如节假日期间对视频网站的突发业务流程冲击），企业内部物理基础设施资源往往无法满足短时间内迅速获取所需资源的需求，以及处置业务高峰过后的资源闲置问题 企业核心信息资产通过个人办公PC/便携外泄的安全风险：部分企业核心信息资产通过员工个人PC电脑或便携设备外泄给竞争对手，对企业竞争力和商业利益带来负面影响。过分严格的信息安全管控措施又导致了工作效率的下降，企业管理层及员工无法便捷地通过无所不在的网络访问企业防火墙内部的信息资产。 中小型企业希望通过宽带网络管道，从运营商托管应用数据中心“按需获取”其所需的企业IT应用能力：数量众多的中小企业，缺少IT领域专业经验，甚至没有财力和精力建设和维持自己专属的IT部门以及IT基础设施平台，普遍希望可以直接从托管运营商那里获取支撑其日常业务运作所需的SaaS服务。 针对解决上述企业IT系统建设和维护过程中遇到的普遍痛点问题，迫切呼唤业界IT软硬件解决方案提供商借助云计算技术，打造TCO、性价比与效率最优的“IT基础设施私有云及公有云”，具体包括： 面向大型企业和行业领域提供全自动化管理、一站式交付、支持与企业ITIL无缝集成融合、TCO最优化的端到端解决方案，实现企业传统IT基础设施及应用的改造、扩容和新建； 面向中小型企业，提供支持多租户安全隔离与动态发放、超大规模资源池调度管理、可最大限度发挥规模经济效益的公有云托管解决方案。 云计算的商业动力：企业ICT转型 互联网企业的核心竞争力 复杂盈利模式 在盈利模式上，相比传统企业，互联网公司最终设计了更为复杂、先进的盈利模式，即我们通常所说的“羊毛出在狗身上，猪来买单”。相比传统企业，这种盈利模式的好处是，用户接受互联网公司的产品和服务几乎没有利益付出的负面障碍。而最终为互联网提供免费服务的付费者也认为其每笔付出均有所值（这些付费者可能包括风险投资商、在互联网上做广告的企业，或者在互联网平台进行产品销售省去了渠道成本的企业）。这种复杂盈利模式让互联网公司的用户数量可以呈爆发式的增长，从而形成对传统企业的一项巨大的竞争优势。如今互联网公司所设计的盈利模式更加复杂，已经不再是简单的羊、狗、猪这种三方的关系了，而是贯穿全产业链、全商业运作的一种生态模式。而传统企业如今还是以简单的商品买卖这种古老的交易模式为主。这种盈利模式的竞争力差异，可能会让传统企业最终落到“把自己卖了，还在替别人点钱”的尴尬境地。 极低成本 互联网公司的这种复杂盈利模式，必须在规模效应（需要时间培育）下才能获得回报，而放大规模，需要更高的经营成本。这就要求互联网公司，为了活到能够盈利，必须再绞尽脑汁考虑如何降低自身的运营成本。构成互联网公司的主要成本主要是经营互联网网站所需的IT设备成本、IT软件成本、机房租赁成本、网络成本、办公场地成本、人员成本、市场广告成本等。其中IT设备、软件机房、网络以及运维人力的成本是互联网公司的最大成本源。对此，互联网公司不能再购买昂贵的商业IT产品，如产自知名IT厂商的小型机、数据库、操作系统等。他们只能寻求最便宜的解决方案，那就是我们现在看到的x86硬件和几乎全部基于开源软件一起构建的云计算平台。这个云计算平台相比传统企业使用的小型机、商业数据库、高端存储、商业应用软件等方式，成本至少下降了80%以上。而通过云计算的自动化运营技术，其大幅降低了运维人力的需求，一个运维人员可以管理数千台乃至上万台的IT设备。同时，基于云计算平台，其对机房基础设施也进行了优化改造，降低机房的能耗，即电力成本与场地成本。在业务上层，互联网公司千方百计地推进业务流程自动化的工作，使得大量传统企业人工流程在互联网平台上实现全自动化的处理，大幅降低了业务处理成本。 而传统企业，在丰厚的业务利润的滋养下，以及IT部门所处的企业非核心地位下，根本没有动力和条件向互联网公司那样拼命地降低IT成本。在大企业病的氛围下，传统企业所谓的降低成本，往往仅仅是为了一个漂亮的财务报表。实现每年10%～20%的成本下降幅度，即可以完美地达成当年业绩。在这种冰火两重天的环境下，让互联网公司在10年左右的时间里，大幅度地拉开了与传统企业的成本领先优势。如今，在传统金融行业每发放一笔贷款的成本竟然是互联网金融企业的1000倍。 极度的敏捷 为了快速推出业务，互联网公司也是无所不用其极。相比传统企业，互联网公司从人员组织架构、企业文化、经营模式、IT基础设施平台等方面都做了大幅改进。在组织架构上，即使是大型互联网企业，也放弃了传统企业那种逐层审批、大小领导签字画押的环节。业务研发团队自行进行业务决策，缩短内部业务研发外的时间。在企业文化上，为了业务快速上线，没日没夜的工作变得稀松平常，上线后再休息。在经营模式上，也完全打破传统企业把产品完美化再推向市场的策略，而是让位于业务上线时间，互联网公司让业务先上线，通过上线后用户反馈，再继续不断优化产品（规避了传统企业普遍存在的那种闭门造车的模式）。为了加快业务研发进度，在IT基础设施平台方面，互联网公司必须考虑用一个自动化的工具平台，利用这个平台，可以在最短的时间，开发出业务应用，并可以灰度发布，上线后还可以不断地继续完善。这就是我们已经熟知的云计算PaaS平台。经过这种为了加快业务上线速度进行的企业工作流程、组织架构、企业文化、IT平台的重构，互联网企业实现了新业务从研发立项到上线周期不会超过2周，最短只需不到2天的敏捷度。 大数据寡头 至今，大型互联网公司拥有几千PB的数据已经稀松平常了，领先的互联网公司已经走向EB乃至ZB的数量级。而大型传统企业所拥有的数据量，也不过几PB到几十PB，拥有几百个PB数据的传统企业已经少之又少了。双方仅在数据量上就已经达到上百倍的差距。这还只是数据的数量，还不算质量，在数据质量方面，互联网公司对消费者（个人）信息的掌握更是拥有巨大的优势，大型互联网公司的用户数量都是以亿为单位。个人的几乎所有活动信息都会呈现在互联网之上，包括但不限于个人的姓名、电话、住址、社会家庭关系、活动轨迹、资金关系、资产数额、知识能力、个人喜好、照片、影像、银行账号、社会交流、商务交流等。除了个人信息，还有大量的企业信息，包括企业（特别是网上开店企业）的所有经营活动、资金活动、客户信息、市场状况、销售活动、广告活动等。当所有的个人信息和企业信息汇聚起来，又形成了整个经济数据。任何经济领域的风吹草动，都不会逃过互联网厂商大数据监测与分析系统的法眼，而且在信息获取时间方向上比传统企业或机构大幅领先（比如某互联网公司公布的大企业景气指数曲线与国家统计局的指数曲线基本相同，但发布时间却比统计局的提前了5个月以上）。互联网企业获取的精准经济数据又可以反过来进行各种金融与市场商业活动。除了数据质量，在大数据处理技术上，互联网厂商也走在了前列。当大部分传统企业还在靠人工进行市场、经营与投资活动的时候，互联网公司已经开始进入了机器智能主导下的信息收集、分析、决策、处理的时代。 传统企业的ICT转型 传统企业应先认清自己 对于传统企业而言，有个重要的疑问需要解决，那就是为何互联网等信息化平台会成为所有企业的业务核心，即提高IT部门的地位，解决这个疑问的最佳答案是德国提出并已经局部实现的工业4.0概念。在工业4.0的场景下，一个制造企业，从产品的需求提出，到产品设计、原型生产、小批量试制、中等规模试制、测试验证，到大规模生产、物流仓储，再到市场销售的全环节、全流程，全部通过IT系统与互联网体系主导完成。人力工作只是在当前计算机设备能力有限的设计阶段和流程的规范性方面进行有限干预而已。 再以政府运营为例，政府未来的运营路径又是如何呢？政府作为公益、监管、执法机构，最高效率的运作就是利用互联网信息化的技术手段，打通与国家公民中所有企业、个人信息连接的壁垒，构建一个密集网状的信息化公共治理平台。比如：企业与个人的纳税可以在公共交易平台的交易与支付瞬间同时完成，税率也可以实现高度的定制化、个性化，可以针对不同企业经营与家庭状况收取不同税负并可调节。在这个公共信息化平台之上，所有的企业销售的商品与服务全程可追溯。而（企业）公民双方争议的调解和裁决大部分也可以在公共信息平台上解决，如此样例还有很多。政府达到这样一个公共信息化平台的信息化治理高度，这种治理模式实际上在一些互联网平台上已经完全或部分成为了现实，只是治理方是互联网公司而非政府而已。 评估自己的防护壁垒与环境允许的转型窗口期 虽然互联网厂商在向各行各业快速渗透，但出于各行业独有的技术壁垒、监管资质壁垒、资源壁垒、垄断市场壁垒等限制，互联网厂商无法一下子在短时间内通吃一切，气愤之余，也因此给这些壁垒冠以“保护既得利益，阻碍改革深入发展”的帽子。无论如何，这些壁垒给各行业的传统企业一个难得的转型窗口期。不同行业的转型窗口期不尽相同，这跟互联网厂商的基础能力相关。当前阶段互联网公司的基础能力聚集在个人消费者与小企业及个人创业者层面。那么以个人和小企业为目标客户的传统企业，转型窗口期就更短。以大型企业或政府机构为目标客户的传统企业转型窗口期则相对较长。因为经济的运转最终要靠个人，所以任何传统企业都不会有太长的转型窗口期。按照IT更新换代的发展周期预计，短的窗口期也就3～5年，长的窗口期也难以超过10年。 企业云计算的发展趋势 里程碑： 阶段 描述 面向数据中心管理员的IT基础设施资源虚拟化阶段 该阶段的关键特征体现为通过计算虚拟化技术的引入，将企业IT应用与底层的基础设施彻底分离解耦，将多个企业IT应用实例及运行环境（客户机操作系统）复用在相同的物理服务器上，并通过虚拟化集群调度软件，将更多的IT应用复用在更少的服务器节点上，从而实现资源利用效率的提升。 面向基础设施云租户和云用户的资源服务化与管理自动化阶段 该阶段的关键特征体现为通过管理平面的基础设施标准化服务与资源调度自动化软件的引入，以及数据平面的软件定义存储和软件定义网络技术，面向内部和外部的租户，将原本需要通过数据中心管理员人工干预的基础设施资源复杂低效的申请、释放与配置过程，转变为在必要的限定条件下（比如资源配额、权限审批等）的一键式全自动化资源发放服务过程。面向云租户的基础设施资源服务供给，可以是虚拟机形式，可以是容器（轻量化虚拟机），也可以是物理机形式。该阶段的企业IT云化演进，暂时还不涉及基础设施层之上的企业IT应用与中间件、数据库软件架构的变化 面向企业IT应用开发者及管理维护者的企业应用架构的分布式微服务化和企业数据架构的互联网化重构及大数据智能化阶段 该阶段的关键特征体现为：企业IT自身的应用架构逐步从纵向扩展应用分层架构体系，走向（依托开源增强的、跨不同业务应用领域高度共享的）数据库、中间件平台服务层以及（功能更加轻量化解耦、数据与应用逻辑彻底分离的）分布式无状态化架构，从而使得企业IT在支撑企业业务敏捷化、智能化以及资源利用效率提升方面迈上一个新的高度和台阶，并为企业创新业务的快速迭代开发铺平了道路。 云计算各阶段间的主要差异： 从IT非关键应用走向电信网络应用和企业关键应用 在云计算的计算虚拟化技术发展初期阶段，Guest OS与Host OS之间的前后端I/O队列在I/O吞吐上的开销较大，而传统的结构化数据由于对I/O性能吞吐和时延要求很高，这两个原因导致很多事务关键型结构化数据在云化的初期阶段并未被纳入虚拟化改造的范畴，从而使得相关结构化数据的基础设施仍处于虚拟化乃至云计算资源池的管理范围之外。然而随着虚拟化XEN/KVM引擎在I/O性能上的不断优化提升（如采用SR-IOV直通、多队列优化技术），使得处于企业核心应用的ERP等关系型关键数据库迁移到虚拟化平台上实现部署和运行已不是问题 从计算虚拟化走向存储虚拟化和网络虚拟化 对于存储来说，由于最基本的硬盘（SATA/SAS）容量有限，而客户、租户对数据容量的需求越来越大，因此必须考虑对数据中心内跨越多个松耦合的分布式服务器单元内的存储资源（服务器内的存储资源、外置SAN/NAS在内的存储资源）进行“小聚大”的整合，组成存储资源池。这个存储资源池，可能是某一厂家提供的存储软硬件组成的同构资源池，也可以是被存储虚拟化层整合成为跨多厂家异构存储的统一资源池。各种存储资源池均能以统一的块存储、对象存储或者文件的数据面格式进行访问。 在多租户虚拟化的环境下，不同租户对于边缘的路由及网关设备的配置管理需求也存在极大的差异化，而物理路由器和防火墙自身的多实例能力也无法满足云环境下租户数量的要求，采用与租户数量等量的路由器与防火墙物理设备，成本上又无法被多数客户所接受。于是人们思考是否可能将网络自身的功能从专用封闭平台迁移到服务器通用x86平台上来。这样至少网络端节点的实例就可以由云操作系统来直接自动化地创建和销毁，并通过一次性建立起来的物理网络连接矩阵，进行任意两个网络端节点之间的虚拟通讯链路建立，以及必要的安全隔离保障，从而里程碑式地实现了业务驱动的网络自动化管理配置，大幅度降低数据中心网络管理的复杂度。 资源池从小规模的资源虚拟化整合走向更大规模的资源池构建，应用范围从企业内部走向多租户的基础设施服务乃至端到端IT服务 通过虚拟化整合之后的资源池的服务对象，不能再仅仅局限于数据中心管理员本身，而是需要扩展到每个云租户。因此云平台必须在基础设施资源运维监控管理Portal的基础上，进一步面向每个内部或者外部的云租户提供按需定制基础设施资源，订购与日常维护管理的Portal或者API界面，并将虚拟化或者物理的基础设施资源的增、删、改、查等权限按照分权分域的原则赋予每个云租户，每个云租户仅被授权访问其自己申请创建的计算、存储以及与相应资源附着绑定的OS和应用软件资源，最终使得这些云租户可以在无须购买任何硬件IT设备的前提下，实现按需快速资源获取，以及高度自动化部署的IT业务敏捷能力的支撑，从而将云资源池的规模经济效益，以及弹性按需的快速资源服务的价值充分发掘出来。 数据规模从小规模走向海量，数据形态从传统结构化走向非结构化和半结构化 对非结构化、半结构化大数据的处理而产生的数据计算和存储量的规模需求，已远远超出传统的Scale-Up硬件系统可以处理的，因此要求必须充分利用云计算提供的Scale-Out架构特征，按需获得大规模资源池来应对大数据的高效高容量分析处理的需求。企业内日常事务交易过程中积累的大数据或者从关联客户社交网络以及网站服务中抓取的大数据，其加工处理往往并不需要实时处理，也不需要系统处于持续化的工作态，因此共享的海量存储平台，以及批量并行计算资源的动态申请与释放能力，将成为未来企业以最高效的方式支撑大数据资源需求的解决方案选择。 企业和消费者应用的人机交互计算模式，也逐步从本地固定计算走向云端计算、移动智能终端及浸入式体验瘦终端接入的模式 为应对云接入管道上不同业务类型对业务体验的不同诉求，业界通用的远程桌面接入协议在满足本地计算体验方面已越来越无法满足当前人机交互模式发展所带来的挑战，需要重点聚焦解决面向IP多媒体音视频的端到端QoS/QoE优化，并针对不同业务类别加以动态识别并区别处理，使其满足如下场景需求。 云资源服务从单一虚拟化，走向异构兼容虚拟化、轻量级容器化以及裸金属物理机服务器 基于共享Linux内核，对应用实例的运行环境以容器为单位进行隔离部署，并将其配置信息与运行环境一同打包封装，并通过容器集群调度技术（如Kubernetes）实现高并发、分布式的多容器实例的快速秒级发放及大规模容动态编排和管理，从而将大规模软件部署与生命周期管理，以及软件DevOps敏捷快速迭代开发与上线效率提升到了一个新的高度。从长远趋势上来看，容器技术终将以其更为轻量化、敏捷化的优势取代虚拟化技术，但在短期内仍很难彻底解决跨租户的安全隔离和多容器共享主机超分配情况下的资源抢占保护问题，因此，容器仍将在可见的未来继续依赖跨虚拟机和物理机的隔离机制来实现不同租户之间的运行环境隔离与服务质量保障。 云平台和云管理软件从闭源、封闭走向开源、开放 随着XEN/KVM虚拟化开源，以及OpenStack、CloudStack、Eucalyptus等云操作系统OS开源软件系统的崛起和快速进步，开源力量迅速发展壮大起来，迎头赶上并逐步成长为可以左右行业发展格局的重要决定性力量。 云计算的架构内涵与关键技术 云计算总体架构 企业数据中心IT架构正在面临一场前所未有的，以“基础设施软件定义与管理自动化”、“数据智能化与价值转换”以及“应用架构开源化及分布式无状态化”为特征的转化。 基础设施资源层融合 面向企业IT基础设施运维者的数据中心计算、存储、网络资源层，不再体现为彼此独立和割裂的服务器、网络、存储设备，以及小规模的虚拟化资源池，而是通过引入云操作系统，在数据中心将多个虚拟化集群资源池统一整合为规格更大的逻辑资源池，甚至进一步将地理上分散、但相互间通过MPLS/VPN专线或公网连接的多个数据中心以及多个异构云中的基础设施资源整合为统一的逻辑资源池，并对外抽象为标准化、面向外部租户（公有云）和内部租户（私有云）的基础设施服务，租户仅需制定其在软件定义的API参数中所需资源的数量、SLA/QoS及安全隔离需求，即可从底层基础设施服务中以全自动模式弹性、按需、敏捷地获取到上层应用所需的资源配备。 数据层融合 面向企业日常业务经营管理者的数据信息资产层，不再体现为散落在各个企业、消费者IT应用中，如多个看似关联不大的结构化事务处理记录（关系型数据库）数据孤岛，非结构化的文档、媒体以及日志数据信息片段，而是通过引入大数据引擎，将这些结构化与非结构化的信息进行统一汇总，汇聚存储和处理，基于多维度的挖掘分析与深度学习，从中迭代训练出对业务发展优化及客户满意度提升有关键价值的信息，从而将经营管理决策从纯粹依赖人员经验积累转变到更多依赖基于大数据信息内部蕴藏的智慧信息，来支撑更科学、更敏捷的商业决策。除大数据之外，数据层融合的另一个驱动力，来自于传统商业数据库在处理高并发在线处理及后分析处理扩展性方面所遭遇的不可逾越的架构与成本的瓶颈，从而驱动传统商业闭源数据库逐步被Scale Out架构的数据库分表分库及水平扩展的开源数据库所替代。 应用平台层融合 企业IT业务开发者和供应者的应用平台层开始积极探索基于云应用开发平台来实现跨应用领域基础公共开发平台与中间件能力去重整合，节省重复投入，同时通过在云开发平台中集成透明的开源中间件来替代封闭的商业中间件平台套件，特别通过引入面向云原生应用的容器化应用安装、监控、弹性伸缩及生命周期版本灰度升级管理的持续集成与部署流水线，来推动企业应用从面向高复杂度、厚重应用服务的瀑布式开发模式，逐步向基于分布式、轻量化微服务的敏捷迭代、持续集成的开发模式演进。 云计算架构上下文 云计算架构应用上下文的相关角色包括：云租户/服务消费者、云应用开发者、云服务运营者/提供者、云设备提供者 从上述云计算的基础上下文描述，我们不难看出云平台和云运营与运维管理系统是介于上层多租户的IT应用、传统数据中心管理软件，以及下层数据中心物理基础设施层之间的一层软件。其中云平台可进一步被分解为面向基础设施整合的云操作系统，面向数据整合的大数据引擎，以及面向应用中间件整合的应用开放平台。而云运营与运维管理系统在云计算引入的初期，与传统数据中心管理系统是并存关系，最终将逐步取代传统数据中心管理。 云平台的南向接口IF4向下屏蔽底层千差万别的物理基础设施层硬件的厂家差异性。针对应用层软件以及管理软件所提出的基础设施资源、数据处理以及应用中间件服务诉求，云平台系统向上层多租户的云应用与传统数据中心管理软件屏蔽如何提供资源调度、数据分析处理，以及中间件实现的细节，并在北向接口IF1、IF2和IF3为上层软件及特定租户提供归一化、标准化的基础设施服务（IaaS）、数据处理及应用平台服务（PaaS）API服务接口。在云平台面向云运营与管理者（拥有全局云资源操作权限）的IF3接口，除了面向租户的基础设施资源生命周期管理API之外，还包括一些面向物理、虚拟设施资源及云服务软件日常OAM运行健康状态监控的操作运维管理API接口。 其中IF1/IF2/IF3接口中关于云租户感知的云平台服务API的典型形态为Web RESTful接口。IF4接口则为业务应用执行平面的x86指令，以及基础设施硬件特有的、运行在物理主机特定类型OS中的管理Agent，或者基于SSL承载的OS命令行管理连接。IF3接口中的OAM API则往往采用传统IT和电信网管中被广泛采用的Web RESTful、SNMP、CORBA等接口。 云计算的典型技术参考架构 物理资源层 所有支撑IaaS层的IT基础设施硬件，其中包括服务器、存储（传统RAID架构垂直扩展的Scale Up存储和基于服务器的分布式水平扩展的Scale Out存储），以及数据中心交换机（柜顶、汇聚以及核心交换）、防火墙、VPN网关、路由器等网络安全设备。 虚拟资源层 （1）计算虚拟化 所有计算应用（含OS）并非直接承载在硬件平台上，而是在上层软件与裸机硬件之间插入了一层弹性计算资源管理及虚拟化软件：弹性计算资源管理软件对外负责提供弹性计算资源服务管理API，对内负责根据用户请求调度分配具体物理机资源；虚拟化软件（Hypervisor）对来自所有的x86指令进行截获，并在不为上层软件（含OS）所知的多道执行环境并行执行“仿真操作”，使得从每个上层软件实例的视角，仍然是在独占底层的CPU、内存以及I/O资源；而从虚拟化软件的视角，则是将裸机硬件在多个客户机（VM）之间进行时间和空间维度的穿插共享（时间片调度、页表划、I/O多队列模拟等）。 虚拟化环境下更高的内存访问效率：应用感知的大内存业务映射技术，通过该技术，可有效提升从虚拟机线性逻辑地址到最终物理地址的映射效率。 虚拟化环境下更高的CPU指令执行效率：通过对机器码指令执行的流程进行优化扫描，通过将相邻执行代码段中的“特权”指令所触发的“VM_Exit”虚拟化仿真操作进行基于等效操作的“合并”，从容达到在短时间内被频繁反复地执行。由于每次VM_Exit上下文进入和退出的过程都需要涉及系统运行队列调度以及运行环境的保存和恢复，即将多次上下文切换合并为一次切换，从而达到提升运行效率的目的。 虚拟化环境下更高的I/O和网络包收发处理效率：由于多个虚拟机在一个物理机内需要共享相同的物理网卡进行网络包收发处理，为有效减少中断处理带来的开销，在网络及I/O发包过程中，通过将小尺寸分组包合并为更大尺寸的分组包，可以减少网络收发接受端的中断次数，从而达到提升虚拟机之间网络吞吐率的目的。 更高的RAS可靠性保障：针对云计算所面临的电信领域网络及业务云化的场景，由于硬件故障被虚拟化层屏蔽了，使得物理硬件的故障无法像在传统物理机运行环境那样直接被传送通知给上层业务软件，从而导致上层业务层无法对故障做出秒级以内的及时响应，比如业务层的倒换控制，从而降低了对整体可靠性水平。如何感知上层的业务要求，快速进行故障检测和故障恢复，保证业务不中断，这给计算虚拟化带来了新的挑战。 单VM及多VM的弹性伸缩技术：单VM及多VM的弹性伸缩技术包括基本资源部件级别、虚拟机级别、云系统级别三个层次的伸缩技术。基本资源部件级别：精细化的Hypervisor资源调度，对指定虚拟机实例的CPU、内存及存储规格进行弹性伸缩，并可对伸缩上下限进行配额限制。虚拟机级别：指虚拟机集群的自动扩展与收缩，基于CloudWatch机制对集群资源忙闲程度的监控，对业务集群进行集群伸缩与扩展的Auto-Scaling控制。云系统级别：在内部私有云资源不足的情况下，自动向外部公有云或其他私有云（计算及存储资源池）“租借”及“释放”资源。 （2）存储虚拟化 通过对所有来自应用软件层的存储数据面的I/O读写操作指令进行“截获”，建立从业务应用视角覆盖不同厂家、不同版本的异构硬件资源的统一的API接口，进行统一的信息建模，使得上层应用软件可以采用规范一致的**、与底层具体硬件内部实现细节解耦的方式访问底层存储资源**。 通过“存储虚拟化”层内对多个对等的分布式资源节点的聚合，实现该资源的“小聚大”。比如，将多个存储/硬盘整合成为一个容量可无限扩展的超大（EB级规模）的共享存储资源池。由此可以看到，存储虚拟化相对计算虚拟化最大的差别在于：其主要定位是进行资源的“小聚大”，而非“大分小”。原因在于，存储资源的 “大分小”在单机存储以及SAN/NAS独立存储系统，乃至文件系统中通过LUN划分及卷配置已经天然实现了，然而随着企业IT与业务数据的爆炸式增长，需要实现高度扁平化、归一化和连续空间，跨越多个厂家服务器及存储设备的数据中心级统一存储，即“小聚大”。存储“小聚大”的整合正在日益凸显出其不可替代的关键价值。 高性能分布式存储引擎：伴随着云计算系统支撑的IT系统越来越大，覆盖范围从不同服务器存储节点，到分布在不同地理区域的数据中心，这就需要有一个分布式存储引擎。这个引擎，能满足高带宽、高I/O等各种场景要求，能很好地进行带宽的扩展。 存储异构能力：如何利旧，将不同厂家原有的独立SAN、NAS设备组合成一个大的存储资源池，也是软件定义存储中需要解决的问题 存储卸载：传统的企业存储系统，在采用各种各样的存储软件，这些软件存储操作对存储I/O和CPU资源均有较大消耗，会影响到用户业务性能的发挥。因此，如何将存储操作标准化，然后将存储操作利用某些标准的硬件动作去代替，这就是存储卸载。 （3）网络虚拟化 Cloud OS管理的资源范畴扩展到了整个数据中心，甚至将跨越多个由广域网物理或者逻辑专线连接起来数据中心。在一个具备一定规模的数据中心内，甚至多个数据中心内，各计算、存储单元之间以完全点对点的方式进行松耦合的网络互联。云数据中心之上承载的业务种类众多，各业务类型对于不同计算单元（物理机、虚拟机）之间，计算单元与存储单元之间，乃至不同安全层次的计算单元与外部开放互联网网络和内部企业网络之间的安全隔离及防护机制要求动态实现不同云租户之间的安全隔离。云数据中心还要满足不同终端用户不同场景的业务组网要求以及他们的安全隔离要求。因此，云操作系统的复杂性将随着云租户及租户内物理机和虚拟机实例的数量增长呈现几何级数的增长，由业务应用驱动的数据中心网络虚拟化和自动化已变得势在必行和不可或缺。为了实现彻底与现有物理硬件网络解耦的网络虚拟化与自动化，唯一的途径与解决方案就是SDN（也即所谓软件定义的网络），即构建出一个与物理网络完全独立的叠加式逻辑网络，其主要部件以及相关技术包括以下几方面。 SDN控制器：这是软件定义网络的集中控制模块。负责云系统中网络资源的自动发现和池化、根据用户需求分配网络资源、控制云系统中网络资源的正常运行。 虚拟交换机：根据SDN控制器，创建出的虚拟交换机实例。可以对这个虚拟交换机进行组网的设计、参数的设置，一如对物理交换机的使用。 虚拟路由器：根据SDN控制器，创建出的虚拟路由器实例。可以对这个虚拟路由器进行组网的设计、参数的设置，一如对物理路由器的使用。 虚拟业务网关：根据用户业务的申请，由SDN控制器创建出虚拟业务网关实例，提供虚拟防火墙的功能。可以对这个虚拟业务网关进行组网的设计、参数的设置，一如对物理业务网关的使用。 虚拟网络建模：面对如此复杂多变的组网，如何保证网络的有效区分和管理，又能保证交换和路由的效率，一个有效的建模方法和评估模型是需要的。虚拟网络建模技术能提前预知一个虚拟网络的运行消耗、效率和安全性。虚拟网络建模可以做成一个独立功能库，在需要的时候启动，以减少对系统资源的占用。 资源服务与调度层 相对虚拟化层在业务执行面和数据面上“资源聚合与分割仿真”，该层次主要体现为管理平面上的“逻辑资源调度”。 由于多个厂家已经投入到云计算的研发和实施中，不可避免地有多种实现方式。而要实现云计算真正的产业化并被广泛使用，各厂家的云计算平台必须要能够互相交互，即进行接口标准化。接口标准化后，主流的虚拟化平台，例如Hyper-V、KVM、UVP、ESX等之间能够互相兼容。各个硬件厂家或者中间件厂家可以自由选择虚拟化内核。 在云计算新的发展阶段中，面向公有云、面对国际化公司的分布式云系统将是重点。这样引发对超大资源的分配和调度。在整个云计算的实现架构上，计算、存储、网络资源的分配和使用将走向专业化。这是因为一个云应用业务，根据性质的不同，它对计算、存储、网络资源的需求可能是不一样的。例如：呼叫中心业务偏向于计算资源使用，而对于网盘业务则偏向于存储资源使用。在这种情况下，为了更有效地利用资源，给业务层提供基本资源调用API是最好的选择，将计算、存储、网络资源都作为基本资源单位，提供统一的资源调用接口，让云业务开发者自己选择如何高效地使用这些资源。这些API包括以下几个方面。 弹性计算资源调用API：计算资源包括CPU和内存，云计算平台根据云运营商的要求，已经将CPU和内存虚拟化和池化。系统提供资源的动态申请、释放、故障检测、隔离和自动切换功能，做到业务不感知。CPU资源又可以分为纯计算型、图像处理型等不同类型。不管是CPU还是内存，都提供瘦分配功能，资源的自动伸缩保证在低业务量时减少资源的消耗，高业务量时开启所有物理资源，确认业务的高效运行。计算资源API还需要提供集群能力。 弹性存储资源调用API：存储资源API提供文件或者卷接口，除了提供常见的资源申请、释放、瘦分配等功能外，还涉及其他几个关键方面。 异构资源的池化：不同的厂家在将存储资源池化后，提供统一的API，一个厂家可以利用这些API，将不同厂家的存储资源池构成一个大的资源池，然后再封装出API供业务调用。 存储资源的分层分级存储：因业务性能要求的不同，分层存储是一个常用的技术，业务系统在申请存储资源的时候，可以选择是否使用这个特性。 内存存储资源的支持：未来的系统，内存一定会成为主存，所有的存储，除非一些特别重要的信息，基本上不再需要存入非易失性介质。而使用内存资源作为主存，可靠性是关键要求。在构造内存存储池的时候，可靠性必须贯彻始终，每个内存存储在其他地方有备份，或者确保内存存储有可靠的UPS保护。 弹性网络资源调用API：网络资源API的基本功能也包括资源的申请、释放、监控、故障隔离和恢复等，也需要考虑异构资源的统一化。 拉通公有云与私有云的混合云架构 我们也不难发现，很多大企业及政府机构在面临云计算的建设使用模式的选择时，不可避免地将安全性问题放在了一个非常重要的位置上，甚至是作为首要考量的因素。目前，仅有在自建数据中心及自己维护管理组织的掌控范围内私有云模式才能保障企业敏感涉密的关键信息资产。这一事实决定了私有云仍将是很多大企业建设云计算首要选择的模式，私有云仍将与公有云在未来相当长一段时间内并存发展。只有拉通公有云和私有云的混合云能够将线上的公有云弹性敏捷优势，与私有云的安全私密保障优势相结合，实现优势互补，才能成为企业的最佳选择。 然而，考虑到大企业、政府机构等的业务负载的多样性，需要向云端迁移的应用并不仅仅包含核心涉密的信息资产，也包括业务突发性强，资源消耗量大，并且具备资源使用完毕之后可以立即释放的特征，比如开发测试应用、大数据分析计算应用、电商渠道的分布式Web前端应用等，均属于此类应用负载。这些应用当然也更适合采用公有云的方式来承载。但对于同一企业租户来说，如果一部分应用负载部署在公有云端，另一部分应用负载部署在私有云端，则仅仅跨云的身份认证、鉴权、拉通的统一发放及API适配是不够的，更重要的是必须实现拉通公有云和私有云的安全可信网络，实现自动化建立网络连接。 为满足上述诉求，需要跨不同的公有云和私有云，构建一层统一的混合云编排调度及API开放层，实现跨不同异构云的统一信息模型，并通过适配层将不同异构私有云、公有云的云服务及API能力集，对齐到混合云的统一信息模型，并通过SDN与各公有云、私有云的网络控制功能相配合，最终完成跨异构云网络互联的自动化。当然这个统一编排调度引擎，以及API开放层的实现架构，存在不同的可选路径。 引入一个全新的编排调度层，逐一识别出跨不同异构云的公共服务能力，并以此公共能力及其信息建模为基础参照，进行到各公有云、私有云的计算，存储原生API能力的逐一适配。该路径下的跨云网络互联方案，需要混合云SDN与各公有云、私有云的VPN网络服务进行紧密协同配合，由于不同异构云之间的网络服务语义及兼容性相比计算和存储服务差别更大，因此也必然给跨云的VPN网络连接适配处理带来了更大的复杂度与挑战。 依托于业界开源事实标准的云服务与调度层（如OpenStack），作为拉通各异构公有云、私有云的信息模型及API能力的基准，通过社区力量推动各异构云主动提供与该事实标准兼容的适配驱动。该路径下的跨云网络互联，采用叠加在所有异构云虚拟化之上的Overlay虚拟网络机制，无须进行跨异构云的网络模型适配转换，即可面向租户实现按需的跨云网络互联，从而大大降低了跨云网络互联处理的难度，为混合云的广泛普及奠定了坚实的基础。 可靠性保障 数据中心内的可靠性保障技术 数据中心内的可靠性保障技术主要包括HA（High Availability）冷备份、FT（Fault Tolerance）热备份、轻量级FT。 HA（High Availability） 冷备份数据中心内基于共享存储的冷迁移，在由于软件或硬件原因引发主用VM/PM故障的情况下，触发应用在备用服务器上启动。其适用于不要求业务零中断或无状态应用的可靠性保障。 FT（Fault Tolerance） 热备份其指指令、内存、所有状态数据同步。该方式的优势是状态完全同步，完全保证一致性，且支持SMP。劣势是性能开销大，会带来40%左右的性能降低。 轻量级FT 其是基于I/O同步的FT热备机制。优势是CPU/网络性能损耗10%以内，支持单核和多核。劣势是适合于网络I/O为主服务的场景。 跨数据中心的可靠性保障技术 跨数据中心的可靠性保障技术，主要是基于存储虚拟化层I/O复制的同步和异步容灾两种。基于存储虚拟化层I/O复制的同步容灾，采用生产和容灾中心同城（＜100KM）部署，时延小于5ms,DC间带宽充裕，并且对RPO（恢复点目标）要求较高，一般RPO接近或者等于0秒。分布式块存储提供更高效的I/O同步复制效率。 基于存储虚拟化层I/O复制的异步容灾采用生产和容灾中心异地（大于100KM）部署，带宽受限，时延大于5ms，同时对RPO有一定的容忍度，如RPO大于5分钟。I/O复制及快照对性能的影响趋近于零。 云计算核心架构竞争力 云计算及大数据开源软件概览 项目 描述 OpenStack 云操作系统框架，基于这个框架，可以集成不同的各类组件，实现满足不同场景需要的云操作系统，并在此基础上，最终构建完整的云计算系统。在计算领域，可以插件化的形式接入KVM、Xen、vCenter、FusionCompute等不同的Hypervisor；在存储领域，可以以插件化的形式实现对不同厂商的存储设备，以及Ceph、FusionStorage、vSAN等不同的软件定义存储的管理；在网络领域，可以实现对不同的网络硬件设备，OVS、Liunx-bridge、HAProxy等开源网络组件，以及多种SDN控制器的接入。 Kubernetes /Mesos / Docker Mesos生态：核心组件包括Mesos容器集群资源管理调度以及不同的应用管理框架。典型的应用管理框架包括Marathon和Chronos，其中Marathon用来管理长期运行服务，如Web服务；Chronos用来管理批量任务。Mesos生态主要由Mesosphere、Twitter等公司主力推动。 Kubernetes生态：Google公司发起的社区项目，涵盖容器集群资源管理调度，以及不同类型应用的应用管理组件。例如副本可靠性管理，服务发现和负载均衡，灰度升级，配置管理等组件。 Docker生态：Docker公司希望向容器生态系统上层发展，推出了Swarm容器资源管理调度组件，以及Compose应用编排组件。 Hadoop/Spark Hadoop来自Apache社区，是一个可水平扩展、高可用、容错的海量数据分布式处理框架，提供了简单分布式编程模型map-reduce。Hadoop设计的假设是底层硬件不可靠，由Hadoop检测和处理底层硬件失效 本文对于云计算的架构设计做了初步总结，后面会细化各个技术和应用场景，分开描述。","categories":[{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://blackforest1990.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"Tcp&ip","slug":"Tcp-ip","date":"2024-01-16T04:52:04.000Z","updated":"2024-01-23T06:28:32.592Z","comments":true,"path":"2024/01/16/Tcp-ip/","link":"","permalink":"https://blackforest1990.github.io/2024/01/16/Tcp-ip/","excerpt":"","text":"本文基于一个TCP&amp;IP项目进行学习，主要目的是来探索网络编程的奇妙。如下为本次学习的一些网络库和知识库。 代码参考：https://github.com/saminiir/level-ip 基础知识参考：https://beej-zhcn.netdpi.net/ TCP&amp;IP 协议层 网络接口层（Network Interface Layer）： 对应于OSI模型的物理层和数据链路层。负责将数据帧封装成比特流，并处理与物理网络设备的交互。 网络层（Internet Layer）： 对应于OSI模型的网络层。主要功能是在网络中路由数据包，为数据包选择最佳路径。IP协议是在这一层工作的关键协议。 传输层（Transport Layer）： 对应于OSI模型的传输层。负责端到端的通信，提供可靠的数据传输。TCP（传输控制协议）和UDP（用户数据报协议）是在这一层工作的协议。 应用层（Application Layer）： 对应于OSI模型的会话层、表示层和应用层。包含了各种应用程序，如HTTP（超文本传输协议）、FTP（文件传输协议）、SMTP（简单邮件传输协议）等。 Ethernet &amp; ARP TAP设备 为了拦截来自 Linux 内核的低级网络流量，我们将使用 Linux TAP 设备。简而言之，网络用户空间应用程序通常使用 TUN/TAP 设备分别操作 L3/L2 流量。在Linux中，TAP代表（Tap Virtual Network Device）。TAP设备是一种虚拟网络设备，通常用于模拟网络连接或实现虚拟专用网络（VPN）等网络功能。由于我们想要从第 2 层开始构建网络堆栈，因此我们需要一个 TAP 设备。我们像这样实例化它： 123456789101112131415161718192021222324252627282930313233static int tun_alloc(char *dev)&#123; struct ifreq ifr; int fd, err; if( (fd = open(&quot;/dev/net/tap&quot;, O_RDWR)) &lt; 0 ) &#123; perror(&quot;Cannot open TUN/TAP dev\\n&quot; &quot;Make sure one exists with &quot; &quot;&#x27;$ mknod /dev/net/tap c 10 200&#x27;&quot;); exit(1); &#125; CLEAR(ifr); /* Flags: IFF_TUN - TUN device (no Ethernet headers) * IFF_TAP - TAP device * * IFF_NO_PI - Do not provide packet information */ ifr.ifr_flags = IFF_TAP | IFF_NO_PI; if( *dev ) &#123; strncpy(ifr.ifr_name, dev, IFNAMSIZ); &#125; if( (err = ioctl(fd, TUNSETIFF, (void *) &amp;ifr)) &lt; 0 )&#123; perror(&quot;ERR: Could not ioctl tun&quot;); close(fd); return err; &#125; strcpy(dev, ifr.ifr_name); return fd;&#125; 在这之后，返回的文件描述符fd可以用于读取和写入数据到虚拟设备的以太网缓冲区。IFF_NO_PI标志在这里非常关键，否则我们将得到不必要的数据包信息附加到以太网帧之前。 以太网帧格式 多种不同的以太网技术是连接局域网(LAN) 中计算机的支柱。 以太网标准：https://en.wikipedia.org/wiki/IEEE_802.3 接下来，我们将看一下以太网帧头。可以将其声明为 C 结构体，如下所示： 1234567struct eth_hdr &#123; uint8_t dmac[6]; uint8_t smac[6]; uint16_t ethertype; uint8_t payload[];&#125; __attribute__((packed)); dmac smac: 它们包含通信双方的 MAC 地址（分别是目标和源）。 重载字段ethertype是一个 2 个八位字节的字段，根据其值，指示有效负载的长度或类型。具体来说，如果该字段的值大于或等于1536，则该字段包含有效负载的类型（例如IPv4、ARP）。如果该值小于该值，则它包含有效负载的长度。 在类型字段之后，以太网帧可能有几个不同的标签。这些标签可用于描述帧的虚拟 LAN (VLAN) 或服务质量(QoS) 类型。以太网帧标签被排除在我们的实现之外，因此相应的字段也不会出现在我们的协议声明中。 payload包含指向以太网帧有效负载的指针。在我们的例子中，这将包含 ARP 或 IPv4 数据包。如果有效负载长度小于所需的最小48字节（不含标签），则将填充字节附加到有效负载的末尾以满足要求。 我们还包含if_ether.hLinux 标头来提供以太类型及其十六进制值之间的映射。 最后，以太网帧格式末尾还包括帧校验序列字段，该字段与循环冗余校验（CRC）一起用于检查帧的完整性。我们将在实现中省略对该字段的处理。 __attribute__((packed))：这是一个GCC特有的属性，用于告诉编译器以最小的字节对齐方式来布局结构体，确保没有额外的填充字节。这对于网络协议帧头等需要严格字节对齐的结构体来说是重要的。 以太网帧解析 解析和处理传入以太网帧的总体场景非常简单： 1234567if (tun_read(buf, BUFLEN) &lt; 0) &#123; print_error(&quot;ERR: Read from tun_fd: %s\\n&quot;, strerror(errno));&#125;struct eth_hdr *hdr = init_eth_hdr(buf);handle_frame(&amp;netdev, hdr); 地址解析协议 地址解析协议（ARP）用于将48位以太网地址（MAC地址）动态映射到协议地址（例如IPv4地址）。这里的关键是，通过 ARP，可以使用多种不同的 L3 协议：不仅是 IPv4，还有其他协议。通常的情况是知道 LAN 中某些服务的 IP 地址，但要建立实际通信，还需要知道硬件地址 (MAC)。因此，ARP用于广播和查询网络，要求IP地址的所有者报告其硬件地址。 ARP数据包格式比较简单： 123456789struct arp_hdr&#123; uint16_t hwtype; uint16_t protype; uint8_t hwsize; uint8_t prosize; uint16_t opcode; unsigned char data[];&#125; __attribute__((packed)); ARP 标头 ( arp_hdr) 包含 2 个八位字节hwtype，它确定所使用的链路层类型。在我们的例子中，这是以太网，实际值为0x0001。 2 个八位字节protype字段指示协议类型。在我们的例子中，这是 IPv4，通过值 进行通信0x0800。 hwsize（硬件地址长度）： 一个8位的整数，表示硬件地址的长度，以字节为单位 prosize（协议地址长度）：一个8位的整数，表示协议地址的长度，以字节为单位。 2 个八位字节字段opcode声明 ARP 消息的类型。它可以是 ARP 请求 (1)、ARP 应答 (2)、RARP 请求 (3) 或 RARP 应答 (4)。 该data字段包含 ARP 消息的实际负载，在我们的例子中，它将包含 IPv4 特定信息： 1234567struct arp_ipv4&#123; unsigned char smac[6]; uint32_t sip; unsigned char dmac[6]; uint32_t dip;&#125; __attribute__((packed)); 这些字段非常不言自明。smac dmac分别包含发送方和接收方的 6 字节 MAC 地址。sip和dip分别包含发送者和接收者的 IP 地址。 地址解析算法 检查硬件类型（ar$hrd）： 如果硬件类型存在（几乎肯定存在），继续。 可选地检查硬件长度（ar$hln）： 检查协议类型（ar$pro）： 如果协议类型存在，继续。 可选地检查协议长度（ar$pln）： 初始化Merge_flag为false： 检查转换表中是否存在 &lt;协议类型，发送者协议地址&gt; 这一对： 如果已存在： 使用数据包中的新信息更新该条目的发送者硬件地址字段。 将 Merge_flag 设置为 true。 检查当前设备是否是目标协议地址： 如果是： 如果 Merge_flag 为false，则将 &lt;协议类型，发送者协议地址，发送者硬件地址&gt; 添加到转换表中。 检查操作码是否是ARP请求 (ares_op$REQUEST)： 如果是： 交换硬件和协议字段，将本地硬件和协议地址放入发送者字段。 将 ar$op 字段设置为 ares_op$REPLY。 将数据包发送到在同一硬件上收到请求时的新目标硬件地址。 这个算法描述了在接收到ARP请求时的处理流程，包括更新转换表、响应ARP请求和发送ARP回复。 translation table用于存储 ARP 结果，以便主机只需查找其缓存中是否已有该条目即可。这可以避免向网络发送冗余 ARP 请求的垃圾邮件。 IPv4 and ICMPv4 IPv4 我们的实现中的下一层 (L3)（在以太网帧之后）处理将数据传送到目的地的情况。也就是说，互联网协议(IP) 的发明是为了为 TCP 和 UDP 等传输协议提供基础。它是无连接的，这意味着与 TCP 不同，所有数据报在网络堆栈中都是相互独立处理的。这也意味着 IP 数据报可能会无序到达。 此外，IP 并不能保证成功传输。这是协议设计者有意识的选择，因为 IP 旨在为同样不保证传输的协议提供基础。UDP 就是这样一种协议。如果通信双方之间需要可靠性，则可以在 IP 之上使用 TCP 等协议。在这种情况下，更高级别的协议负责检测丢失的数据并确保所有数据均已传输。 Header IPv4 标头的长度通常为 20 个八位位组。标头可以包含尾随选项，但我们的实现中省略了它们。字段的含义相对简单，可以用 C 结构体来描述： 123456789101112131415struct iphdr &#123; uint8_t ihl : 4; /* TODO: Support Big Endian hosts */ uint8_t version : 4; uint8_t tos; uint16_t len; uint16_t id; uint16_t frag_offset; uint8_t ttl; uint8_t proto; uint16_t csum; uint32_t saddr; uint32_t daddr; uint8_t [];&#125; __attribute__((packed)); 4 位字段version指示 Internet 标头的格式。在我们的例子中，IPv4 的值为 4。 互联网报头长度字段ihl的长度同样为 4 位，由于该字段的大小为 4 位，因此它最多只能容纳 15。因此，IP 标头的最大长度为 60 个八位字节（15 乘以 32 除以 8）。 服务类型字段源自tos为IP 数据报的服务质量。 总长度字段len 传达整个 IP数据报的长度。由于它是 16 位字段，因此最大长度为 65535 字节。大型 IP 数据报会被分割成更小的数据报，以满足不同通信接口的最大传输单元(MTU)。 id字段用于索引数据报，最终用于重组分片的IP数据报。该字段的值只是一个由发送方递增的计数器。反过来，接收方知道如何对传入的片段进行排序。 flags字段定义了数据报的各种控制标志。具体来说，发送方可以指定数据报是否允许分片，是否是最后一个分片，或者是否还有更多分片传入。 片段偏移字段frag_offset指示片段在数据报中的位置。当然，第一个数据报的索引设置为 0。 ttl是一个常见属性，数据报的生命周期。它通常由原始发送方设置为 64，每个接收方都会将该计数器减 1。当它达到零时，数据报将被丢弃，并且可能会回复一条 ICMP 消息以指示错误。 该proto字段为数据报提供了在其有效负载中携带其他协议的固有能力。该字段通常包含 16 (UDP) 或 6 (TCP) 等值，仅用于向接收方传达实际数据的类型。 报头校验和字段csum，用于验证 IP 报头的完整性。 最后，saddr和daddr字段分别指示数据报的源地址和目标地址。 Internet Checksum 互联网校验和字段用于检查 IP 数据报的完整性。算法的实际代码如下： 12345678910111213141516171819202122232425262728293031323334uint32_t sum_every_16bits(void *addr, int count)&#123; register uint32_t sum = 0; uint16_t * ptr = addr; while( count &gt; 1 ) &#123; /* This is the inner loop */ sum += * ptr++; count -= 2; &#125; /* Add left-over byte, if any */ if( count &gt; 0 ) sum += * (uint8_t *) ptr; return sum;&#125;uint16_t checksum(void *addr, int count, int start_sum)&#123; /* Compute Internet Checksum for &quot;count&quot; bytes * beginning at location &quot;addr&quot;. * Taken from https://tools.ietf.org/html/rfc1071 */ uint32_t sum = start_sum; sum += sum_every_16bits(addr, count); /* Fold 32-bit sum to 16 bits */ while (sum&gt;&gt;16) sum = (sum &amp; 0xffff) + (sum &gt;&gt; 16); return ~sum;&#125; sum_every_16bits 函数： 参数 void *addr 是指向内存地址的指针，表示数据的起始地址。 参数 int count 是要计算校验和的数据字节数。 函数使用指针 ptr 迭代访问数据，每次累加两个字节的值到 sum 中，直到处理完所有的字节。 如果数据字节数是奇数，最后一个字节单独累加。 返回一个 32 位整数，表示所有 16 位字的和。 checksum 函数： 参数 void *addr 是数据的起始地址。 参数 int count 是要计算校验和的数据字节数。 参数 int start_sum 是初始的校验和值。 函数调用 sum_every_16bits 函数计算数据的 16 位字的和，然后将其加到初始校验和值 start_sum 中。 然后，使用一个循环将 32 位的和折叠为 16 位，直到和不再超过 16 位。 最后，返回计算得到的校验和的一位补码。 这些函数的目的是按照 RFC1071 中描述的方式计算 Internet 校验和。计算校验和的过程涉及将所有 16 位字相加，然后将 32 位和折叠为 16 位。最终，返回的值是校验和的一位补码。 Internet Control Message Protocol version 4 由于互联网协议缺乏可靠性机制，因此需要某种方式来通知通信方可能的错误情况。因此，互联网控制消息协议(ICMP) 用于网络中的诊断措施。 Header ICMP 标头驻留在相应 IP 数据包的有效负载中。ICMPv4报头的结构如下： 123456struct icmp_v4 &#123; uint8_t type; uint8_t code; uint16_t csum; uint8_t data[];&#125; __attribute__((packed)); type字段传达消息的目的。类型字段保留了42个不同的值，但常用的只有大约8个。在我们的实现中，使用类型 0（Echo Reply）、3（Destination Unreachable）和 8（Echo request）。 code字段进一步描述了消息的含义。例如，当类型为 3（目的地不可达）时，代码字段暗示原因。一个常见错误是当数据包无法路由到网络时：始发主机很可能会收到类型为 3 且代码为 0（网络不可达）的 ICMP 消息。 csum字段与IPv4报头中的校验和字段相同，并且可以使用相同的算法来计算它。在 ICMPv4 中，校验和是端到端的，这意味着计算校验和时还包括有效负载。 消息及其处理 实际的 ICMP 负载由查询/信息消息和错误消息组成。首先，我们来看看回显请求/回复消息，在网络中通常称为“ping”： 12345struct icmp_v4_echo &#123; uint16_t id; uint16_t seq; uint8_t data[];&#125; __attribute__((packed)); 消息格式紧凑。该字段id由发送主机设置，以确定回显应答要发送给哪个进程。例如，可以在此字段中设置进程 ID。 该字段seq是回显的序列号，它只是一个从零开始的数字，每当形成新的回显请求时就加一。这用于检测回显消息在传输过程中是否消失或重新排序。 该data字段是可选的，但通常包含回显时间戳等信息。然后可以使用它来估计主机之间的往返时间。 ICMPv4 错误消息Destination Unreachable具有以下格式： 123456struct icmp_v4_dst_unreachable &#123; uint8_t unused; uint8_t len; uint16_t var; uint8_t data[];&#125; __attribute__((packed)); 首先，第一个八位字节未使用。然后，长度字段（len field）表示原始数据报的长度，以IPv4的4字节单位计算。2字节字段var的值取决于ICMP代码。最后，导致目标不可达状态的原始IP数据包的尽可能多的部分被放置到数据字段中。 TCP 基础知识和握手 可靠性机制 可靠发送数据的问题看似表面，但涉及到其实际的实现。主要是，在数据报式网络中的错误修复方面出现了几个问题： 发送方应等待接收方确认多长时间？ 如果接收方处理数据的速度无法赶上发送数据的速度怎么办？ 如果中间的网络（例如路由器）无法像发送数据一样快速处理数据怎么办？ 在所有情况下，数据包交换网络的潜在危险都存在——来自接收方的确认可能在传输过程中被损坏甚至丢失，这使发送方陷入困境。 为了解决这些问题，可以使用多种机制。也许最常见的是滑动窗口技术，双方都对传输的数据进行记录。窗口数据被认为是连续的（就像数组的切片），并且当双方处理（并确认）数据时，窗口向前“滑动”： 123456789101112Left window edge Right window edge | | | | --------------------------------------------------------- ...| 3 | 4 | 5 | 6 | 7 |... --------------------------------------------------------- ^ ^ ^ ^ | \\ / | | \\ / | Sent and Window size: 3 Cannot be ACKed sent yet 使用这种滑动窗口的便利特性是它还减轻了流量控制的问题。当接收方处理数据的速度不能达到发送数据的速度时，就需要进行流量控制。在这种情况下，滑动窗口的大小将协商得较小，从而导致发送方的输出受到限制。 另一方面，拥塞控制有助于发送方和接收方之间的网络堆栈不发生拥塞。有两种通用方法：在显式版本中，协议有一个字段用于专门通知发送方有关拥塞状态的信息。在隐式版本中，发送方尝试猜测网络何时拥塞并应限制其输出。总体而言，拥塞控制是一个复杂的、反复出现的网络问题。 TCP 基础知识 TCP 中的底层机制比 UDP 和 IP 等其他协议涉及更多。TCP是面向连接的协议，这意味着首先在正好两端之间建立单播通信通道。这个连接是由双方主动处理的：建立连接（握手），告知对方数据的状态和可能出现的问题。 TCP 的另一个重要属性是它是一种流协议。与 UDP 不同，TCP 不保证应用程序在发送和接收数据时保持稳定的数据“块”。相反，TCP 实现必须缓冲数据，当数据包丢失、重新排序或损坏时，TCP 必须等待并组织缓冲区中的数据。只有当数据被认为是完整的时，TCP才可以将数据交给应用程序的套接字。 由于 TCP 将数据作为流进行操作，因此流中的“块”必须转换为 IP 可以承载的数据包。这称为打包，其中 TCP 标头包含流中当前索引的序列号。这还有一个方便的特性，即流可以分成许多可变大小的段，然后 TCP 知道如何重新打包它们。 与 IP 类似，TCP 也检查消息的完整性。这是通过与 IP 中相同的校验和算法实现的，但增加了细节。主要是，校验和是端到端的，这意味着标头和数据都包含在校验和中。此外，还包括根据 IP 标头构建的伪标头。 如果 TCP 实现接收到损坏的段，它会丢弃它们并且不会通知发送方。这个错误可以通过发送方设置的定时器来纠正，如果接收方从未确认过该报文段，则可以使用该定时器来重新传输该报文段。 TCP 也是一个全双工系统，这意味着流量可以同时在两个方向上流动。这意味着通信双方必须在内存中保持双向数据的排序。更深入地说，TCP 通过在其发送的段中包含对相反流量的确认来保留其流量足迹。 从本质上讲，数据流的排序是 TCP 的主要原理。然而，保持同步的问题并不是一个简单的问题。 TCP Header 接下来，我们将定义消息标头并描述其字段。TCP头看似简单，但是包含了很多关于通信状态的信息。 TCP 标头为 20 个八位位组，大小为： 123456789101112 0 15 31-----------------------------------------------------------------| source port | destination port |-----------------------------------------------------------------| sequence number |-----------------------------------------------------------------| acknowledgment number |-----------------------------------------------------------------| HL | rsvd |C|E|U|A|P|R|S|F| window size |-----------------------------------------------------------------| TCP checksum | urgent pointer |----------------------------------------------------------------- 源端口和目标端口字段用于建立来自和到主机的多个连接。也就是说，Berkeley 套接字是应用程序绑定到 TCP 网络堆栈的流行接口。通过端口，网络堆栈知道将流量定向到哪里。由于字段大小为 16 位，因此端口值范围为 0 到 65535。 由于流中的每个字节都有编号，因此序列号代表 TCP 段的窗口索引。握手时，它包含初始序列号(ISN)。 确认号包含发送方期望接收的下一个字节的窗口索引。握手后，必须始终填充 ACK 字段。 报头长度(HL) 字段以 32 位字表示报头的长度。 接下来，展示了几个标志。前 4 位 ( rsvd ) 未使用。 拥塞窗口降低©用于通知发送方降低了其发送速率。 ECN Echo (E) 通知发送方收到拥塞通知。 Urgent Pointer（U）指示该段包含优先级数据。 ACK (A) 字段用于传达 TCP 握手的状态。它在连接的剩余时间内保持开启状态。 PSH（P）用于指示接收方应尽快将数据“推送”到应用程序。 RST ® 重置 TCP 连接。 SYN (S)用于在初始握手时同步序列号。 FIN (F)表示发送方已完成数据发送。 window size字段用于通告窗口大小。换句话说，这是接收方愿意接受的字节数。由于它是 16 位字段，因此最大窗口大小为 65,535 字节。 TCP checksum字段用于验证 TCP 段的完整性。该算法与 Internet 协议相同，但输入段还包含 TCP 数据以及来自 IP 数据报的伪标头。 当设置 U 标志时，使用紧急指针。该指针指示紧急数据在流中的位置。 在可能的选项之后，接下来是实际数据。然而，该数据不是必需的。例如，握手仅通过 TCP 标头完成。 TCP握手 TCP连接通常经历以下阶段：连接建立（握手）、数据传输和连接关闭。下图描述了 TCP 通常的握手例程： 1234567891011TCP A TCP B 1. CLOSED LISTEN 2. SYN-SENT --&gt; &lt;SEQ=100&gt;&lt;CTL=SYN&gt; --&gt; SYN-RECEIVED 3. ESTABLISHED &lt;-- &lt;SEQ=300&gt;&lt;ACK=101&gt;&lt;CTL=SYN,ACK&gt; &lt;-- SYN-RECEIVED 4. ESTABLISHED --&gt; &lt;SEQ=101&gt;&lt;ACK=301&gt;&lt;CTL=ACK&gt; --&gt; ESTABLISHED 5. ESTABLISHED --&gt; &lt;SEQ=101&gt;&lt;ACK=301&gt;&lt;CTL=ACK&gt;&lt;DATA&gt; --&gt; ESTABLISHED 主机 A 的套接字处于关闭状态，这意味着它不接受连接。相反，主机 B 绑定到特定端口的套接字正在侦听新连接。 主机 A 打算发起与主机 B 的连接。因此，A 制作了一个 TCP 段，该段设置了 SYN 标志，并且序列字段填充了值 (100)。 主机 B 使用设置了 SYN 和 ACK 字段的 TCP 段进行响应，并通过向其加 1 来确认 A 的序列号 (ACK=101)。同样，B 生成序列号（300）。 3 次握手由连接请求发起者 (A) 的 ACK 完成。确认字段反映了主机接下来期望从另一端接收到的序列号。 数据开始流动，主要是双方都确认了对方的段号。 这是建立TCP连接的常见场景。然而，出现了几个问题： 初始序列号是如何选择的？ 如果双方同时请求彼此连接怎么办？ 如果路段延迟一段时间或无限期怎么办？ 初始序列号(ISN) 由通信双方在第一次联系时独立选择。由于它是识别连接的关键部分，因此必须选择它，使其最有可能是唯一的且不易被猜测。事实上，在TCP 序列号攻击中，攻击者可以复制 TCP 连接并有效地将数据传送给目标，冒充受信任的主机。 最初的规范建议 ISN 由每 4 微秒递增的计数器选择。然而，攻击者可以猜测到这一点。实际上，现代网络堆栈通过更复杂的方法生成 ISN。 两个端点收到对方的连接请求（SYN）的情况称为同时打开。这是通过 TCP 握手中的额外消息交换来解决的：双方都发送 ACK（不知道另一方也已完成），并且双方对请求进行 SYN-ACK。此后，数据传输开始。 最后，TCP 实现必须有一个计时器来知道何时放弃建立连接。尝试重新建立连接，通常采用指数退避，但一旦达到最大重试次数或时间阈值，连接将被视为不存在。 TCP 选项 TCP 标头段中的最后一个字段是为可能的 TCP 选项保留的。最初的规范提供了三个选项，但后来的规范添加了更多选项。接下来，我们将看看最常见的选项。 最大段大小(MSS) 选项告知 TCP 实现愿意接收的最大 TCP 段大小。IPv4 中的典型值为 1460 字节。 选择性确认(SACK) 选项优化了许多数据包在传输过程中丢失且接收器的数据窗口充满“漏洞”的情况。为了弥补由此造成的吞吐量下降，TCP 实现可以通过 SACK 通知发送方它未收到的特定数据包。因此，发送方以比累积确认方案更直接的方式接收有关数据状态的信息。 窗口缩放选项增加了有限的 16 位窗口大小。也就是说，如果双方在握手段中都包含此选项，则窗口大小将乘以此比例。拥有更大的窗口大小对于批量数据传输非常重要。 时间戳选项允许发送方将时间戳放入 TCP 分段中，然后可用于计算每个 ACK 分段的 RTT。然后可以使用该信息来计算 TCP 重传超时。 TCP Data Flow &amp; Socket API Transmission Control Block 通过定义记录数据流状态的变量来开始讨论 TCP 数据管理是有益的。 简而言之，TCP 必须跟踪它已发送和接收确认的数据序列。为了实现这一点，为每个打开的连接初始化一个称为传输控制块（TCB）的数据结构。 传出（发送）端的变量是： 123456789Send Sequence Variables SND.UNA - send unacknowledged SND.NXT - send next SND.WND - send window SND.UP - send urgent pointer SND.WL1 - segment sequence number used for last window update SND.WL2 - segment acknowledgment number used for last window update ISS - initial send sequence number 依次，为接收方记录以下数据： 123456Receive Sequence Variables RCV.NXT - receive next RCV.WND - receive window RCV.UP - receive urgent pointer IRS - initial receive sequence number 此外，当前正在处理的段的辅助变量定义如下： 12345678Current Segment Variables SEG.SEQ - segment sequence number SEG.ACK - segment acknowledgment number SEG.LEN - segment length SEG.WND - segment window SEG.UP - segment urgent pointer SEG.PRC - segment precedence value 这些变量共同构成了给定连接的大部分 TCP 控制逻辑。 TCP数据通讯 一旦建立连接，就会开始显式处理数据流。TCB 中的三个变量对于状态的基本跟踪非常重要： SND.NXT- 发件人将跟踪要在SND.NXT中使用的下一个序列号。 RCV.NXT- 接收方记录下一个期望的序列号RCV.NXT。 SND.UNA- 发送方将在SND.UNA 中记录最早的未确认序列号。 在足够的时间段内，当TCP管理数据通信且没有传输发生时，这三个变量将相等。 例如，当 A 决定向 B 发送带有数据的段时，以下事件发生： TCP A发送一个段并在其自己的记录（TCB）中推进SND.NXT。 TCB B接收该段并通过推进RCV.NXT进行确认，并发送一个ACK。 TCB A接收ACK并推进SND.UNA。 这些变量推进的数量是段中数据的长度。这是TCP在传输数据上的控制逻辑基础。 TCP 连接终止 关闭 TCP 连接同样是一个复杂的操作，可以强制终止（RST）或通过双方协议（FIN）完成。 基本场景如下： 主动关闭者发送FIN 报文段。 被动关闭者通过发送 ACK 段来确认这一点。 被动关闭器者开始自己的关闭操作（当它没有更多数据要发送时）并有效地成为主动关闭者。 一旦双方都向对方发送了 FIN 并且向两个方向都确认了它们，连接就会关闭。 显然，TCP 连接的关闭需要四个段，而 TCP 连接建立（三次握手）则需要三个段。 此外，TCP 是一种双向协议，因此可以让另一端宣布它没有更多数据要发送，但仍保持在线状态以接收传入数据。这称为TCP 半关闭。 数据包交换网络的不可靠特性给连接终止带来了额外的复杂性 - FIN 段可能会消失或永远不会被有意发送，从而使连接处于尴尬的状态。例如，在 Linux 中，内核参数tcp_fin_timeout控制 TCP 在强制关闭连接之前等待最终 FIN 数据包的秒数。这违反了规范，但却是预防拒绝服务 (DoS) 所必需的。 中止连接涉及设置了 RST 标志的段。发生重置的原因有很多，但常见的原因有： 对不存在的端口或接口的连接请求 另一个 TCP 已崩溃并最终处于不同步连接状态 尝试干扰现有连接 因此，TCP 数据传输的最佳路径永远不会涉及 RST 段。 套接字API 为了能够利用网络堆栈，必须为应用程序提供某种接口。BSD Socket API是最著名的一种，它起源于 1983 年的 4.2BSD UNIX 版本。Linux 中的 Socket API 与 BSD Socket API 兼容。 socket(2)通过调用并将套接字类型和协议作为参数传递，可以从网络堆栈中保留套接字。通用值为AF_INET和SOCK_STREAM。这将默认为 TCP-over-IPv4 套接字。 成功从网络堆栈保留 TCP 套接字后，它将连接到远程端点。这是connect(2)使用的地方，调用它将启动 TCP 握手。 从那时起，我们就可以从套接字write(2)和read(2)获取数据了。 网络堆栈将处理 TCP 流中数据的排队、重传、错误检查和重组。对于应用程序来说，TCP的内部行为大多是不透明的。应用程序唯一可以依赖的是 TCP 已确认发送和接收数据流的责任，并且它将通过套接字 API 通知应用程序意外行为。 socket()： 创建一个套接字，返回一个套接字描述符。 1int socket(int domain, int type, int protocol); bind()： 将套接字与特定地址（IP地址和端口号）绑定。 1int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); listen()： 启动服务端等待客户端连接请求的过程。 1int listen(int sockfd, int backlog); accept()： 接受客户端的连接请求，返回一个新的套接字用于与客户端通信。 1int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen); connect()： 发起与远程服务器的连接。 1int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); send() 和 recv()： 用于发送和接收数据。 12size_t send(int sockfd, const void *buf, size_t len, int flags);size_t recv(int sockfd, void *buf, size_t len, int flags); TCP 重传 自动重复请求 许多可靠协议的基础是自动重复请求 (ARQ)的概念。 在 ARQ 中，接收方发送其已接收数据的确认，而发送方则重新传输其从未收到确认的数据。 正如我们所讨论的，TCP 将传输数据的序列号保存在内存中并以确认进行响应。传输的数据被放入重传队列中，并且启动与数据相关的定时器。如果在定时器超时之前没有收到数据序列的确认，则会发生重传。 TCP重传 当TCP传输包含数据的报文段时，它会将副本放入重传队列中并启动计时器；当收到该数据的确认时，该段将从队列中删除。如果在定时器超时之前没有收到确认，则重新传输该段。 然而，原来的重传超时计算公式被认为不适用于不同的网络环境,最新的规范化规范可以从 RFC6298中找到。 基本算法相对简单。对于给定的 TCP 发送方，定义状态变量来计算超时： srtt是平滑的往返时间，用于平均分段的往返时间 (RTT) rttvar保存往返时间变化 rto最终保存重传超时，例如以毫秒为单位 简而言之，srtt 充当连续 RTT 的低通滤波器。由于 RTT 可能存在较大的变化，rttvar 用于检测这些变化并防止它们影响平均函数。此外，假设时钟的粒度为 G 秒。 如RFC6298中所述，计算步骤如下： 在第一次 RTT 测量之前： 1rto = 1000ms 在第一个 RTT 测量R上： 123srtt = Rrttvar = R/2rto = srtt + max(G, 4*rttvar) 关于后续测量： 12345alpha = 0.125beta = 0.25rttvar = (1 - beta) * rttvar + beta * abs(srtt - r)srtt = (1 - alpha) * srtt + alpha * rrto = srtt + max(g, 4*rttvar) 计算后rto，如果小于1秒，则四舍五入为1秒。可以提供最大数量，但必须至少为 60 秒 TCP 实现的时钟粒度传统上被估计为相当高，范围从 500 毫秒到 1 秒。然而，像 Linux 这样的现代系统使用的时钟粒度为 1 毫秒。 需要注意的一件事是， 建议 RTO 始终至少为 1 秒。这是为了防止虚假重传，即当某个数据段重传过快时，会导致网络拥塞。在实践中，许多实现选择亚秒级的四舍五入：Linux 使用 200 毫秒。 Karn’s Algorithm Karn 算法是一种防止 RTT 测量给出错误结果的强制算法。它只是指出不应为重传的数据包获取 RTT 样本。 换句话说，TCP 发送方会跟踪其发送的分段是否为重传，并跳过这些确认的 RTT 例程。这是有道理的，因为否则发送方无法区分原始段和重传段之间的确认。 然而，当使用时间戳 TCP 选项时，可以测量每个 ACK 段的 RTT。 管理 RTO 定时器 管理重传定时器相对简单。RFC6298推荐以下算法： 当发送数据段且 RTO 定时器未运行时，将其激活，超时值为rto 当所有未完成的数据段均被确认后，关闭 RTO 定时器 当收到新数据的 ACK 时，用以下值重新启动 RTO 定时器：rto 当 RTO 定时器到期时： 重传最早的未确认段 将 RTO 定时器退后 2 倍，即 ( rto = rto * 2) 启动RTO定时器 此外，当 RTO 值出现回退并且成功进行后续测量时，RTO 值可能会急剧缩小。在进行退避并等待确认时，TCP 实现可能会清除 srtt 和 rttvar。 请求重传 TCP 通常不仅仅依靠 TCP 发送方的计时器来修复丢失的数据包。接收方还可以通知发送方需要重传分段。 重复确认是一种对无序段进行确认的算法，但按最新有序段的序列号进行确认。在三个重复确认之后，TCP 发送方应该意识到它需要重新传输由重复确认通告的段。 此外，选择性确认（SACK）是重复确认的更复杂版本。它是一个 TCP 选项，接收器能够将接收到的序列编码到其确认中。然后发送者立即注意到任何丢失的数据段并重新发送它们。 代码走读 代码结构如下 先看下makefile文件，看看程序是怎么编译出来的？ 123456789101112131415161718192021222324252627282930313233343536CPPFLAGS = -I include -Wall -Werror -pthreadsrc = $(wildcard src/*.c)obj = $(patsubst src/%.c, build/%.o, $(src))headers = $(wildcard include/*.h)apps = apps/curl/curllvl-ip: $(obj) $(CC) $(CFLAGS) $(CPPFLAGS) $(obj) -o lvl-ip @echo @echo &quot;lvl-ip needs CAP_NET_ADMIN:&quot; sudo setcap cap_setpcap,cap_net_admin=ep lvl-ipbuild/%.o: src/%.c $&#123;headers&#125; $(CC) $(CFLAGS) $(CPPFLAGS) -c $&lt; -o $@debug: CFLAGS+= -DDEBUG_SOCKET -DDEBUG_TCP -g -fsanitize=threaddebug: lvl-ipapps: $(apps) $(MAKE) -C tools $(MAKE) -C apps/curl $(MAKE) -C apps/curl-pollall: lvl-ip appstest: debug apps @echo @echo &quot;Networking capabilites are required for test dependencies:&quot; which arping | sudo xargs setcap cap_net_raw=ep which tc | sudo xargs setcap cap_net_admin=ep @echo cd tests &amp;&amp; ./test-run-allclean: rm build/*.o lvl-ip CPPFLAGS 定义了一些编译选项，包括头文件目录、警告标志等。 src 使用wildcard函数列举了src目录下所有的.c文件。 obj 使用patsubst函数将src中的.c文件路径转换为build中的.o文件路径。 headers 列举了include目录下所有的头文件。 apps 定义了一个变量，包含了一个名为curl的应用程序。 lvl-ip 是默认目标规则，依赖于$(obj)。它编译了所有的.o文件并生成可执行文件lvl-ip，然后使用setcap命令赋予了特定的权限。 build/%.o 规则定义了如何将.c文件编译成.o文件。 debug 目标规则用于构建带有调试信息和线程检测的lvl-ip。 apps 目标规则用于构建额外的应用程序。 all 是一个依赖于lvl-ip和apps的目标规则。 test 目标规则用于运行测试，要求一些网络权限。 clean 规则用于清理生成的.o文件和可执行文件。 函数从main函数入口， 在链接阶段，链接器将编译生成的目标文件（例如，.o文件）合并成一个可执行文件。在这个过程中，链接器会解析main函数的地址，将其设置为程序的入口点。 123456789101112131415int main(int argc, char** argv)&#123; /*1. argc（Argument Count）： 这是一个整数，表示命令行参数的数量（argument count）。它表示在运行程序时通过命令行 输入的参数的个数。`argc` 至少为 1，因为程序的名称通常被认为是一个参数。 2. argv（Argument Vector）：这是一个字符指针数组，其中每个指针指向一个字符串，表示实际的命令行参数（argument ` vector）。`argv[0]` 是程序的名称，`argv[1]` 到 `argv[argc-1]` 是通过命令行输入的其他参数。每个参数都以字符串的形式表 示。*/ parse_cli(argc, argv);//该函数可能用于解析命令行参数 argc 和 argv，以便你的程序可以接受和处理命令行输入 init_signals();//该函数可能用于初始化信号处理程序 init_stack();//该函数可能用于初始化程序的堆栈 init_security();//该函数可能用于执行一些与程序安全性相关的初始化操作 run_threads();//启动线程 wait_for_threads();//该函数可能用于等待所有线程执行完毕或达到某种状态 free_stack();//该函数可能用于释放之前初始化的堆栈或相关资源&#125; 12345678static void init_stack()&#123; tun_init(); netdev_init(); route_init(); arp_init(); tcp_init();&#125; tun_init(): 这个函数用于初始化TUN（网络隧道）设备。TUN设备是一种虚拟网络设备，用于在用户空间和内核空间之间传递网络数据包。 netdev_init(): 这个函数用于初始化网络设备。网络设备是计算机网络中的硬件设备或虚拟设备，用于进行数据包的输入和输出。 route_init(): 这个函数用于初始化路由表。路由表用于确定数据包从源地址到目标地址的传输路径。 arp_init(): 这个函数用于初始化ARP（地址解析协议）。ARP协议用于将IP地址映射到物理硬件地址（如MAC地址）。 tcp_init(): 这个函数用于初始化TCP协议。TCP是一种可靠的、面向连接的协议，用于在计算机网络中进行可靠的数据传输。 1234567static void run_threads()&#123; create_thread(THREAD_CORE, netdev_rx_loop);// 创建并启动网络设备接收循环的线程 create_thread(THREAD_TIMERS, timers_start); // 创建并启动定时器的线程 create_thread(THREAD_IPC, start_ipc_listener); // 创建并启动 IPC 监听器的线程 create_thread(THREAD_SIGNAL, stop_stack_handler);// 创建并启动处理停止事件的线程&#125; 这段代码定义了一个名为 create_thread 的函数，用于创建线程并启动执行指定的函数。以下是对代码的主要解释： 1234567static void create_thread(pthread_t id, void *(*func) (void *))&#123; // 使用 pthread_create 函数创建线程 if (pthread_create(&amp;threads[id], NULL, func, NULL) != 0) &#123; // 如果创建线程失败，输出错误信息 print_err(&quot;Could not create core thread\\n&quot;); &#125; } pthread_create 函数： 这是 POSIX 线程库中用于创建线程的函数。它接受四个参数： &amp;threads[id]：指向 pthread_t 类型的变量，用于存储新创建线程的标识符。 NULL：线程的属性，这里设置为默认属性。 func：指向线程执行的函数的指针。 NULL：传递给线程执行函数的参数，这里设置为 NULL。 print_err 函数： 如果创建线程失败，调用 print_err 函数输出错误信息。 这个函数的作用是通过调用 pthread_create 创建线程，并将线程的标识符存储在全局数组 threads 的指定位置。如果创建线程失败，输出错误信息。这样的设计通常用于并发执行多个任务，提高程序的性能和响应性。","categories":[{"name":"编程","slug":"编程","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"how to make","slug":"how-to-make","permalink":"https://blackforest1990.github.io/tags/how-to-make/"}]},{"title":"万历十五年读书笔记","slug":"万历十五年读书笔记","date":"2024-01-09T09:42:32.000Z","updated":"2024-01-10T04:15:14.998Z","comments":true,"path":"2024/01/09/万历十五年读书笔记/","link":"","permalink":"https://blackforest1990.github.io/2024/01/09/%E4%B8%87%E5%8E%86%E5%8D%81%E4%BA%94%E5%B9%B4%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"","text":"​ 万历十五年，黄公仁宇之大作，英文名为“1587, A year of No Significance”, 在历史上1587实在为寡淡的一年，但是通过这一年万历皇帝，申时行，张居正，海瑞，戚继光等人的遭遇，阐述出明帝国在结构系统上的硬伤，无论如何的天纵奇才都不可能避免走向衰落。 ​ 明朝的衰败，实为制度的衰败，名为帝国，在本质上无非是数不清的农村合并成的一个集合体，礼仪和道德代替了法律，对于违法的行为作掩饰则被认为忠厚识大体。各个机构之间的联系，从来没有可资遵守的成文条例，所以造成行政效率低下，整个国家不能做到如臂指使的运转。当一个人口众多的国家，各人行动全凭儒家简单粗浅而又无法固定的原则所限制，而法律又缺乏创造性，则其社会发展的程度，必然受到限制。即便是宗旨善良，也不能补助技术之不及。 万历皇帝 ​ 万历皇帝朱翊（yì）钧（jūn）为国家的最高元首，本朝在开国之初曾经设立过丞相的职位，经过一个时期，内阁大学士在某种程度上就行使了丞相的职权。这种以阁臣代行相职的制度，来源于开国之君为了巩固政权而做出的苦心设计，目的是使皇权不被分割，也不致为旁人取代。大学士原来属于文学侍从之臣。由于殿试时文理出众，名列前茅，就可以进入翰林院。翰林几经升转，其中最突出的人物就可以被任命为大学士，供职于文渊阁。由于文渊阁是皇帝的文书机构，和皇帝最为接近，在不设丞相的情况下，这个机构的职权就由于处理政事的需要而越来越大，大学士一职也变成了皇帝的秘书而兼顾问，张居正名为首辅，或称元辅，其他大学士的任命则出于他的推荐。大学士之中有了主次之分，造成了今后朝臣之间的更加复杂的纠纷局面。内阁制度，破坏了皇帝-群臣-相的体系（皇帝负责天命和军事，宰相负责政务和官员任命），造成了文官体系的无比强大。万历皇帝在励精图治以后，发现了文官体系的不可战胜，从此不在做反抗，让帝国慢慢腐烂下去。 张居正 ​ 洪武皇帝两百年以前创建本朝，并确立了整套的政治和经济制度，其主要的着眼点在于保存一个农业社会的俭朴风气。当时全国的文官仅有八千人。所有办理文牍和事务的技术人员称之为“吏”，和文官属于两个不同的阶层，如泾渭之分明。官可以罚降为吏，吏却很少能上升为官。这些吏的薪给极为微薄，仅足以供一家糊。农村的组织方式是以每一乡村为单位，构成一个近于自治的集团，按照中央政府的规定订立自己的乡约。一村内设“申明亭”和“旌善亭”各一座，前者为村中耆老仲裁产业、婚姻、争斗等纠纷的场所，后者则用以表扬村民中为人所钦佩的善行。洪武皇帝所推行的农村政策及一整套的措施，对本朝今后的历史，影响至为深远。其最显著的后果是，在全国的广大农村中遏止了法制的成长发育，而以抽象的道德取代了法律。上自官僚下至村民，其判断是非的标准是“善”和“恶”，而不是“合法”或“非法”。在财政制度上，政府规定了按面积征收田赋，除浙西（当时的浙西包括今日的苏南）而外，其他地区的税率都比较低。征收不分贫富，其限制富户的办法即上述的服役。征收不分贫富，其限制富户的办法为服役。这种服役名目繁多，而且按累进税的原则分派，即家室愈是殷富，其负担也愈是繁重。这种以赢补亏而不由上级机关总揽收支以节约交通、通讯、簿记、仓库管理等各项后勤支出的财政制度贯彻于始终。全国满布着无数的短途运输线，缺乏统一的组织和管理。到后来税收已由实物折为现银。这种原始的方式也由于积重难返，而且中级机构又缺乏组织，而无法完全改变。显而易见，这种财政制度的弊病在于缺乏弹性，不能适应环境而调整。各府县的税率、税额长期凝固，即使耕地的收获量增加，其利益也为业主和高利贷者分润，于国库则无所裨益。 ​ 当张居正出任首辅的时候，本朝已经有了两百年的历史。开国时的理想和所提倡的风气与今天的实际距离已经愈来愈远了。很多问题，按理说应该运用组织上的原则予以解决，但事实上无法办到，只能代之以局部的人事调整。这种积弊的根源在于财政的安排。在开国之初，政府厘定各种制度，其依据的原则是“四书”上的教条，认为官员们应当过简单朴素的生活是万古不磨的真理。从这种观念出发而组成的文官集团，是一个庞大无比的组织，在中央控制下既没有重点，也没有弹性，更谈不上具有随着形势发展而作调整的能力。各种技术力量，诸如交通通讯、分析统计、调查研究、控制金融、发展生产等等则更为缺乏。一个必然的后果，即政府对民间的经济发展或衰退，往往感到隔膜，因之税收和预算不能随之而增加或减缩。财政上死板、混乱与缺乏控制，给予官员的俸禄又微薄到不合实际，官员们要求取得额外收入也就是不可避免的了。以张居正的精明干练，他没有能解决这个问题。他的十年首辅生涯，仅仅刚把问题看清楚。他的一套改革办法使文官们感受到极大的压力而不能成功，而且招致了死后的被清算。 申时行 ​ 申时行为皇帝的老师，张居正死后，申时行担任首辅，接替他的位置，他胸中富有积蓄，但是不近悬崖，不树异帜，在张居正死后，他承认张居正的过错，但并不借此夸大前任的过失，作为自己执政的资本。施政的要诀，仍不外以抽象的方针为主，以道德为一切事业的根基。朝廷最大的任务是促进文官之间的互相信赖与和谐。除非把全部文官罢免，而代之以不同的组织和不同的原则，身为首辅的人只能和文官合作，按照他们的共同意志办事。申时行没有忽略文官的双重性格：即虽称公仆，实系主人；有阳则有阴。他必须恰如其分地处理此中矛盾。时势要求申时行充当和事佬，他就担任这样角色。他不得不把目标降低。他所说的“使不肖者犹知忌惮，而贤者有所依归”，就表现了他调和这阴阳两极的方针。他无意于鼓励不法，但也不能对操守过于认真。他欣赏自己“从中调剂，就事匡维”这一处世和执政的原则，对待皇帝的办法则是“显谏者不若潜移为妙”，因为这种办法既对皇帝的权威无损，而臣下的目的又可以达到。 海瑞 ​ 和很多同僚不同，海瑞不能相信治国的根本大计是在上层悬挂一个抽象的、至美至善的道德标准，而责成下面的人在可能范围内照办，行不通就打折扣。而他的尊重法律，乃是按照规定的最高限度执行。 海瑞充分重视法律的作用并且执法不阿，但是作为一个在圣经贤传培养下成长的文官，他又始终重视伦理道德的指导作用： 凡讼之可疑者，与其屈兄，宁屈其弟；与其屈叔伯，宁屈其侄。与其屈贫民，宁屈富民；与其屈愚直，宁屈刁顽。事在争产业，与其屈小民，宁屈乡宦，以救弊也。事在争言貌，与其屈乡宦，宁屈小民，以存体也。 用这样的精神来执行法律，确实与“四书”的训示相符合。可是他出任文官并在公庭判案，上距“四书”的写作已经两千年，距本朝的开国也已近两百年。这一段有关司法的建议恰恰暴露了这个帝国在制度上长期存在的困难：以熟读诗书的文人治理农民，他们不可能改进这个司法制度，更谈不上保障人权。法律的解释和执行离不开传统的伦理，组织上也没有对付复杂的因素和多元关系的能力。 戚继光 ​ 武将领兵作战，和文官集团的施政原则在根本上是不能相容的。当社会和经济的发展不能平衡，冲突激化，以政治手段调剂无效，通常就会导致战争。有时候严重的天灾造成大面积的饥荒，百姓面对死亡的威胁，也会铤而走险，诉诸武力。但是我们帝国的文官，则一贯以保持各方面的平衡作为施政的前提，如果事情弄到动用武力，对他们来说就是失败的象征。他们具有一种牢不可破的观念，即上自国家，下至个人，不能把力量作为权威。如果一个地区有什么特殊的经济利益，那么就应当加以压抑，而不是提倡。至于天灾足以引起战争，则尤为无知妄说，因为从道德观念来说，天下的事物无不可以共同分配，灾民的暴动乃是小人犯上作乱的劣根性使然。 ​ 在维持军队给养的问题上，同样表现了帝国政府重文轻武的风气。让军人自己组织和管理后方勤务，根本不能考虑；即使是在文官管辖之下，把仓库的地点按照战略需要来作适当的配置，也被看作有背于平衡施政的原则。述种风气还使军人退伍以后不能得到正常的社会地位。本朝治理农民的根本方针是保持他们的淳朴无知，一个士兵退伍还乡，就等于增加一个无业游民，因为他在军队里所学到的技术和养成的起居习惯，已经难于再度适应农村的生活，事情的复杂性就会因之而增加。军官退伍以后所引起的问题更为严重。在别的国家里，一个退伍军官通常都受到应有的尊敬，如果担任民政职务，他的管理经验也能保证他胜任愉快。然而事情适得其反，我们的军官在长期训练中所培养的严格和精确，退伍以后竟毫无用武之地。他会发现在军队以外，人们所重视的是安详的仪表、华丽的文辞、口若悬河的辩才以及圆通无碍的机智。 ​ 也许是有鉴于唐朝藩镇的跋扈，本朝从洪武开始，就具有这重文轻武的趋向。大约经过了一百年，文官集团进入了成熟的阶段，他们的社会地位上升到历史上的最高点；换句话说，也就是武官的社会地位下降到历史上的最低点。这种畸形的出现，原因在于本朝的政治组织为一元化，一元化的思想基础则是两千年来的孔孟之道。如果让军队保持独立的、严格的组织，和文官集团分庭抗礼，这一元化的统治就不可能如所预期地成长、发展，以至于登峰造极。这种制度既经固定，将领们即使出生入死，屡建奇功，其社会影响，也未必抵得上一篇精彩的大块文章。这种制度和风气所造成的严重后果早已被事实所证明。 ​ 面对这样令人焦虑的局面，戚继光的任务决不仅止于单纯地击败倭寇。他首先要组织一支新型的军队，从他的军事著作《纪效新书》中可以看到如何有条不紊地实施他的建军方案：宣布招兵的办法，规定月饷的数字，拟订分配列兵职务的原则，明确官兵的职责，设计队、哨、局的组织，统一武器的规格，颁发旗帜金鼓这一类通讯器材，等等。他所制定的赏罚原则并不完全决定于战斗的胜负。即使大败，有功者仍然要给予奖赏；相反，即使大胜，作战不力和临阵脱逃者仍然要受到处罚。 ​ 在戚继光以前，在军队中受到重视的是个人的武艺，等到他们被有组织的倭寇屡屡击溃以后，当局者才觉悟到一次战斗的成败并非完全决定于个人武艺。戚继光对一个步兵班作了如下的配置：队长一名，火伕一名，战士十名。这这种配置由于左右对称而名为“鸳鸯阵”。右边持方形藤牌的士兵，其主要的任务在于保持既得的位置，稳定本队的阵脚。左边持圆形藤牌的士兵，则要匍匐前进，并在牌后掷出标枪，引诱敌兵离开有利的防御的位置。引诱如果成功，后面的两个士兵则以狼筅把敌人扫倒于地，然后让手持长枪的伙伴一跃而上把敌人刺死戳伤。最后两个手持镋钯的士兵则负责保护本队的后方，警戒侧翼，必要时还可以支援前面的伙伴，构成第二线的攻击力量。可以明显地看出，这一个十二人的步兵班乃是一个有机的集体。预定的战术取得成功，全靠各个士兵分工合作，很少有个人突出的机会。正由于如此，主将戚继光才不惮其烦地再三申明全队人员密切配合的重要性，并以一体赏罚来作纪律上的保证。 ​ 戚继光的方案比较现实。他没有去触动整个的国家体制，而只是脚踏实地，做他职责范围内力所能及的事。他的部队从来没有一个后勤司令，也没有一个固定的军需处和兵工署。在整个国家机构之中，也没有委派过向他的部队作后勤供应的专职人员。他部队中的装备和武器，来源于各府县的分散供应。这种情况自然不能保持武器的质量。戚继光的募兵原则是只收农民，而不收城市居民。他认为来自市井的人都属于狡猾无赖之徒。这种观点，虽然有它的片面性，但揆诸实际，在城市中有固定职业的人是极少自愿从军的。士兵为社会所普遍轻视，其军饷也相当微薄，城市中的应募者绝大多数只是把兵营当作解决食宿的救济所，一有机会就想另谋高就。戚继光的求实精神还表现于使革新不与传统距离过远，更不大事声张。他的部队保留了古老而朴素的农村作风，有时也和卫所内来自军户的部队并肩作战。他们日常的军饷，大体和在农村中充当短工的收入相等，但另设重赏以鼓励士气，一个敌军的头颅，赏额高达白银三十两。在作战中，总兵戚继光不惜初期接战的损失。经验告诉他，战斗无非是击破敌方的军事组织。如果以雷霆万钧之力，加于对方组织重点之上，则其配转运活的枢纽既被消灭，其全局必迅速瓦解。 ​ 在抗倭战争中功绩最为卓著的戚继光不是在理想上把事情做得至善至美的将领，而是最能适应环境以发挥他的天才的将领。他所以获得成功的要点，在于他清醒的现实感。他看清并适应了当时的政治，而把军事技术作为必要的辅助，这是在当时的环境里惟一可以被允许的方案。至于在一个以文人治国的农业国家之内，谁想要极端强调军事效率，提倡技术的发展，而导致军人和文官的并驾齐驱，哪怕他能举出无数动听的理由，在事实上也是绝对办不到的。 ​ 本书清晰透彻，明朝有如submarine sandwich，上面是一块长面包，大而无当，此乃文官集团；下面也是一块长面包，也没有有效的组织，此乃成千上万的农民。其中三个基本的组织原则，此即尊卑男女老幼，没有一个涉及经济及法治和人权，也没有一个可以改造利用。中国以道德代替法律，作者批评得很透彻。但西方的法律，也非道德的根源。比如西方所谓“自由”及“民主”，都是抽象的观念，务必透过每一个国家的地理及历史上的因素，才行得通。英国之民主，即不可能与日本之民主相同，而法国的自由也和美国的自由有差别。大历史的观点，亦即是从“技术上的角度看历史”(technical interpretation of history)。中国的革命，好像一个长隧道，须要一百零一年才可以通过。我们的生命纵长也难过九十九岁。以短衡长，只是我们个人对历史的反应，不足为大历史。将历史的基点推后三五百年才能摄入大历史的轮廓。","categories":[{"name":"历史","slug":"历史","permalink":"https://blackforest1990.github.io/categories/%E5%8E%86%E5%8F%B2/"}],"tags":[{"name":"黄仁宇","slug":"黄仁宇","permalink":"https://blackforest1990.github.io/tags/%E9%BB%84%E4%BB%81%E5%AE%87/"}]},{"title":"CPU调度","slug":"CPU调度","date":"2024-01-03T05:03:58.000Z","updated":"2024-01-19T07:03:37.319Z","comments":true,"path":"2024/01/03/CPU调度/","link":"","permalink":"https://blackforest1990.github.io/2024/01/03/CPU%E8%B0%83%E5%BA%A6/","excerpt":"","text":"基本概念 ​ 多道程序设计的目标是始终有一些进程在运行，以最大限度地提高CPU利用率。在一次性保持内存中有多个进程。当一个进程必须等待时，操作系统将CPU从该进程中取走，并将CPU交给另一个进程。这种类型的调度是操作系统功能的基本部分。几乎所有计算机资源在使用之前都会进行调度。当然，CPU是主要的计算机资源之一。因此，它的调度对操作系统设计至关重要。 CPU-I/O Burst Cycle 进程的执行包括CPU执行和I/O等待的循环。进程在这两种状态之间交替。进程执行始于一个CPU脉冲。接着是一个I/O脉冲，然后是另一个CPU脉冲，再接着是另一个I/O脉冲，如此往复。最终，最后一个CPU脉冲以系统请求终止执行而结束。 CPU脉冲的持续时间已经得到广泛测量。尽管它们在进程之间和计算机之间变化很大，但它们倾向于具有类似于下图所示的频率曲线。该曲线通常被描述为指数或超指数，其中有大量短的CPU脉冲和少量长的CPU脉冲。 I/O密集型程序通常有许多短的CPU脉冲。而CPU密集型程序可能有一些长的CPU脉冲。这种分布在选择适当的CPU调度算法时可能很重要。 CPU调度器 ​ 每当CPU变为空闲状态，操作系统必须选择就绪队列中的一个进程来执行。选择过程由短期调度器，或称为CPU调度器，执行。调度器从内存中准备执行的进程中选择一个，并分配CPU给该进程。 ​ 需要注意的是，就绪队列不一定是先进先出（FIFO）队列。当我们考虑各种调度算法时，就绪队列可以被实现为FIFO队列、优先级队列、树状结构，或者仅仅是一个无序的链表。就绪队列中的所有进程都排队等待在CPU上运行的机会。队列中的记录通常是进程的PCB。 抢占式调度 CPU调度决策可能发生在以下四种情况下： 当一个进程从运行状态切换到等待状态时（例如，由于I/O请求或对子进程终止的wait()调用导致） 当一个进程从运行状态切换到就绪状态时（例如，发生中断时） 当一个进程从等待状态切换到就绪状态时（例如，I/O完成时） 当一个进程终止时 ​ 对于情况1和4，从调度的角度来看没有选择。必须选择一个新的进程（如果在就绪队列中存在的话）来执行。然而，对于情况2和3，有选择的余地。 当调度仅在情况1和4下发生时，我们称调度方案为非抢占式。否则，它是抢占式的。在非抢占式调度下，一旦CPU被分配给一个进程，该进程将保持CPU，直到它通过终止或切换到等待状态释放CPU。Windows 95引入了抢占式调度，之后的所有Windows操作系统版本都使用了抢占式调度。Macintosh的Mac OS X操作系统也使用抢占式调度；Macintosh操作系统的早期版本依赖于合作式调度。合作式调度是某些硬件平台上唯一可用的方法，因为它不需要抢占式调度所需的特殊硬件（例如计时器）。 不幸的是，抢占式调度可能导致多个进程共享数据时出现竞态条件。考虑两个共享数据的进程的情况。在一个进程正在更新数据时，它被抢占以便第二个进程可以运行。然后，第二个进程试图读取数据，这时数据处于不一致的状态。这 抢占还影响操作系统内核的设计。在处理系统调用时，内核可能正忙于代表一个进程进行活动。这些活动可能涉及更改重要的内核数据（例如I/O队列）。如果进程在这些更改的过程中被抢占，并且内核（或设备驱动程序）需要读取或修改相同的结构，会发生混乱。包括大多数UNIX版本在内的某些操作系统通过等待系统调用完成或I/O块发生之前进行上下文切换来处理这个问题。这种方案确保内核结构简单，因为内核不会在内核数据结构处于不一致状态时抢占一个进程。不幸的是，这种内核执行模型对支持必须在给定时间范围内完成执行的实时计算的情况不利。 ​ 由于中断可以在任何时候发生（根据定义），并且因为内核不能总是忽略它们，受中断影响的代码段必须在同时使用时受到保护。操作系统需要几乎在任何时候都能接受中断。否则，可能会丢失输入或覆盖输出。为了确保这些代码段不会被多个进程同时访问，它们在进入时禁用中断，而在退出时重新启用中断。需要注意的是，禁用中断的代码段并不经常发生，通常包含很少的指令。 Dispatcher 参与CPU调度功能的另一个组件是调度程序（dispatcher）。调度程序是一个模块，它将CPU的控制权交给短期调度器选择的进程。这个功能涉及以下内容： • 切换上下文 • 切换到用户模式 • 跳转到用户程序中重新启动该程序的正确位置 调度程序应尽可能快，因为它在每次进程切换时被调用。调度程序停止一个进程并启动另一个进程运行所需的时间被称为调度延迟。 调度标准 不同的CPU调度算法具有不同的特性，选择特定算法可能会偏向某一类进程。在选择在特定情况下使用哪种算法时，我们必须考虑各种算法的特性。 有许多用于比较CPU调度算法的标准。用于比较的特性在判断哪个算法最好时可能会有很大的差异。这些标准包括： CPU利用率CPU utilization。我们希望保持CPU尽可能繁忙。在概念上，CPU利用率可以从0到100％。在实际系统中，它应该在40％（对于轻载系统）到90％（对于重载系统）之间。 吞吐量Throughput 。如果CPU正在执行进程，那么工作正在进行。工作的一个度量是每个时间单位完成的进程数量，称为吞吐量。对于长时间进程，这个速率可能是每小时一个进程；对于短交易，它可能是每秒十个进程。 周转时间Turnaround time。从某个进程的角度来看，重要的标准是执行该进程需要多长时间。从提交进程到完成的时间间隔是周转时间。周转时间是在等待进入内存、在就绪队列中等待、在CPU上执行以及进行I/O的时间段的总和。 等待时间Waiting time。CPU调度算法不会影响进程执行或进行I/O的时间。它只影响进程在就绪队列中等待的时间。等待时间是在就绪队列中等待的时间段的总和。 响应时间Response time。在交互式系统中，周转时间可能不是最佳标准。通常，进程可以在相当早的时候产生一些输出，并且在将先前的结果输出给用户的同时可以继续计算新的结果。因此，另一种度量是从提交请求到产生第一个响应的时间。这个度量称为响应时间，是启动响应所需的时间，而不是输出响应所需的时间。周转时间通常受输出设备速度的限制。 调度算法 First-Come, First-Served Scheduling ​ 最简单的算法就是FCFS算法，采用这种方案，先请求CPU的进程先分配到CPU。FCFS策略可以用FIFO队列实现。当一个进程进入到就绪队列，其PCB链接到队列的尾部。当CPU空闲的时候，CPU分配给位于队列头的进程，接着该运行进程就从队列里删除。 会产生护航效应（convoy effect）：短进程在长进程后面排队（I/O密集型进程等待CPU密集型进程） Shortest-Job-First Scheduling ​ 这一算法将每个进程与其下一个 CPU 区间段相关联。当 CPU 为空闲时，它会赋给具有最短 CPU 区间的进程。如果两个进程具有同样长度，那么可以使用FCFS 调度来处理。有两种方案，抢占与非抢占： ●非抢占：一旦CPU给了进程，它就不能被抢占，直到完成它的CPU burst ●抢占：如果一个新进程到达时的CPU burst长度小于当前执行进程的剩余时间，则抢占。这个方案被称为最短剩余时间优先(Shortest-Remaining-Time-First SRTF) SJF 调度算法可证明为最佳的，这是因为对于给定的一组进程， SJF 算法的平均等待时间最小。 虽然 SJF 算法最佳，但是它不能在短期CPU 调度层次上加以实现。因为没有办法知道下一个 CPU 区间的长度。→种方法是近似SJF 调度。虽然不知道下一个CPU 区间的长度，但是可以预测它。 Priority Scheduling ■ 每个进程都关联有一个优先级数（整数） ■ CPU分配给具有最高优先级的进程（最小整数表示最高优先级） ​ ● 抢占式 ​ ● 非抢占式 ■ SJF是一种优先级调度，其中优先级是预测的下一个CPU执行时间 优先级调度算法的一个主要问题是无穷阻塞(indefinite blocking) 或饥饿 (starvation) 。可以运行但缺乏 CPU 的进程可认为是阻塞的，它在等待CPU。优先级调度算法会使某个低优先级进程无穷等待 CPU。 低优先级进程无穷等待问题的解决之一是老化(aging) 。老化是一种技术，以逐渐增加在系统中等待很长时间的进程的优先级。 Round-Robin Scheduling 为了实现 RR 调度，将就绪队列保存为进程的FIFO 队列。新进程增加到就绪队列的尾部。 CPU 调度程序从就绪队列中选择第一个进程，设置定时器在个时间片之后中断，再分派该进程。 每个进程得到一个小单位的CPU时间（time quantum or time slice），通常是10-100毫秒。经过此时间后，该进程将被抢占并添加到准备队列的末尾。 如果准备队列中有n个进程，time quantum为q，那么每个进程一次获得1/n的CPU时间，最多为q时间单位。没有进程等待的时间超过（n-1）q时间单位。 算法的性能很大程度上依赖于时间片的大小。在极端情况下，如果时间片非常大，那么RR算法与 FCFS 算法一样。如果时间片很小(如 1 ms) 时，那么 RR 算法称为处理器共享， (从理论上来说 )n 个进程对于用户都有它自己的处理器，速度为真正处理器速度的 1/n 。这种方法用在 Control Data Corporation (CDC) 的硬件上，可以用1组硬件和10 组寄存器实现 10 个外设处理器。硬件为一组寄存器执行一个指令，然后为下一组执行。这种循环不断进行，形成了10 个慢处理器而不是1个快处理器。(实际上，由于处理器比内存快很多，而每个指令都要使用内存，所以这些处理器并不比10 个真正处理器慢很多。时间片较小的时候同样要考虑上下文切换问题。 ​ Multilevel Queue Scheduling 在进程可容易地分成不同组的情况下，可以建立另一类调度算法。例如，一个常用的划分方法是前台(交互）进程和后台(批处理）进程。 多级队列调度算法 (multilevel queue scheduling algorithm) 将就绪队列分成多个独立队列。根据进程的属性，如内存大小、进程优先级、进程类型，一个进程被永久地分配到一个队列。每个队列有自己的调度算法。例如，前台进程和后台进程可处于不同队列。前台队列可能采用 RR 算法调度，而后台队列可能采用 FCFS 算法调度。 调度方式： ​ ● 固定优先级调度；（即，先为前台服务，然后为后台服务）。可能导致饥饿。 ​ ● 时间片 – 每个队列都获得一定量的CPU时间，可以在其进程之间进行调度；即，循环轮询中的80%分配给前台，FCFS中的20%分配给后台。 Multilevel Feedback Queue Scheduling ​ 通常在使用多级队列调度算法时，进程进入系统时被永久地分配到一个队列。例如，如果前台进程和后台进程分别有独立队列，进程并不从一个队列转移到另一个队列，这是因为进程并不改变前台或后台性质。这种设置的优点是低调度开销，缺点是不够灵活。与之相反，多级反馈队列调度算法 (multilevel feedback queue scheduling algorithm) 允许进程在队列之间移动。主要思想是根据不同CPU 区间的特点以区分进程。如果进程使用过多 CPU 时间，那么它会被转移到更低优先级队列。这种方案将I/O约束和交互进程留在更高优先级队列。此外，在较低优先级队列中等待时间过长的进程会被转移到更高优先级队列。这种形式的老化阻止饥饿的发生。 例如，考虑一个多级反馈队列调度程序，它有三个队列，从0~2。调度程序首先执行队列0内的所有进程。只有当队列0为空时，它才能执行队列1内的进程。类似地，只有队列0和1都为空时，队列2的进程才能执行。到达队列1的进程会抢占队列2的进程。同样，到达队列0的进程会抢占队列1的进程。进入就绪队列的进程被放入队列0内。队列0中的每个进程都有 8ms 的时间片。如果一个进程不能在这一时间内完成，那么它就被移到队列1的尾部。如果队列0为空，队列1的头部进程会得到1个 16 ms 的时间片。如果它不能完成，那么将被抢占，并被放到队列2 中。只有当队列0和1为全时，队列2内的进程才可根据 FCFS 来运行。、 这种调度算法将给那些 CPU 区间不超过 8 ms 的进程最高优先级。这种进程可以很快地得到 CPU ，完成其 CPU 区间，并处理下一个 I/O 区间。所需超过 8 ms 但不超过 24 ms的进程也会很快被服务，但是它们的优先级比最短进程要低一点。长进程会自动沉入到队列2，在队列0和队列1不用的 CPU 周期可按FCFS顺序来服务。 通常，多级反馈队列调度程序可由下列参数来定义: 队列数量。 每个队列的调度算法。 用以确定何时升级到更高优先级队列的方法。 用以确定何时降级到更低优先级队列的方法。 用以确定进程在需要服务时应进入哪个队列的方法。 线程调度 为了能在CPU上运行，用户线程必须映射到相应的内核级线程，尽管这种映射可能是间接的，可能使用轻量级进程（LWP）。 竞争范围 ​ 用户线程与内核线程的区别之一在于它们是如何被调度的。在执行多对一模型和多对多模型的系统上，线程库调度用户级线程到一个有效的LWP上运行，这被称为进程竞争范围 (process-contention scope, PCS) 方法，因为 CPU竞争发生在属于相同进程的线程之间。当提及线程库调度用户线程到有效的LWP时，并不意味着线程实际上就在 CPU 上运行，这需要操作系统将内核线程调度到物理 CPU 上。为了决定调度哪个内核线程到 CPU ，内核采用系统竞争范围 (system-contention scope, SCS)方法来进行。采用SCS 调度方法，竞争 CPU 发生在系统的所有线程中，采用一对一的模型(如 WindowsXP 、Solaris 9 、Linux)的系统，调度仅使用 SCS 方法。 ​ 典型地，PCS 是根据优先级完成的一一调度程序选择具有最高优先级的可运行的线程来运行。用户级线程优先级由程序员给定，并且不被线程库调节，尽管有些线程库允许程序员改变线程的优先级。值得注意的是，PCS 通常抢占当前具有较高优先级的正在运行的线程，但在具有相同优先级的线程间并没有时间分割的保证。 Pthread 调度 现在，我们重点介绍了允许在线程创建期间指定PCS或SCS的POSIX Pthread API。 • PTHREAD SCOPE PROCESS 使用PCS调度线程。 • PTHREAD SCOPE SYSTEM 使用SCS调度线程。 在实现多对多模型的系统上，PTHREAD SCOPE PROCESS策略将用户级线程调度到可用的LWPs上。LWPs的数量由线程库维护，可能使用调度器激活的方式。对于多对多系统，PTHREAD SCOPE SYSTEM调度策略将为每个用户级线程创建并绑定一个LWP，有效地使用一对一策略映射线程。 Pthread IPC提供了两个函数来获取和设置争用范围策略： • pthread_attr_setscope(pthread_attr_t *attr, int scope) • pthread_attr_getscope(pthread_attr_t *attr, int *scope) 这两个函数的第一个参数都包含指向线程的属性集的指针。pthread_attr_setscope()函数的第二个参数传递PTHREAD SCOPE SYSTEM或PTHREAD SCOPE PROCESS值，指示如何设置争用范围。对于pthread_attr_getscope()，第二个参数包含一个指向int值的指针，该值设置为争用范围的当前值。如果发生错误，这些函数中的每一个都返回非零值。 如下，我们展示了一个Pthread调度API。程序首先确定了现有的争用范围，并将其设置为PTHREAD SCOPE SYSTEM。然后，它创建了五个单独的线程，这些线程将使用SCS调度策略运行。请注意，在某些系统上，只允许特定的争用范围值。例如，Linux和Mac OS X系统仅允许PTHREAD SCOPE SYSTEM。 123456789101112131415161718192021222324252627282930313233343536#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#define NUM_THREADS 5int main(int argc, char *argv[])&#123; int i, scope; pthread_t_tid[NUM_THREADS]; pthread_attr_t attr; /* get the default attributes */ pthread_attr_init(&amp;attr); /* first inquire on the current scope */ if (pthread_attr_getscope(&amp;attr, &amp;scope) != 0) fprintf(stderr, &quot;Unable to get scheduling scope\\n&quot;); else &#123; if (scope == PTHREAD_SCOPE_PROCESS) printf(&quot;PTHREAD_SCOPE_PROCESS&quot;); else if (scope == PTHREAD_SCOPE_SYSTEM) printf(&quot;PTHREAD_SCOPE_SYSTEM&quot;); else fprintf(stderr, &quot;Illegal scope value.\\n&quot;); &#125; /* set the scheduling algorithm to PCS or SCS */ pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM); /* create the threads */ for (i = 0; i &lt; NUM_THREADS; i++) pthread_create(&amp;tid[i],&amp;attr,runner,NULL); /* now join on each thread */ for (i = 0; i &lt; NUM_THREADS; i++) pthread_join(tid[i], NULL);&#125;/* Each thread will begin control in this function */void *runner(void *param)&#123; /* do some work ... */ pthread exit(0);&#125; 多处理器调度 ​ 迄今为止，主要集中讨论了单处理器系统内的CPU调度问题。如果有多个CPU ，则负载均衡(load sharing) 成为可能，但调度问题也相应地变得更为复杂。己试验过许多可能的方法，与单处理器中的CPU调度算法一样，没有最好的解决方案。下面简要讨论多处理器调度的相关问题。其中主要讨论处理器功能相同(或同构)的系统，可以将任何处理器用于运行队列内的任何进程(但请注意，即使对同构多处理器，也有一些调度限制。考虑一个系统，有一个I/O设备与一个处理器通过私有总线相连，希望使用该设备的进程必须调度到该处理器上运行)。 多处理器调度的方法 ​ 在一个多处理器中，CPU 调度的一种方法是让一个处理器(主服务器)处理所有的调度决定、I/O 处理以及其他系统活动，其他的处理器只执行用户代码。这种非对称多处理(asymmetric multiprocessing) 方法更为简单，因为只有一个处理器访问系统数据结构，减轻了数据共享的需要。另一种方法是使用对称多处理 (symmetric multiprocessing, SMP) 方法，即每个处理器自我调度。所有进程可能处于一个共同的就绪队列中，或每个处理器都有它自己的私有就绪进程队列。无论如何，调度通过每个处理器检查共同就绪队列并选择一个进程来执行。如果多个处理器试图访问和更新一个共同数据结构，那么每个处理器必须仔细编程:必须确保两个处理器不能选择同一进程，且进程不会从队列中丢失。现代操作系统多采用 SMP方法。 处理器亲和性 ​ 考虑一个进程在特定处理器上运行时对缓存内存的影响。进程最近访问的数据填充了该处理器的缓存。因此，进程的连续内存访问往往可以在缓存内存中得到满足。现在考虑一下如果进程迁移到另一个处理器会发生什么。必须使第一个处理器的缓存内存无效，并重新填充第二个处理器的缓存。由于使缓存无效和重新填充的成本很高，大多数SMP系统试图避免将进程从一个处理器迁移到另一个处理器，而是尝试让进程继续在同一处理器上运行。这被称为处理器亲和性，即进程对其当前运行的处理器有亲和性。 ​ 处理器亲和性有多种形式。当操作系统有一个策略，尝试使进程继续在同一处理器上运行（但不能保证这样做）时，我们有一个称为软亲和性的情况。在这里，操作系统将尝试将进程保持在单个处理器上，但进程可能在处理器之间迁移。相反，一些系统提供支持硬亲和性的系统调用，从而允许进程指定其可以运行的处理器子集。许多系统同时提供软亲和性和硬亲和性。例如，Linux实现了软亲和性，但它也提供了sched_setaffinity()系统调用，支持硬亲和性。 系统的主存体系结构可能会影响处理器亲和性问题。下图演示了一种特征非均匀存储访问（NUMA）的体系结构，在这种体系结构中，CPU对主存储器的某些部分的访问速度比对其他部分的速度更快。通常，这发生在包含组合CPU和内存板的系统中。板上的CPU可以更快地访问该板上的内存，而不是系统中其他板上的内存。如果操作系统的CPU调度器和内存放置算法能够协同工作，那么被分配到特定CPU的进程可以在该CPU所在的板上分配内存。这个例子还表明，操作系统通常并不像操作系统教科书中描述的那样清晰定义和实现。相反，操作系统各部分之间的“实线”通常只是“虚线”，算法以旨在优化性能和可靠性的方式创建连接。 负载均衡 ​ 在SMP系统中，保持所有处理器的工作负载均衡，以完全利用多处理器的优点，这是很重要的。否则，将会产生一个或多个处理器空闲，而其他处理器处于高工作负载状态，并有一系列进程在等待 CPU 。负载均衡(load balancing) 设法将工作负载平均地分配到SMP系统中的所有处理器上。值得注意的是，负载均衡通常只是对那些拥有自己私有的可执行进程的处理器而言是必要的。在具有共同队列的系统中，通常不需要负载均衡，因为一旦处理器空闲，它立刻从共同队列中取走一个可执行进程。但同样值得注意的是，在绝大多数支持 SMP的当代操作系统中，每个处理器都具有一个可执行进程的私有队列。 ​ 负载平衡通常有两种方法:push migration和 pull migration。对于 push migration，一个特定的任务周期性地检查每个处理器上的负载，如果发现不平衡，即通过将进程从超载处理器移到(或推送)空闲或不太忙的处理器，从而平均地分配负载。当空闲处理器从一个忙的处理器上推送 (pull)一个等待任务时，发生pull migration. push migration 和 pull migration 不能相互排斥，事实上，在负载平衡系统中它们常被并行地实现。例如，Linux 每过 200 ms (push migration) 或每当一个处理器的运行队列为空时(pull migration) ,运行其负载平衡算法。 有趣的是,负载平衡常会抵消处理器亲和性的优点。即保持一个进程在同一处理器上运行的优点在于，进程可以利用它在处理器缓存中的数据。无论是从一个处理器向另一处理器 push 或 pull 进程，都使此优点失效。事实上，在系统工程中，关于何种方式是最好的，没有绝对的规则。因此，在某些系统中，空闲的处理器常会从非空闲的处理器中 pull 进程，而在另一些系统中，只有当不平衡达到一定额度后才会移动进程。 多核处理器 ​ 传统上，SMP系统通过提供多个物理处理器来允许多个线程同时运行。然而，计算机硬件的一种最新做法是在同一物理芯片上放置多个处理器核心，从而形成多核处理器。每个核心维护其体系结构状态，因此在操作系统看来，它就像是一个独立的物理处理器。 使用多核处理器的SMP系统比每个处理器都有自己的物理芯片的系统更快且消耗更少的电力。 多核处理器可能会复杂化调度问题。让我们考虑一下可能发生的情况。研究人员发现，当处理器访问内存时，它会花费大量时间等待数据变得可用。这种情况被称为内存停顿，可能由于各种原因引起，例如缓存未命中（访问不在缓存内存中的数据）。下图说明了内存停顿。在这种情况下，处理器可能会花费高达50％的时间等待从内存获取数据。 为了解决这种情况，许多最近的硬件设计在多线程处理器核心中实现了两个（或更多）硬件线程分配给每个核心。这样，如果一个线程在等待内存时停滞，核心可以切换到另一个线程。下图说明了一个双线程处理器核心，其中线程0的执行和线程1的执行交错进行。从操作系统的角度看，每个硬件线程都表现为一个逻辑处理器，可用于运行软件线程。因此，在双线程，双核系统上，向操作系统呈现了四个逻辑处理器。UltraSPARC T3 CPU每个芯片有16个核心和每个核心8个硬件线程。从操作系统的角度看，似乎有128个逻辑处理器。 总的来说，有两种方法可以使处理核心多线程：粗粒度和细粒度多线程。对于粗粒度多线程，线程在处理器上执行，直到发生长延迟事件，例如内存停顿。由于长延迟事件引起的延迟，处理器必须切换到另一个线程开始执行。然而，由于在其他线程可以在处理器核心上开始执行之前，必须刷新指令流水线，因此在线程之间切换的成本很高。一旦新线程开始执行，它就开始用其指令填充流水线。细粒度（或交错）多线程在更细的粒度上（通常在指令周期的边界处）在线程之间进行切换。然而，细粒度系统的体系结构设计包括用于线程切换的逻辑。因此，在线程之间切换的成本很小。 注意，多线程多核处理器实际上需要两个不同级别的调度。在一个级别上是操作系统在选择在每个硬件线程（逻辑处理器）上运行哪个软件线程时必须做出的调度决策。对于这个调度级别，操作系统可以选择任何调度算法。第二个调度级别指定每个核心如何决定运行哪个硬件线程。在这种情况下有几种策略可供采用。前面提到的UltraSPARC T3使用简单的循环调度算法将8个硬件线程调度到每个核心。另一个例子是Intel Itanium，它是一个双核处理器，每个核心有两个硬件管理的线程。分配给每个硬件线程的是一个动态紧急值，范围从0到7，其中0表示最低的紧急值。 实时调度（Real-Time Scheduling） 实时操作系统的CPU调度涉及到一些特殊的问题。一般来说，我们可以区分软实时系统和硬实时系统。软实时系统不能保证关键实时进程何时被调度。它们只能保证该进程会优先于非关键进程。硬实时系统有更严格的要求。任务必须在其截止日期之前得到服务；在截止日期过期后提供的服务与没有服务是一样的。 最小化延迟 考虑实时系统的事件驱动特性。系统通常在实时等待事件发生。事件可能在软件中发生，比如定时器到期，也可能在硬件中发生，比如远程控制的车辆检测到它正在接近障碍物。当事件发生时，系统必须尽快响应并对其进行服务。我们将事件延迟定义为事件发生到服务完成经过的时间。 通常，不同的事件有不同的延迟要求。例如，防抱死刹车系统的延迟要求可能是3到5毫秒。也就是说，从车轮首次检测到打滑的时刻起，防抱死刹车系统有3到5毫秒的时间来响应并控制情况。任何需要更长时间的响应都可能导致汽车失控。相反，控制飞机上雷达的嵌入式系统可能容忍几秒钟的延迟周期。 有两种延迟影响实时系统性能： 中断延迟 调度延迟 中断延迟是指从中断到达CPU到开始服务中断的例行程序的时间段。当发生中断时，操作系统必须首先完成正在执行的指令并确定发生的中断类型。然后，在使用特定的中断服务例行程序（ISR）服务中断之前，它必须保存当前进程的状态。执行这些任务所需的总时间就是中断延迟。显然，对于实时操作系统来说，最小化中断延迟至关重要，以确保实时任务得到及时关注。事实上，对于硬实时系统，中断延迟不仅需要最小化，还必须受到约束，以满足这些系统的严格要求。 影响中断延迟的一个重要因素是中断在内核数据结构更新期间被禁用的时间。实时操作系统要求只在很短的时间内禁用中断。 调度调度程序停止一个进程并启动另一个进程所需的时间被称为调度延迟。为实时任务提供对CPU的即时访问要求实时操作系统最小化这种延迟。保持调度延迟低的最有效技术是提供具有抢占性内核。调度延迟的冲突阶段有两个组成部分： 抢占内核中正在运行的任何进程 低优先级进程释放高优先级进程所需的资源 例如，在Solaris中，禁用抢占的调度延迟超过100毫秒。启用抢占后，它减少到小于1毫秒。 Priority-Based Scheduling 实时操作系统最重要的特性之一是在实时进程需要CPU时立即做出响应。因此，实时操作系统的调度器必须支持基于优先级的抢占式算法。回顾一下，基于优先级的调度算法根据进程的重要性为每个进程分配一个优先级；更重要的任务被分配比被视为不太重要的任务更高的优先级。如果调度器还支持抢占，那么如果一个更高优先级的进程可用于运行，当前在CPU上运行的进程将被抢占。 请注意，提供一个抢占性的、基于优先级的调度器仅保证软实时功能。硬实时系统必须进一步保证实时任务将按照其截止日期要求进行服务，而要实现这样的保证需要额外的调度特性。 然而，在我们继续讨论各个调度器的详细信息之前，我们必须定义要进行调度的进程的某些特征。首先，这些进程被视为周期性的。也就是说，它们需要在固定的时间间隔（周期）内使用CPU。一旦周期性进程获得了CPU，它有一个固定的处理时间t，一个截止日期d，必须在此截止日期之前由CPU服务，并且有一个周期p。处理时间、截止日期和周期之间的关系可以表示为0 ≤ t ≤ d ≤ p。周期性任务的速率为1/p。下图说明了周期性进程随时间的执行情况。调度程序可以利用这些特性，根据进程的截止日期或速率要求分配优先级。 这种调度形式的不寻常之处在于，一个进程可能必须向调度器宣告其截止日期要求。然后，使用一种称为准入控制算法的技术，调度器执行以下两种操作之一。如果能够保证任务将按时完成，则接受该进程，并保证该进程将按时完成，否则拒绝请求，因为无法保证该任务将按照其截止日期得到服务。 速率单调调度 速率单调调度算法（Rate-Monotonic Scheduling）使用静态优先级策略和抢占来调度周期性任务。如果正在运行一个优先级较低的进程，并且有一个优先级更高的进程可用于运行，那么它将抢占较低优先级的进程。进入系统时，每个周期性任务被分配一个与其周期成反比的优先级。周期越短，优先级越高；周期越长，优先级越低。这个策略背后的理念是将更频繁需要CPU的任务分配更高的优先级。此外，假设周期性进程的处理时间对于每个CPU突发都是相同的。也就是说，每次进程获取CPU时，其CPU突发的持续时间是相同的。 让我们考虑一个例子。我们有两个进程P1和P2。P1和P2的周期分别为50和100，即p1 = 50和p2 = 100。P1的处理时间为t1 = 20，P2的处理时间为t2 = 35。每个进程的截止日期要求在其下一个周期的开始之前完成其CPU突发。我们首先要问自己是否可能安排这些任务，使每个任务都满足其截止日期。如果我们将进程Pi的CPU利用率定义为其突发到其周期的比值——t1/p1——则P1的CPU利用率为20/50 = 0.40，P2的CPU利用率为35/100 = 0.35，总CPU利用率为75％。因此，似乎我们可以以这样的方式安排这些任务，使它们都满足其截止日期，同时仍然保留CPU可用周期。 假设我们为P2分配一个比P1更高的优先级。在这种情况下，P1和P2的执行如下图所示。正如我们所见，P2首先开始执行，并在时间35完成。此时，P1开始运行；它在时间55完成其CPU突发。然而，P1的第一个截止日期是在时间50，因此调度器导致P1错过了其截止日期。 现在假设我们使用速率单调调度，其中我们为P1分配比P2更高的优先级，因为P1的周期比P2短。在这种情况下，这些进程的执行如下图所示。P1首先启动并在时间20完成其CPU突发，从而满足其第一个截止日期。P2在这时开始运行，并运行到时间50。此时，它被P1抢占，尽管它的CPU突发仍有5毫秒未完成。P1在时间70完成其CPU突发，此时调度器恢复P2。P2在时间75完成其CPU突发，也满足其第一个截止日期。系统在时间100之前处于空闲状态，然后再次调度P1。 速率单调调度被认为是最优的，因为如果一组进程无法通过这种算法进行调度，那么任何分配静态优先级的其他算法都不能进行调度。接下来，让我们看一组不能使用速率单调算法进行调度的进程。假设进程P1的周期为p1 = 50，CPU突发为t1 = 25。对于P2，相应的值为p2 = 80和t2 = 35。速率单调调度会为进程P1分配更高的优先级，因为它的周期较短。这两个进程的总CPU利用率为(25/50)+(35/80) = 0.94，因此似乎这两个进程可以被调度，并仍然使CPU有6％的可用时间。下图显示了进程P1和P2的调度。最初，P1运行直到在时间25完成其CPU突发。然后，P2开始运行，并一直运行到时间50，然后被P1抢占。此时，P2的CPU突发仍有10毫秒未完成。进程P1运行直到时间75；因此，P2错过了在时间80完成其CPU突发的截止日期。 因此，尽管速率单调调度是最优的，但它有一个限制：CPU利用率是有界的，而且并不总是能够充分最大化CPU资源。调度N个进程的最坏情况CPU利用率是。对于系统中的一个进程，CPU利用率为100％，但随着进程数量接近无穷大，它会下降到约69％。对于两个进程，CPU利用率约为83％。 最早截止期优先调度 最早截止期优先调度（Earliest-Deadline-First Scheduling，简称EDF）根据截止期动态分配任务的优先级。截止期越早，优先级越高；截止期越晚，优先级越低。在EDF策略下，当一个进程变为可运行状态时，它必须向系统宣告其截止期要求。优先级可能需要根据新可运行进程的截止期进行调整。请注意，这与速率单调调度不同，速率单调调度中优先级是固定的。 p1 = 50和t1 = 25，p2 = 80和t2 = 35。这些进程的EDF调度显示在下图中。进程P1具有最早的截止期，因此其初始优先级高于进程P2。进程P2在P1的CPU执行结束时开始运行。然而，与速率单调调度允许P1在其下一个周期开始时抢占P2不同，EDF调度允许进程P2继续运行。P2现在比P1具有更高的优先级，因为其下一个截止期（在时间80）比P1的（在时间100）更早。因此，P1和P2都满足了第一个截止期。进程P1在时间60再次开始运行，并在时间85完成其第二个CPU执行，也在时间100满足其第二个截止期。P2在此时开始运行，只是在P1在时间100的下一个周期开始时被抢占。P2被抢占是因为P1的截止期（时间150）比P2的（时间160）更早。在时间125，P1完成其CPU执行，P2恢复执行，于时间145完成，并满足其截止期。系统在时间150直到P1再次被调度时为空闲。 与速率单调算法不同，EDF调度不要求进程是周期性的，也不要求进程每个执行周期需要固定的CPU时间。唯一的要求是当进程变为可运行状态时，它必须向调度程序宣告其截止期。EDF调度的吸引力在于它在理论上是最优的，理论上它可以调度进程，使每个进程都能满足其截止期要求，而CPU利用率将达到100％。然而，在实践中，由于进程间切换的成本和中断处理的开销，不可能达到这种CPU利用率水平。 比例份额调度 比例份额调度器通过在所有应用程序之间分配 T 份额来运行。每个应用程序可以获得 N 份额的时间，从而确保该应用程序将获得总处理器时间的 N/T。例如，假设要在三个进程 A、B 和 C 之间分配 T = 100 份额的总时间。A 被分配了 50 份额，B 被分配了 15 份额，C 被分配了 20 份额。这个方案确保了 A 将拥有总处理器时间的 50%，B 将拥有 15%，C 将拥有 20%。 比例份额调度器必须与一个入场控制策略配合工作，以确保应用程序获得其分配的时间份额。入场控制策略将仅在有足够份额可用时才接受请求特定数量份额的客户端。在我们当前的例子中，我们已经分配了 50 + 15 + 20 = 85 份额的总共 100 份额。如果一个新的进程 D 请求 30 份额，入场控制器将拒绝 D 进入系统。 POSIX Real-Time Scheduling POSIX标准还为实时计算提供了扩展—POSIX.1b。在这里，我们介绍了一些与调度实时线程相关的POSIX API。POSIX定义了两个用于实时线程的调度类别： SCHED_FIFO SCHED_RR SCHED_FIFO根据先到先服务的原则使用FIFO队列调度线程。然而，在相同优先级的线程之间没有时间切片。因此，FIFO队列前面的最高优先级实时线程将获得CPU，直到它终止或阻塞。SCHED_RR使用循环轮询策略。它类似于SCHED_FIFO，只是它在相同优先级的线程之间提供时间切片。POSIX提供了另一个调度类别—SCHED_OTHER—但其实现是未定义的，并且特定于系统；它在不同系统上的行为可能不同。 POSIX API指定了以下两个用于获取和设置调度策略的函数： pthread_attr_getschedpolicy(pthread_attr_t *attr, int *policy) pthread_attr_setschedpolicy(pthread_attr_t *attr, int policy) 这两个函数的第一个参数是指向线程属性集的指针。第二个参数是（1）一个指向整数的指针，该整数被设置为当前调度策略（对于pthread_attr_getschedpolicy()），或者是（2）一个整数值（SCHED_FIFO、SCHED_RR或SCHED_OTHER）用于pthread_attr_setschedpolicy()函数。如果发生错误，这两个函数都返回非零值。 12345678910111213141516171819202122232425262728293031323334353637#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;#define NUM_THREADS 5int main(int argc, char *argv[])&#123; int i, policy; pthread_t_tid[NUM_THREADS]; pthread_attr_t attr; /* get the default attributes */ pthread_attr_init(&amp;attr); /* get the current scheduling policy */ if (pthread_attr_getschedpolicy(&amp;attr, &amp;policy) != 0) fprintf(stderr, &quot;Unable to get policy.\\n&quot;); else &#123; if (policy == SCHED_OTHER) printf(&quot;SCHED_OTHER\\n&quot;); else if (policy == SCHED_RR) printf(&quot;SCHED_RR\\n&quot;); else if (policy == SCHED_FIFO) printf(&quot;SCHED_FIFO\\n&quot;); &#125; /* set the scheduling policy - FIFO, RR, or OTHER */ if (pthread_attr_setschedpolicy(&amp;attr, SCHED_FIFO) != 0) fprintf(stderr, &quot;Unable to set policy.\\n&quot;); /* create the threads */ for (i = 0; i &lt; NUM_THREADS; i++) pthread_create(&amp;tid[i],&amp;attr,runner,NULL); /* now join on each thread */ for (i = 0; i &lt; NUM_THREADS; i++) pthread_join(tid[i], NULL);&#125;/* Each thread will begin control in this function */void *runner(void *param)&#123; /* do some work ... */ pthread exit(0);&#125; Linux Scheduling Linux中的进程调度经历了有趣的历史。在版本2.5之前，Linux内核运行的是传统UNIX调度算法的变体。然而，由于这个算法并非设计用于SMP系统，它不能很好地支持具有多个处理器的系统。此外，对于具有大量可运行进程的系统，它导致性能不佳。随着内核版本2.5的推出，调度器进行了彻底的改革，引入了一种称为O(1)的调度算法，该算法在系统中的任务数量不同的情况下都可以在恒定时间内运行。O(1)调度器还增加了对SMP系统的支持，包括处理器亲和性和处理器之间的负载平衡。然而，在实际应用中，尽管O(1)调度器在SMP系统上表现出色，但对许多桌面计算机系统上常见的交互性进程导致了较差的响应时间。在2.6内核的开发过程中，调度器再次进行了修订；在内核2.6.23版本中，完全公平调度器（CFS）成为默认的Linux调度算法。 Linux系统中的调度基于调度类。每个类别都被分配一个特定的优先级。通过使用不同的调度类，内核可以根据系统和进程的需求容纳不同的调度算法。例如，Linux服务器的调度标准可能与运行Linux的移动设备的标准不同。为了决定下一个要运行的任务，调度器选择属于最高优先级调度类的最高优先级任务。标准Linux内核实现了两个调度类别：（1）使用CFS调度算法的默认调度类，和（2）实时调度类。我们在这里讨论每个类别。当然，可以添加新的调度类别。 CFS调度器不是使用将相对优先级值与时间量的长度关联的严格规则，而是为每个任务分配CPU处理时间的一部分。这个比例是根据分配给每个任务的nice value计算的。nice value的范围是-20到+19，其中数值较低的nice value表示较高的相对优先级。具有较低nice value的任务接收CPU处理时间的比例比具有较高nice value的任务多。默认的nice value为0。 （好的这个术语来自这样的思想，即如果一个任务将其nice value从0增加到+10，它对系统中的其他任务很好，因为它降低了其相对优先级。）CFS不使用时间片的离散值，而是确定一个目标延迟，即每个可运行任务应在该时间间隔内至少运行一次的时间间隔。 CPU时间的份额是从目标延迟的值中分配的。除了具有默认值和最小值之外，如果系统中的活动任务数量增加到某个阈值以上，目标延迟的值还可以增加。 CFS调度器不直接分配优先级。相反，它通过使用每个任务的虚拟运行时间来维护虚拟运行时间来记录每个任务运行了多长时间，使用每个任务的变量vruntime。虚拟运行时间与基于任务优先级的衰减因子相关联：较低优先级的任务比较高优先级的任务具有更高的衰减率。对于正常优先级的任务（nice value为0），虚拟运行时间与实际物理运行时间相同。因此，如果一个默认优先级的任务运行了200毫秒，它的vruntime也将是200毫秒。但是，如果一个较低优先级的任务运行了200毫秒，其vruntime将高于200毫秒。同样，如果一个较高优先级的任务运行了200毫秒，其vruntime将小于200毫秒。为了决定下一个要运行的任务，调度器只需选择具有最小vruntime值的任务。此外，变得可运行的较高优先级任务可以抢占较低优先级任务。 CFS PERFORMANCE Linux CFS调度器提供了一个有效的算法来选择下一个要运行的任务。每个可运行的任务都放在一个红黑树中，这是一棵平衡的二叉搜索树，其关键字基于vruntime的值。该树如下所示： 当一个任务变为可运行时，它被添加到树中。如果树上的一个任务不可运行（例如，如果它被阻塞等待I/O），则它被移除。一般来说，已分配较少处理时间（较小的vruntime值）的任务位于树的左侧，而已分配更多处理时间的任务位于右侧。根据二叉搜索树的属性，最左边的节点具有最小的关键值，对于CFS调度器而言，这意味着它是具有最高优先级的任务。由于红黑树是平衡的，发现最左边的节点将需要O(lgN)的操作（其中N是树中的节点数）。但是，出于效率原因，Linux调度器将此值缓存在变量rb_leftmost中，因此确定下一个要运行的任务只需要检索缓存的值。 让我们看看CFS调度器的实际运行：假设两个任务具有相同的nice value。一个任务是I/O绑定的，另一个是CPU绑定的。通常，I/O绑定的任务在阻塞等待额外I/O之前只运行短时间，而CPU绑定的任务在有机会在处理器上运行时会耗尽其时间周期。因此，I/O绑定任务的vruntime值最终将低于CPU绑定任务的vruntime值，从而给I/O绑定任务比CPU绑定任务更高的优先级。此时，如果当I/O绑定任务有资格运行时（例如，当它等待的I/O变得可用时），CPU绑定任务正在执行，则I/O绑定任务将抢占CPU绑定任务。 Linux还使用POSIX标准描述的方法实现了实时调度，使用SCHED_FIFO或SCHED_RR实时策略调度的任何任务都比普通（非实时）任务具有更高的优先级。Linux使用两个单独的优先级范围，一个用于实时任务，另一个用于正常任务。实时任务在0到99的范围内分配静态优先级，并且正常（即非实时）任务在100到139之间分配优先级。这两个范围映射到全局优先级方案，其中数值较低的值表示较高的相对优先级。普通任务根据其nice value分配优先级，其中值-20映射到优先级100，值+19映射到139。 算法评估 我们如何为特定系统选择CPU调度算法呢？ 第一个问题是定义在选择算法时要使用的标准。标准通常以CPU利用率、响应时间或吞吐量的形式定义。为了选择一种算法，我们必须首先定义这些元素的相对重要性。我们的标准可能包括多个度量，例如： 在最大响应时间为1秒的约束下，最大化CPU利用率 通过确保周转时间（平均而言）与总执行时间成线性比例，最大化吞吐量 一旦选择标准被定义，我们希望评估考虑中的算法。接下来，我们将描述可以使用的各种评估方法。 确定模型 接受一个特定的预定工作负载，并定义该工作负载的每个算法的性能 排队模型 在许多系统上，运行的进程每天都会变化，因此没有静态的一组进程（或时间）可用于确定性建模。然而，可以确定的是CPU和I/O突发的分布。这些分布可以进行测量，然后进行逼近或简单估算。结果是描述特定CPU突发概率的数学公式。通常，这个分布是指数分布，并由其均值描述。同样，我们可以描述进程到达系统的时间的分布（到达时间分布）。通过这两个分布，可以计算大多数算法的平均吞吐量、利用率、等待时间等。 计算机系统被描述为一组服务器的网络。每个服务器都有一个等待进程的队列。CPU是一个具有就绪队列的服务器，I/O系统是一个具有设备队列的服务器。通过了解到达率和服务率，我们可以计算利用率、平均队列长度、平均等待时间等。这个研究领域被称为排队网络分析。 例如，n是平均队列长度（不包括正在服务的进程），W是队列中的平均等待时间，λ是新进程的平均到达速率（例如每秒三个进程）。我们期望在等待时间W内，将有 λ × W 个新进程到达队列。如果系统处于稳定状态，那么离开队列的进程数量必须等于到达的进程数量。因此n=λ×W. 这个等式被称为Little’s定理，特别有用，因为它对于任何调度算法和到达分布都是有效的。 我们可以使用Little’s定理来计算其中一个变量，如果我们知道其他两个变量。例如，如果我们知道每秒平均有7个进程到达，而队列中通常有14个进程，那么我们可以计算每个进程的平均等待时间为2秒。 排队分析可以帮助比较调度算法，但它也有局限性。目前，可以处理的算法和分布类别相当有限。复杂算法和分布的数学运算可能很难处理。因此，到达和服务分布通常以数学上可处理但不切实际的方式定义。通常还需要进行许多独立的假设，这可能不准确。由于这些困难，排队模型通常只是实际系统的近似，并且计算结果的准确性可能值得怀疑。 模拟 为了更准确地评估调度算法，我们可以使用模拟。运行模拟涉及编写计算机系统模型的程序。软件数据结构表示系统的主要组件。模拟器具有一个表示时钟的变量。随着此变量的值的增加，模拟器修改系统状态以反映设备、进程和调度程序的活动。在执行模拟时，收集和打印指示算法性能的统计数据。 驱动模拟的数据可以通过多种方式生成。最常见的方法是使用编程为根据概率分布生成进程、CPU突发时间、到达、离开等的随机数生成器。这些分布可以在数学上定义（均匀、指数、泊松）或经验性地定义。如果要经验性地定义分布，就需要对正在研究的实际系统进行测量。结果定义了实际系统中事件的分布；然后可以使用这个分布来驱动模拟。 然而，由于实际系统中连续事件之间的关系，基于分布的模拟可能不准确。频率分布仅指示每个事件发生的实例数量；它不指示它们发生的顺序。为了解决这个问题，我们可以使用跟踪磁带。我们通过监视实际系统并记录实际事件的序列来创建跟踪磁带。然后我们使用这个序列来驱动模拟。跟踪磁带为在完全相同的一组真实输入上比较两个算法提供了一种优秀的方式。这种方法可以为其输入产生准确的结果。 模拟可能很昂贵，通常需要几小时的计算机时间。更详细的模拟提供更准确的结果，但也需要更多的计算机时间。此外，跟踪磁带可能需要大量的存储空间。最后，模拟器的设计、编码和调试可能是一项重大任务。 实现 即使是模拟的准确性也是有限的。评估调度算法的唯一完全准确的方法是将其编码、放入操作系统中，并查看其运行情况。这种方法将实际的算法放入实际操作条件下进行评估。 这种方法的主要困难在于成本较高。费用不仅包括编写算法和修改操作系统以支持它（以及其所需的数据结构），还包括用户对不断变化的操作系统的反应。大多数用户对构建更好的操作系统不感兴趣；他们只是想执行他们的进程并使用它们的结果。不断变化的操作系统无助于用户完成工作。 另一个困难是算法使用的环境将发生变化。环境将不仅以通常的方式变化，即编写新程序和问题类型变化，还将因调度程序的性能而变化。如果短进程被赋予优先级，那么用户可能会将较大的进程分成一组较小的进程。如果交互进程优先于非交互进程，那么用户可能会切换到交互使用。 例如，研究人员设计了一个系统，通过查看终端I/O的数量自动分类交互和非交互进程。如果一个进程在1秒的时间间隔内没有与终端进行输入或输出，则将该进程分类为非交互，并将其移动到较低优先级的队列。作为对此策略的响应，一个程序员修改了他的程序，以在少于1秒的定期间隔内向终端写入一个任意字符。尽管终端输出完全没有意义，系统仍然为他的程序分配了较高的优先级。 最灵活的调度算法是那些可以由系统管理员或用户更改以便调整为特定应用程序或应用程序集的算法。例如，执行高端图形应用程序的工作站可能具有与Web服务器或文件服务器不同的调度需求。一些操作系统——特别是UNIX的几个版本——允许系统管理员为特定的系统配置微调调度参数。 另一种方法是使用可以修改进程或线程优先级的API。Java、POSIX和Windows API提供了这样的功能。这种方法的缺点在于调节一个系统或应用并不能在更通用的情况下改进性能。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"进程同步","slug":"进程同步","date":"2023-12-11T02:27:52.000Z","updated":"2023-12-29T06:50:54.577Z","comments":true,"path":"2023/12/11/进程同步/","link":"","permalink":"https://blackforest1990.github.io/2023/12/11/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/","excerpt":"","text":"合作进程是指可以影响或受其他在系统中执行的进程影响的进程。合作进程可以直接共享逻辑地址空间（即代码和数据），也可以通过文件或消息允许仅共享数据。前一种情况通过使用线程来实现。然而，并发访问共享数据可能导致数据不一致。在本文中，我们讨论确保共享逻辑地址空间的合作进程有序执行的各种机制，以维护数据一致性。 背景 我们将解释并发或并行执行如何导致涉及由多个进程共享的数据完整性的问题。现在我们回到有界缓冲区的问题上。我们最初的解决方案在缓冲区中最多允许 BUFFER SIZE - 1 个项同时存在。假设我们想修改算法来解决这个问题。一种可能性是添加一个整数变量 counter，初始化为 0。每次向缓冲区添加一个新项时，counter 增加，并且每次从缓冲区移除一个项时，counter 减少。生产者进程的代码可以修改如下： 12345678while (true) &#123; /* 在 next produced 中生成一个项 */ while (counter == BUFFER SIZE) ; /* 什么也不做 */ buffer[in] = next produced; in = (in + 1) % BUFFER SIZE; counter++;&#125; 消费者进程的代码可以修改如下： 12345678while (true) &#123; while (counter == 0) ; /* 什么也不做 */ next consumed = buffer[out]; out = (out + 1) % BUFFER SIZE; counter--; /* 在 next consumed 中消费一个项 */&#125; 尽管上述的生产者和消费者例程在单独执行时是正确的，但在并发执行时可能无法正常工作。举例说明，假设变量 counter 的值当前为 5，并且生产者和消费者进程同时执行语句 “counter++” 和 “counter–”。在执行这两个语句之后，变量 counter 的值可能是 4、5 或 6！然而，唯一的正确结果是 counter == 5，只有在生产者和消费者分别执行时才能正确生成。 我们可以通过以下方式说明 counter 的值可能不正确。注意语句 “counter++” 在机器语言中可以如下实现： 123register1 = counterregister1 = register1 + 1counter = register1 同样，语句 “counter–” 可以如下实现： 123register2 = counterregister2 = register2 - 1counter = register2 即使 register1 和 register2 可能是相同的物理寄存器（比如累加器），该寄存器的内容将由中断处理程序保存和恢复。“counter++” 和 “counter–” 的并发执行相当于以某种任意顺序交错执行先前介绍的低级语句的顺序执行（但在每个高级语句内部的顺序被保留）。这样的交错执行可能是以下的其中一种： T0: producer execute register1 = counter {register1 = 5} T1: producer execute register1 = register1 + 1 {register1 = 6} T2: consumer execute register2 = counter {register2 = 5} T3: consumer execute register2 = register2 − 1 {register2 = 4} T4: producer execute counter = register1 {counter = 6} T5: consumer execute counter = register2 {counter = 4} 我们将达到这种不正确的状态是因为我们允许两个进程同时操作变量 counter。这种情况，其中多个进程同时访问和操作相同的数据，并且执行的结果取决于访问发生的具体顺序，被称为竞态条件。为了防范上述竞态条件，我们需要确保一次只有一个进程能够操作变量 counter。为了做出这样的保证，我们需要以某种方式同步这些进程。 临界区问题 首先讨论所谓的临界区问题。考虑一个由 n 个进程 {P0，P1，…，Pn−1} 组成的系统。每个进程都有一段代码，称为临界区，其中进程可能正在更改共享变量、更新表、写文件等。系统的重要特性是，当一个进程正在执行其临界区时，不允许其他进程在其临界区中执行。也就是说，没有两个进程可以同时在其临界区中执行。临界区问题是设计一个协议，进程可以使用该协议进行合作。每个进程必须请求进入其临界区的权限。执行此请求的代码部分是入口部分。临界区可能后面跟着一个退出部分。其余代码是剩余部分。 解决临界区问题的解决方案必须满足以下三个要求： 互斥性。如果进程 Pi 正在执行其临界区，则其他进程不能在其临界区中执行。 进展性。如果没有进程在其临界区中执行，并且有些进程希望进入其临界区，则只有那些不在其剩余部分中执行的进程可以参与决定下一个将进入其临界区的进程，而且此选择不能无限期地推迟。 有限等待。在进程发出请求进入其临界区后，以及在该请求被授予之前，允许其他进程进入其临界区的次数存在一个限制或界限。 我们假设每个进程以非零速度执行。然而，我们不能对 n 个进程的相对速度做任何假设。 在某一时刻，操作系统中可能有许多内核模式进程处于活动状态。因此，实现操作系统的内核代码可能存在几种可能的竞争条件。以维护系统中所有打开文件列表的内核数据结构为例。如果两个进程同时打开文件，对此列表的单独更新可能导致竞争条件。其他可能存在竞争条件的内核数据结构包括用于维护内存分配的结构、用于维护进程列表的结构以及用于中断处理的结构。内核开发人员需要确保操作系统没有这样的竞争条件。 在操作系统中处理临界区的两种一般方法是：抢占式内核和非抢占式内核。抢占式内核允许在内核模式下运行的进程在运行时被抢占。非抢占式内核不允许在内核模式下运行的进程被抢占；内核模式进程将一直运行，直到退出内核模式、阻塞或自愿放弃对CPU的控制。 显然，非抢占式内核基本上不会在内核数据结构上产生竞争条件，因为一次只有一个进程在内核中活动。我们不能说抢占式内核也是如此，因此必须仔细设计以确保共享内核数据不受竞争条件的影响。对于多处理器架构来说，抢占式内核尤其难以设计，因为在这些环境中，两个内核模式进程可能同时在不同的处理器上运行。 那么，为什么有人会选择抢占式内核呢？抢占式内核可能更具响应性，因为在内核模式下运行的进程在放弃处理器给等待中的进程之前有较小的风险在任意长的时间内运行。当然，通过设计不会以这种方式行为的内核代码，也可以将此风险最小化。此外，抢占式内核更适用于实时编程，因为它允许实时进程抢占当前在内核中运行的进程。 彼得森解法 由于现代计算机架构执行基本的机器语言指令（如加载和存储）的方式，Peterson解法在这些架构上不能保证正确运行。然而，我们还是介绍这个解决方案，因为它提供了解决关键区问题的良好算法描述，并展示了设计满足互斥、进展和有限等待要求的软件所涉及的一些复杂性。 Peterson解法仅适用于两个进程，它们在它们的关键区和剩余区之间交替执行。这两个进程分别编号为P0和P1。为了方便起见，在表示Pi时，我们使用Pj来表示另一个进程，即j = 1−i。 Peterson解法要求这两个进程共享两个数据项： 12int turn;boolean flag[2]; 变量turn表示轮到谁进入关键区。也就是说，如果turn == i，则允许进程Pi在其关键区执行。数组flag用于指示一个进程是否准备好进入其关键区。例如，如果flag[i]为true，则表示Pi准备好进入其关键区。 为了进入关键区，进程Pi首先将flag[i]设置为true，然后将turn设置为j的值，从而断言如果另一个进程希望进入关键区，它可以这样做。如果两个进程同时尝试进入，turn将大致同时设置为i和j。只有一个赋值将持续；另一个将发生但将立即被覆盖。turn的最终值确定了这两个进程中哪一个被允许首先进入其关键区。 需要满足以下几个前提条件： ● 进程必须能够独立地执行，并且它们之间可以相互干扰。 ● 进程必须可以共享一些公共变量。 ● 进程之间的速度可以不同，但不能有任何假设。 这个算法的一个限制是它只适用于两个进程之间的情况。在有多个进程需要互斥访问共享资源的情况下，需要采用其他算法或技术来解决。 同步硬件 我们刚刚描述了解决临界区问题的一种基于软件的方法。然而，正如前面提到的，像Peterson这样的基于软件的解决方案在现代计算机体系结构上不能保证正常工作。在接下来的讨论中，我们将探讨解决临界区问题的几种更多方法，涉及从硬件到软件的 API 技术，这些技术对内核开发人员和应用程序员都是可用的。所有这些解决方案都基于锁的概念——即通过使用锁来保护关键区域。正如我们将看到的，这些锁的设计可能非常复杂。 我们首先介绍一些简单的硬件指令，这些指令在许多系统上都可用，并展示它们如何有效地解决临界区问题。硬件特性可以使任何编程任务变得更加容易，并提高系统效率。如果我们能够在修改共享变量时阻止中断，那么在单处理器环境中就可以简单地解决临界区问题。这样，我们可以确保当前的指令序列将被允许按顺序执行而不会被抢占。不会运行其他指令，因此不会对共享变量进行意外的修改。这通常是非抢占内核采用的方法。不幸的是，在多处理器环境中，禁用中断可能会很耗时，因为消息会传递到所有处理器。这种消息传递延迟了进入每个关键区的时间，降低了系统效率。还要考虑系统时钟如果通过中断保持更新的影响。 因此，许多现代计算机系统提供了特殊的硬件指令，允许我们测试和修改一个字的内容，或原子地交换两个字的内容——即作为一个不可中断的单元。我们可以使用这些特殊指令以相对简单的方式解决临界区问题。我们通过描述测试与设置（test and set()）和比较与交换（compare and swap()）指令来抽象这些指令背后的主要概念，而不是讨论某个具体机器的某个具体指令。 互斥锁 基于硬件的解决方案对于应用程序员来说通常很复杂且难以访问。相反，操作系统设计者构建软件工具来解决临界区问题。其中最简单的工具之一是互斥锁（mutex lock）。实际上，“mutex”一词是“mutual exclusion”的缩写。我们使用互斥锁来保护临界区，从而防止竞态条件。也就是说，进程必须在进入临界区之前获取锁；当退出临界区时释放锁。acquire()函数用于获取锁，release()函数用于释放锁。 互斥锁有一个布尔变量available，其值指示锁是否可用。如果锁可用，对acquire()的调用成功，并且然后将锁视为不可用。试图获取不可用锁的进程将被阻塞，直到锁被释放。 acquire()的定义如下： 12345acquire() &#123; while (!available) ; /* 忙等待 */ available = false;&#125; release()的定义如下： 123release() &#123; available = true;&#125; 对acquire()或release()的调用必须以原子方式执行。主要缺点是它需要忙等待。当一个进程处于其临界区时，任何尝试进入其临界区的其他进程必须在调用acquire()时不断循环。事实上，这种类型的互斥锁也称为自旋锁，因为进程在等待锁变得可用时会“旋转”。这种持续循环在真正的多编程系统中显然是个问题，其中一个单独的 CPU 在许多进程之间共享。忙等待浪费了 CPU 周期，其他一些进程可能能够有效地使用。 然而，自旋锁也有一个优点，即在进程必须等待锁时不需要进行上下文切换，而上下文切换可能需要很长时间。因此，当预计锁将被持有的时间很短时，自旋锁是有用的。它们通常用于多处理器系统，其中一个线程可以在一个处理器上“旋转”，而另一个线程在另一个处理器上执行其临界区。 信号量 互斥锁，正如我们之前提到的，通常被认为是最简单的同步工具之一。我们将研究一个更健壮的工具，它可以表现得类似于互斥锁，但也可以提供更复杂的方式来使进程同步它们的活动。信号量（Semaphore）S 是一个整数变量，除了初始化之外，只能通过两个标准的原子操作进行访问：wait() 和 signal()。wait() 操作最初被称为 P；signal() 最初被称为 V。wait() 的定义如下： 1234wait(S) &#123; while (S &lt;= 0) S--;; // 忙等待&#125; signal() 的定义如下： 123signal(S) &#123; S++;&#125; 在 wait() 和 signal() 操作中对信号量的整数值的所有修改都必须是不可分割的。也就是说，当一个进程修改信号量值时，没有其他进程可以同时修改相同的信号量值。此外，在 wait(S) 的情况下，对 S 的整数值的测试（S ≤ 0）以及可能的修改（S–）必须在没有中断的情况下执行。 信号量的使用 操作系统通常区分计数信号量和二进制信号量。计数信号量的值可以在无限的域内变化。二进制信号量的值只能在0和1之间变化。因此，二进制信号量的行为类似于互斥锁。实际上，在不提供互斥锁的系统上，可以使用二进制信号量来提供互斥。 计数信号量可以用于控制对由有限数量实例组成的特定资源的访问。信号量被初始化为可用资源的数量。每个想要使用资源的进程都执行对信号量的 wait() 操作（从而减少计数）。当进程释放资源时，它执行 signal() 操作（增加计数）。当信号量的计数变为0时，所有资源都被使用。在此之后，想要使用资源的进程将被阻塞，直到计数变得大于0。 我们还可以使用信号量来解决各种同步问题。例如，考虑两个并发运行的进程：P1 具有语句 S1，P2 具有语句 S2。假设我们要求只有在 S1 完成后才能执行 S2。我们可以通过让 P1 和 P2 共享一个初始值为0的信号量 synch 来轻松实现这个方案。在进程 P1 中，我们插入以下语句： 12S1;signal(synch); 在进程 P2 中，我们插入以下语句： 12wait(synch);S2; 因为 synch 被初始化为0，所以只有在 P1 调用 signal(synch) 之后（也就是在语句 S1 执行后），P2 才会执行 S2。 信号量的实现 为了克服忙等待的需要，我们可以修改 wait() 和 signal() 操作的定义如下：当一个进程执行 wait() 操作并发现信号量值不是正数时，它必须等待。然而，与其忙等待，该进程可以自我阻塞。阻塞操作将一个进程放入与信号量关联的等待队列中，并将进程的状态切换到等待状态。然后控制被传递给 CPU 调度程序，该程序选择另一个进程执行。 一个被阻塞在等待信号量 S 上的进程应该在另一个进程执行 signal() 操作时重新启动。进程通过 wakeup() 操作重新启动，该操作将进程从等待状态更改为就绪状态。然后，将进程放入就绪队列中。（根据 CPU 调度算法，可能或可能不会从运行中的进程切换到新的就绪进程。） 为了在这个定义下实现信号量，我们将信号量定义为： 1234typedef struct &#123; int value; struct process *list;&#125; semaphore; 每个信号量都有一个整数值和一个进程列表。当一个进程必须等待信号量时，它被添加到进程列表中。signal() 操作从等待进程列表中删除一个进程并唤醒该进程。 现在，wait() 信号量操作可以定义为： 1234567wait(semaphore *S) &#123; S-&gt;value--; if (S-&gt;value &lt; 0) &#123; add this process to S-&gt;list; block(); &#125;&#125; signal() 信号量操作可以定义为： 1234567signal(semaphore *S) &#123; S-&gt;value++; if (S-&gt;value &lt;= 0) &#123; remove a process P from S-&gt;list; wakeup(P); &#125;&#125; 阻塞操作挂起调用它的进程。唤醒操作恢复一个被阻塞的进程 P。这两个操作由操作系统作为基本系统调用提供。请注意，在这个实现中，信号量的值可能是负数，而在具有忙等待的信号量的经典定义下，信号量的值永远不会是负数。如果信号量的值是负数，那么它的大小是等待该信号量的进程数。这是由于在 wait() 操作的实现中切换了减法和测试的顺序。 等待进程列表可以通过每个进程控制块（PCB）中的链接字段轻松实现。每个信号量包含一个整数值和指向 PCB 列表的指针。为了确保有界等待，可以使用 FIFO 队列的方式添加和删除进程，其中信号量包含队列的头指针和尾指针。然而，通常情况下，列表可以使用任何排队策略。信号量列表的正确使用不依赖于信号量列表的特定排队策略。 信号量操作必须以原子方式执行是至关重要的。我们必须确保没有两个进程可以同时在同一个信号量上执行 wait() 和 signal() 操作。这是一个关键部分问题；在单处理器环境中，我们可以通过在执行 wait() 和 signal() 操作时简单地禁用中断来解决它。这个方案在单处理器环境中有效，因为一旦中断被禁用，不同进程的指令就不能交错执行。只有当前运行的进程执行，直到重新启用中断，调度程序才能重新获得控制。 在多处理器环境中，必须在每个处理器上禁用中断。否则，来自不同处理器的进程（在不同处理器上运行）的指令可能以任意的方式交错。在每个处理器上禁用中断可能是一项困难的任务，并且可能严重降低性能。因此，SMP 系统必须提供替代的锁定技术——例如 compare and swap() 或自旋锁（spinlocks）——以确保 wait() 和 signal() 的原子执行。 重要的是要承认，通过这种对 wait() 和 signal() 操作的定义，我们并没有完全消除忙等待。相反，我们将忙等待从进程应用程序的入口部分移到了关键部分。此外，我们将忙等待限制在 wait() 和 signal() 操作的关键部分，而这些部分是短暂的（如果编写正确，它们应该不超过大约十条指令）。 死锁和饥饿 使用等待队列实现的信号量可能导致两个或更多进程无限期等待仅由等待进程之一引起的事件。所涉及的事件是执行 signal() 操作。当达到这种状态时，这些进程被称为死锁。为了说明这一点，考虑一个由两个进程 P0 和 P1 组成的系统，每个进程都访问两个信号量 S 和 Q，它们的初始值均为 1： 12345678P0 P1wait(S); wait(Q);wait(Q); wait(S);. .. .. .signal(S); signal(Q);signal(Q); signal(S); 假设 P0 执行 wait(S)，然后 P1 执行 wait(Q)。当 P0 执行 wait(Q) 时，它必须等待 P1 执行 signal(Q)。同样，当 P1 执行 wait(S) 时，它必须等待 P0 执行 signal(S)。由于这些 signal() 操作无法执行，P0 和 P1 陷入了死锁。 当一个进程集合中的每个进程都在等待只能由该集合中的另一个进程引起的事件时，我们说该进程集合处于死锁状态。这里我们主要关注的事件是资源的获取和释放。与死锁相关的另一个问题是无限阻塞或饥饿，即进程在信号量中无限期等待的情况。如果我们以后进先出（LIFO，last-in, first-out）的顺序从与信号量关联的列表中移除进程，那么可能会发生无限阻塞。 优先级反转 当一个更高优先级的进程需要读取或修改当前由一个低优先级进程（或一系列低优先级进程）访问的内核数据时，就会出现调度的挑战。由于内核数据通常受到锁的保护，更高优先级的进程将不得不等待低优先级进程完成对资源的使用。如果低优先级进程被抢占，让步给优先级更高的进程，情况就变得更加复杂。 例如，假设我们有三个进程L、M和H，它们的优先级按照 L &lt; M &lt; H 的顺序。假设进程H需要资源R，而该资源当前正在被进程L访问。通常情况下，进程H会等待L完成对资源R的使用。但是，现在假设进程M变为可运行状态，从而抢占了进程L。间接地，一个具有较低优先级的进程（进程M）影响了进程H等待L放弃资源R的时间。 这个问题被称为优先级反转。它只会在具有两个以上优先级的系统中发生，因此一种解决方案是只有两个优先级。然而，对于大多数通用操作系统来说，这是不够的。通常，这些系统通过实现优先级继承协议来解决这个问题。根据该协议，所有正在访问由更高优先级进程需要的资源的进程都会继承更高的优先级，直到它们完成对这些资源的使用。当它们完成后，它们的优先级将恢复到原始值。在上面的例子中，优先级继承协议将允许进程L暂时继承进程H的优先级，从而防止进程M抢占其执行。当进程L完成对资源R的使用后，它将放弃从H继承的优先级并恢复为其原始优先级。由于资源R现在可用，接下来运行的将是进程H，而不是M。 经典的同步问题 有界缓冲问题（Bounded-Buffer Problem） 在生产者-消费者模型中，有一个有界的缓冲区，生产者将数据放入缓冲区，而消费者从中取出数据。问题在于要保证在缓冲区满或空的情况下，生产者和消费者能够正确地进行同步，避免溢出或下溢。 在有界缓冲问题中，我们有N个缓冲区，每个缓冲区可以容纳一个项。 为了解决这个问题，我们需要以下信号量： mutex：用于保护对缓冲区的访问，防止多个进程同时访问。初始值为1，表示最初是可用的。 full：用于记录当前有多少个缓冲区已经被占用（即已经放入了项）。初始值为0。 empty：用于记录当前还有多少个缓冲区是空的（即可以放入项）。初始值为N，表示所有缓冲区都是空的。 这样，通过合理地使用这三个信号量，我们可以实现多个生产者和消费者正确、安全地访问缓冲区。 生产者进程结构： 123456789101112do &#123; . . . /* produce an item in next produced */ . . . wait(empty); wait(mutex); . . . /* add next produced to the buffer */ . . . signal(mutex); signal(full);&#125; while (true); 消费者进程结构： 123456789101112do &#123; wait(full); wait(mutex); . . . /* remove an item from buffer to next consumed */ . . . signal(mutex); signal(empty); . . . /* consume the item in next consumed */ . . .&#125; while (true); 读者-写者问题（Readers and Writers Problem） 假设一个数据库需要被多个并发进程共享。其中一些进程可能只想读取数据库，而其他一些可能想要更新数据库。我们通过将前者称为读者，将后者称为写者来区分这两种类型的进程。显然，如果两个读者同时访问共享数据，不会产生不良影响。然而，如果一个写者和另一个进程（无论是读者还是写者）同时访问数据库，可能会导致混乱。 为了确保这些困扰不会出现，我们要求写者在写入数据库时具有对共享数据库的独占访问权。这个同步问题被称为读者-写者问题。自从最初提出以来，它一直被用来测试几乎每个新的同步原语。读者-写者问题有几个变种，都涉及到优先级。最简单的一个，被称为第一个读者-写者问题，要求在写者已经获得使用共享对象的许可之前，不应使读者等待。换句话说，没有读者应该因为有写者在等待而等待其他读者完成。第二个读者-写者问题要求一旦写者准备好，就应该尽快执行其写操作。换句话说，如果一个写者正在等待访问对象，那么新的读者就不能开始读取。在第一种情况下，写者可能会饿死；在第二种情况下，读者可能会饿死。 在解决第一个读者-写者问题的方案中，读者进程共享以下数据结构： 123semaphore rw_mutex = 1;semaphore mutex = 1;int read_count = 0; 信号量 rw_mutex 和 mutex 被初始化为 1；read_count 被初始化为 0。rw_mutex 信号量对读者和写者进程都是共享的。mutex 信号量用于在更新变量 read_count 时确保互斥。变量 read_count 用于跟踪当前正在读取对象的进程数量。rw_mutex 信号量用作写者的互斥信号量。它还由第一个或最后一个进入或退出关键部分的读者使用。它不被在其他读者处于其关键部分时进入或退出的读者使用。 请注意，如果一个写者在关键部分中，有 n 个读者在等待，那么一个读者排队在 rw_mutex 上，而 n - 1 个读者排队在 mutex 上。还请注意，当一个写者执行 signal(rw_mutex) 时，我们可能会恢复等待的读者或一个等待的写者的执行。选择由调度程序做出。 12345678910111213141516171819202122232425writer:do &#123; wait(rw_mutex); . . . /* writing is performed */ . . . signal(rw_mutex);&#125; while (true);Reader:do &#123; wait(mutex); read count++; if (read_count == 1) wait(rw_mutex); signal(mutex); . . . /* reading is performed */ . . . wait(mutex); read_count--; if (read_count == 0) signal(rw_mutex); signal(mutex);&#125; while (true); 读者-写者问题及其解决方案已经推广为在某些系统上提供读者-写者锁。获取读者-写者锁需要指定锁的模式：读取或写入访问。当一个进程只想读取共享数据时，它以读取模式请求读者-写者锁。希望修改共享数据的进程必须以写入模式请求锁。多个进程可以同时以读取模式获取读者-写者锁，但只有一个进程可以以写入模式获取锁，因为写者需要独占访问权限。 读者-写者锁在以下情况下最为有用： 在应用程序中很容易确定哪些进程仅读取共享数据，哪些进程仅写入共享数据。 在具有比写者或互斥锁更多读者的应用程序中。这是因为读者-写者锁通常需要更多的开销来建立，比信号量或互斥锁多。允许多个读者同时进行提高了允许并发的能力，以弥补设置读者-写者锁的开销。 哲学家就餐问题（Dining-Philosophers Problem） 在这个问题中，有五位哲学家坐在圆桌前，每位哲学家之间有一根筷子，共有五根筷子。哲学家可以进行思考或就餐，但只有同时拿到左右两根筷子时才能吃饭。 数据结构: 一个装有米饭的碗（数据集） 五根筷子的信号量数组 chopstick[5]，初始化为1，表示每根筷子最初可用。 哲学家的行为规则如下： 当一个哲学家想要进餐时，他必须先拿起他左边的筷子，然后拿起他右边的筷子，才能开始进餐。 进餐完毕后，他会先放下右边的筷子，再放下左边的筷子，然后继续思考。 为了解决哲学家就餐问题，可以使用信号量来控制筷子的访问。当一个哲学家想要拿起筷子时，他会先等待两根筷子都可用，然后将它们标记为不可用，以防止其他哲学家同时取用同一根筷子。 这个问题的解决方案需要巧妙地使用信号量来保证哲学家们可以安全地就餐，同时避免死锁等问题。 12345678910111213semaphore chopstick[5];do &#123; wait(chopstick[i]); wait(chopstick[(i+1) % 5]); . . . /* eat for awhile */ . . . signal(chopstick[i]); signal(chopstick[(i+1) % 5]); . . . /* think for awhile */ . . .&#125; while (true); 同步案例 Linux 在 Linux 的 Version 2.6 之前，Linux 内核是非抢占式的，这意味着在内核模式下运行的进程即使有更高优先级的进程可运行也不能被抢占。然而，现在的 Linux 内核是完全抢占式的，因此当一个任务在内核中运行时可以被抢占。 Linux 提供了内核中的多种不同的同步机制。由于大多数计算机体系结构都提供了原子版本的简单数学操作的指令，因此在 Linux 内核中最简单的同步技术是原子整数，它使用不透明的数据类型 atomic_t 表示。正如其名称所示，使用原子整数的所有数学操作都是无中断的。以下代码演示了声明一个原子整数计数器并执行各种原子操作： 1234567atomic_t counter;int value;atomic_set(&amp;counter, 5); /* counter = 5 */atomic_add(10, &amp;counter); /* counter = counter + 10 */atomic_sub(4, &amp;counter); /* counter = counter - 4 */atomic_inc(&amp;counter); /* counter = counter + 1 */value = atomic_read(&amp;counter); /* value = 12 */ 原子整数在需要更新整数变量（如计数器）的情况下特别高效，因为原子操作不需要锁定机制的开销。然而，它们的使用仅限于这些场景。在存在多个变量可能导致竞态条件的情况下，必须使用更复杂的锁定工具。 在 Linux 中，提供了用于保护内核中关键部分的互斥锁。在这里，任务在进入关键部分之前必须调用 mutex_lock() 函数，而在退出关键部分后必须调用 mutex_unlock() 函数。如果互斥锁不可用，调用 mutex_lock() 的任务将进入睡眠状态，并在锁的所有者调用 mutex_unlock() 时被唤醒。 Linux 还为内核提供自旋锁和信号量（以及这两种锁的读写版本）用于内核中的锁定。在 SMP（对称多处理）机器上，基本的锁定机制是自旋锁，并且内核被设计成仅在短时间内保持自旋锁。在单处理器机器上，例如仅有一个处理核心的嵌入式系统，自旋锁不适用并被启用和禁用内核抢占来替代。换句话说，在单处理器系统上，内核禁用内核抢占而不是持有自旋锁，然后启用内核抢占而不是释放自旋锁。总结如下： Linux 采用了一种有趣的方法来禁用和启用内核抢占。它提供了两个简单的系统调用——preempt_disable() 和 preempt_enable()——用于禁用和启用内核抢占。但是，如果运行在内核中的任务持有锁，则内核是不可抢占的。为了强制执行这一规则，系统中的每个任务都有一个包含计数器 preempt_count 的线程信息结构，用于指示任务持有的锁的数量。当获取锁时，preempt_count 递增，释放锁时递减。如果当前在内核中运行的任务的 preempt_count 值大于 0，则不能安全地抢占内核，因为此任务当前持有锁。如果计数为 0，则内核可以安全地被中断（假设没有未完成的对 preempt_disable() 的调用）。自旋锁以及启用和禁用内核抢占仅在必须短时间内保持锁时在内核中使用。当需要长时间持有锁时，信号量或互斥锁适合使用。 Pthreads同步 尽管Solaris中使用的锁定机制对用户级线程和内核线程都可用，但基本上讨论的同步方法涉及内核内的同步。相比之下，Pthreads API可用于用户级别的程序员，不属于任何特定的内核。该API为线程同步提供了互斥锁、条件变量和读-写锁。互斥锁代表了Pthreads中使用的基本同步技术。互斥锁用于保护代码的临界部分，即线程在进入临界部分之前获取锁，在退出临界部分时释放锁。Pthreads使用pthread_mutex_t数据类型表示互斥锁。可以使用pthread_mutex_init()函数创建互斥锁。第一个参数是指向互斥锁的指针。通过将第二个参数设置为NULL，我们将互斥锁初始化为其默认属性。下面是一个示例： 1234#include &lt;pthread.h&gt;pthread_mutex_t mutex;/* 创建互斥锁 */pthread_mutex_init(&amp;mutex, NULL); 互斥锁通过pthread_mutex_lock()和pthread_mutex_unlock()函数获取和释放。如果在调用pthread_mutex_lock()时互斥锁不可用，调用线程将被阻塞，直到所有者调用pthread_mutex_unlock()。以下代码示例说明了使用互斥锁保护临界部分： 12345/* 获取互斥锁 */pthread_mutex_lock(&amp;mutex);/* 临界部分 *//* 释放互斥锁 */pthread_mutex_unlock(&amp;mutex); 所有互斥锁函数在正确操作时返回0；如果发生错误，则这些函数返回非零错误代码。 许多实现Pthreads的系统也提供信号量，尽管信号量不是Pthreads标准的一部分，而是属于POSIX SEM扩展。POSIX规定了两种类型的信号量 - 命名和未命名。两者之间的基本区别在于，命名信号量在文件系统中有一个实际的名称，并且可以被多个无关的进程共享。未命名信号量只能由属于同一进程的线程使用。在本节中，我们描述未命名信号量。 下面的代码示例演示了用于创建和初始化未命名信号量的sem_init()函数： 1234#include &lt;semaphore.h&gt;sem_t sem;/* 创建信号量并将其初始化为1 */sem_init(&amp;sem, 0, 1); sem_init()函数传递了三个参数： 信号量的指针 表示共享级别的标志 信号量的初始值 在此示例中，通过传递标志0，我们表示该信号量只能由创建信号量的进程的线程共享。非零值将允许其他进程也访问该信号量。此外，我们将信号量初始化为值1。 我们描述了经典的wait()和signal()信号量操作。Pthreads将这些操作命名为sem_wait()和sem_post()。以下代码示例说明了使用上述创建的信号量保护临界部分： 12345/* 获取信号量 */sem_wait(&amp;sem);/* 临界部分 *//* 释放信号量 */sem_post(&amp;sem); 与互斥锁一样，所有信号量函数在成功时返回0，在发生错误条件时返回非零值。 Pthreads API还有其他扩展，包括自旋锁，但重要的是要注意，并非所有扩展都被认为在一个实现中可以从另一个实现中移植。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"线程","slug":"线程","date":"2023-12-05T07:40:18.000Z","updated":"2024-03-19T03:39:53.774Z","comments":true,"path":"2023/12/05/线程/","link":"","permalink":"https://blackforest1990.github.io/2023/12/05/%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"线程 几乎所有现代操作系统都提供了使进程能够包含多个控制线程的功能。我们引入了与多线程计算机系统相关的许多概念，包括对Pthreads和Java线程库的API的讨论。我们探讨了与多线程编程及其对操作系统设计的影响相关的许多问题。最后，我们探讨了Linux操作系统如何在内核级别支持线程。 概述 线程是CPU利用的基本单位；它包括线程ID、程序计数器、寄存器集和堆栈。它与属于同一进程的其他线程共享代码段、数据段和其他操作系统资源，如打开的文件和信号。传统（或重量级）进程具有单一的控制线程。如果一个进程具有多个控制线程，它可以同时执行多个任务。下图说明了传统的单线程进程和多线程进程之间的区别。 动机 一个应用程序通常被实现为一个独立的进程，拥有多个控制线程。例如，一个网络浏览器可能有一个线程显示图像或文本，一个线程从网络检索数据。一个文字处理软件可能有一个用于显示图形的线程，一个用于响应用户按键的线程，还有一个用于在后台执行拼写和语法检查的线程。应用程序还可以被设计为利用多核系统上的处理能力。这样的应用程序可以在多个计算核心上并行执行多个计算密集型任务。 在某些情况下，一个单一的应用程序可能需要执行多个类似的任务。例如，一个网络服务器接受客户端请求，请求可能包括网页、图像、声音等。一个繁忙的网络服务器可能同时有多个（可能是数千个）客户端访问它。如果网络服务器作为一个传统的单线程进程运行，它只能同时为一个客户端提供服务，客户端可能需要等待很长时间才能得到响应。 一种解决方法是让服务器作为一个单一进程接受请求。当服务器收到一个请求时，它创建一个单独的进程来处理该请求。事实上，在线程变得流行之前，这种进程创建的方法很常见。然而，进程创建是耗时且资源密集的。如果新进程将执行与现有进程相同的任务，为什么要承担这么大的开销呢？通常更有效的方法是使用一个包含多个线程的进程。如果网络服务器进程是多线程的，服务器将创建一个单独的线程来监听客户端请求。当请求到达时，服务器不是创建另一个进程，而是创建一个新的线程来处理该请求，并继续监听其他请求。 线程还在远程过程调用（RPC）系统中发挥着重要作用。通常，RPC服务器是多线程的。当服务器接收到一条消息时，它使用一个单独的线程来处理该消息。这使服务器能够同时处理多个并发请求。 最后，大多数操作系统内核现在也是多线程的。在内核中运行多个线程，每个线程执行特定的任务，如设备管理、内存管理或中断处理。例如，Solaris内核中有一组专门用于处理中断的线程；Linux使用一个内核线程来管理系统中的空闲内存量。 优点 响应性： 将交互式应用程序多线程化可能使程序在其部分被阻塞或执行长时间操作时继续运行，从而提高对用户的响应性。这一特性在设计用户界面时特别有用。例如，考虑当用户点击导致执行耗时操作的按钮时会发生什么情况。单线程应用程序在操作完成之前对用户不响应。相比之下，如果耗时操作在单独的线程中执行，应用程序将保持对用户的响应。 资源共享： 进程只能通过共享内存和消息传递等技术来共享资源。这些技术必须由程序员明确安排。然而，线程默认共享它们所属进程的内存和资源。共享代码和数据的好处是允许应用程序在同一地址空间内具有多个不同的活动线程。 经济性： 为进程创建分配内存和资源的成本很高。由于线程共享它们所属进程的资源，创建和切换线程更经济。在经验上评估开销的差异可能很困难，但总体上创建和管理进程比线程耗时得多。例如，在Solaris中，创建一个进程约比创建一个线程慢30倍，而上下文切换则慢5倍。 可伸缩性： 在多处理器架构中，多线程的好处可能更大，因为线程可以并行在不同的处理核心上运行。单线程的进程只能在一个处理器上运行，而不管有多少可用。 进程和线程有什么区别 进程是运行程序的抽象：二进制映像、虚拟化内存、各种内核资源、关联的安全上下文等。线程是进程中的执行单元：虚拟处理器、堆栈和程序状态。换句话说，进程是运行的二进制文件，线程是操作系统进程调度程序可调度的最小执行单元。 一个进程包含一个或多个线程。在单线程进程中，进程包含一个线程。你可以说线程就是进程——正在发生一件事。在多线程进程中，进程包含多个线程，即发生不止一件事。 现代操作系统中两个主要的虚拟化抽象是虚拟化内存和虚拟化处理器。两者都给正在运行的进程提供了一种错觉，认为它们单独消耗了机器的资源。虚拟化内存为进程提供了独特的内存视图，可以无缝映射回物理 RAM 或磁盘存储（交换空间）。虚拟化处理器让进程就像单独在处理器上运行一样，而实际上多个进程跨多个处理器执行多任务。 虚拟化内存与进程相关联，而不是与线程相关联。因此，线程共享一个内存地址空间。相反，不同的虚拟化处理器与每个线程相关联。每个线程都是一个独立的可调度实体。 多核编程 在计算机设计的早期阶段，为了提高计算性能，单CPU系统逐渐演变成多CPU系统。在系统设计的更近期的趋势是将多个计算核心集成到一个芯片上。每个核心对操作系统来说都表现为一个独立的处理器。无论这些核心是跨CPU芯片还是在CPU芯片内部，我们称这些系统为多核或多处理器系统。 多线程编程为更有效地利用这些多个计算核心和提高并发性提供了一种机制。考虑一个具有四个线程的应用程序。在一个具有单个计算核心的系统上，并发仅意味着线程的执行将随时间交错进行，因为处理核心一次只能执行一个线程。然而，在具有多个核心的系统中，并发意味着线程可以并行运行，因为系统可以为每个核心分配一个单独的线程。 在这个讨论中，请注意并行性和并发性之间的区别。如果系统能够同时执行多个任务，则系统是并行的。相反，一个并发的系统通过允许所有任务取得进展来支持多个任务。因此，有并发而没有并行是可能的。在SMP和多核架构出现之前，大多数计算机系统只有一个处理器。CPU调度器被设计为通过在系统中迅速切换进程，从而使每个进程取得进展，从而提供并行性的假象。这样的进程是同时运行的，但不是并行的。 随着系统从几十个线程增长到数千个线程，CPU设计者通过添加硬件来改善线程性能，从而提高了系统性能。现代的Intel CPU通常支持每个核心两个线程，而Oracle T4 CPU支持每个核心八个线程。这种支持意味着可以将多个线程加载到核心中进行快速切换。多核计算机毫无疑问将继续增加核心数和硬件线程支持。 阿姆达尔定律是一个公式，用于确定将额外的计算核心添加到既有串行（非并行）组件又有并行组件的应用程序中可能获得的性能增益。如果S是在具有N个处理核心的系统上必须串行执行的应用程序的部分，该公式如下所示： $$ speedup ≤ 1/(S+(1-S)/N) $$ 例如，假设我们有一个应用程序，其中75%是并行的，25%是串行的。如果我们在一个具有两个处理核心的系统上运行此应用程序，我们可以获得1.6倍的加速。如果我们添加两个额外的核心（总共四个），加速比为2.28倍。 随着N趋近于无穷大，加速比趋近于1/S。例如，如果应用程序的40%是串行执行的，最大加速比为2.5倍，无论我们添加多少个处理核心。这是阿姆达尔定律的基本原则：应用程序的串行部分可以对通过添加额外计算核心获得的性能增益产生不成比例的影响。 有人认为阿姆达尔定律未考虑当代多核系统设计中使用的硬件性能增强因素。这些观点表明，随着处理核心数量在现代计算机系统上继续增加，阿姆达尔定律可能不再适用。 编程挑战 多核系统的趋势继续给系统设计师和应用程序员带来了更大的压力，要更好地利用多个计算核心。操作系统的设计者必须编写使用多个处理核心的调度算法，以实现并行执行。对于应用程序员来说，挑战在于修改现有程序以及设计新的支持多线程的程序。 总的来说，在为多核系统编程时有五个方面的挑战： 识别任务： 这涉及检查应用程序，找到可以分为独立并发任务的领域。理想情况下，任务是相互独立的，因此可以在各个核心上并行运行。 平衡： 在识别可以并行运行的任务的同时，程序员还必须确保这些任务执行相等价值的相等工作。在某些情况下，某个任务对整个过程的贡献可能不如其他任务大。为了运行该任务，使用一个单独的执行核心可能不值得成本。 数据分割： 正如应用程序被划分为独立任务一样，任务访问和操作的数据必须分割以在不同的核心上运行。 数据依赖性： 必须检查任务访问的数据，以了解两个或更多任务之间的依赖关系。当一个任务依赖于另一个任务的数据时，程序员必须确保任务的执行是同步的，以适应数据依赖性。 测试和调试： 当一个程序在多个核心上并行运行时，可能有许多不同的执行路径。测试和调试这样的并发程序比测试和调试单线程应用程序更加困难。 并行性的类型 总的来说，有两种类型的并行性：数据并行性和任务并行性。数据并行性侧重于将相同数据的子集分布到多个计算核心上，并在每个核心上执行相同的操作。例如，考虑对大小为N的数组的内容求和。在单核系统上，一个线程将简单地对元素[0] . . . [N − 1]进行求和。然而，在双核系统上，运行在核心0上的线程A可以对元素[0] . . . [N/2 − 1]进行求和，同时在核心1上运行的线程B可以对元素[N/2] . . . [N − 1]进行求和。这两个线程将在分别的计算核心上并行运行。 任务并行性涉及在多个计算核心上分发任务（线程），而不是数据。每个线程执行唯一的操作。不同的线程可以操作相同的数据，也可以操作不同的数据。再次考虑上面的例子。与那种情况相反，任务并行性的一个例子可能涉及两个线程，每个线程在元素数组上执行唯一的统计操作。这两个线程再次在分别的计算核心上并行运行，但每个线程执行的是唯一的操作。 因此，基本上，数据并行性涉及在多个核心上分布数据，任务并行性涉及在多个核心上分布任务。然而，在实践中，很少有应用程序严格遵循数据或任务并行性。在大多数情况下，应用程序使用这两种策略的混合形式。 多线程模型 到目前为止，我们讨论的线程是在一个通用的范围内。然而，线程的支持可以在用户级别提供，用于用户线程，也可以由内核提供，用于内核线程。用户线程在内核之上得到支持，并且在没有内核支持的情况下进行管理，而内核线程由操作系统直接支持和管理。几乎所有当代操作系统，包括Windows、Linux、Mac OS X和Solaris，都支持内核线程。最终，用户线程和内核线程之间必须建立某种关系。我们将讨论建立这种关系的三种常见方式：多对一模型、一对一模型和多对多模型。 多对一模型 将多个用户级线程映射到一个内核线程。线程管理由用户空间中的线程库完成，因此它是高效的。然而，如果一个线程进行了阻塞系统调用，整个进程将被阻塞。而且，由于一次只能有一个线程访问内核，多个线程无法在多核系统上并行运行。绿色线程（Solaris系统上提供的线程库，在早期Java版本中采用了这种模型）使用了多对一模型。然而，由于无法充分利用多个处理核心，很少有系统继续使用这种模型。 一对一模型 将每个用户线程映射到一个内核线程。相较于多对一模型，它提供了更多的并发性，因为当一个线程进行阻塞系统调用时，允许另一个线程运行。它还允许多个线程在多处理器上并行运行。这个模型唯一的缺点是创建一个用户线程需要创建相应的内核线程。由于创建内核线程的开销可能会影响应用程序的性能，因此该模型的大多数实现会限制系统支持的线程数量。Linux以及Windows操作系统家族实现了一对一模型。 多对多模型 多对多模型将许多用户级线程复用到较小或同等数量的内核线程上。内核线程的数量可能特定于特定应用程序或特定机器（在多处理器上，与在单处理器上相比，一个应用程序可能会被分配更多的内核线程）。开发者可以创建任意数量的用户线程，相应的内核线程可以在多处理器上并行运行。此外，当一个线程执行阻塞系统调用时，内核可以调度另一个线程来执行。 多对多模型的一种变体仍然将许多用户级线程复用到较小或等于数量的内核线程上，但还允许将用户级线程绑定到内核线程上。这种变体有时被称为两级模型。Solaris操作系统在Solaris 9之前的版本中支持了两级模型。然而，从Solaris 9开始，该系统采用了一对一模型。 线程库 线程库为程序员提供了一个用于创建和管理线程的API。实现线程库有两种主要方法。第一种方法是在用户空间完全提供一个没有内核支持的库。库的所有代码和数据结构都存在于用户空间。这意味着调用库中的函数会导致用户空间中的本地函数调用，而不是系统调用。第二种方法是实现一个由操作系统直接支持的内核级库。在这种情况下，库的代码和数据结构存在于内核空间。调用库的API中的函数通常会导致对内核的系统调用。 今天有三个主要的线程库在使用中：POSIX Pthreads、Windows和Java。Pthreads是POSIX标准的线程扩展，可以提供作为用户级或内核级库。Windows线程库是一个在Windows系统上可用的内核级库。Java线程API允许在Java程序中直接创建和管理线程。然而，由于在大多数情况下JVM在主机操作系统之上运行，Java线程API通常是使用主机系统上可用的线程库实现的。这意味着在Windows系统上，Java线程通常是使用Windows API实现的；UNIX和Linux系统通常使用Pthreads。 对于POSIX和Windows线程，任何在函数外部声明的全局数据（即在任何函数外部声明的数据）都将在属于同一进程的所有线程之间共享。因为Java没有全局数据的概念，对共享数据的访问必须在线程之间明确安排。在函数内部声明的数据通常存储在堆栈上。由于每个线程都有自己的堆栈，每个线程都有自己的本地数据副本。 两种通用线程策略： 异步线程：一旦父线程创建了一个子线程，父线程就会恢复其执行，使得父线程和子线程同时执行。每个线程独立于其他每个线程运行，父线程无需知道其子线程何时终止。由于线程是独立的，通常在线程之间很少共享数据。 同步线程：发生在父线程创建一个或多个子线程，然后必须等待所有子线程终止才能继续执行的情况下，即所谓的fork-join策略。在这里，由父线程创建的线程并行执行工作，但是父线程在这项工作完成之前不能继续。一旦每个线程完成了它的工作，它就会终止并与其父线程合并。只有在所有子线程都合并后，父线程才能继续执行。通常，同步线程涉及大量线程之间的数据共享。 Pthreads Pthreads指的是POSIX标准（IEEE 1003.1c），定义了线程创建和同步的API。这是线程行为的规范，而不是具体的实现。操作系统设计者可以以任何他们希望的方式实现这个规范。许多系统实现了Pthreads规范；大多数是UNIX类型的系统，包括Linux、Mac OS X和Solaris。 下面显示的C程序演示了用于构建多线程程序的基本Pthreads API，该程序在单独的线程中计算非负整数的求和。当此程序开始运行时，一个控制线程从main()开始。在一些初始化之后，main()创建了一个第二个线程，该线程从runner()函数开始控制。两个线程共享全局数据sum。所有Pthreads程序都必须包含pthread.h头文件。语句pthread_t tid声明了我们将要创建的线程的标识符。每个线程都有一组属性，包括堆栈大小和调度信息。pthread_attr_t attr声明表示线程的属性。我们在函数调用pthread_attr_init(&amp;attr) 中设置这些属性。因为我们没有明确设置任何属性，所以使用了提供的默认属性。使用pthread_create()函数调用创建一个单独的线程。除了传递线程标识符和线程的属性外，我们还传递新线程将开始执行的函数的名称，即runner()函数。最后，我们传递从命令行提供的整数参数，即argv[1]。此时，程序有两个线程：在main()中的父线程和在runner()函数中执行求和操作的子线程。该程序遵循先前描述的fork-join策略：创建求和线程后，父线程将通过调用pthread_join()函数等待其终止。求和线程将在调用pthread_exit()函数时终止。一旦求和线程返回，父线程将输出共享数据sum的值。 123456789101112131415161718192021222324252627282930313233#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;int sum; /* this data is shared by the thread(s) */void *runner(void *param); /* threads call this function */int main(int argc, char *argv[])&#123; pthread_t tid; /* the thread identifier */ pthread_attr_t attr; /* set of thread attributes */ if (argc != 2) &#123; fprintf(stderr,&quot;usage: a.out &lt;integer value&gt;\\n&quot;); return -1; &#125; if (atoi(argv[1]) &lt; 0) &#123; fprintf(stderr,&quot;%d must be &gt;= 0\\n&quot;,atoi(argv[1])); return -1; &#125; /* get the default attributes */ pthread_attr_init(&amp;attr); /* create the thread */ pthread_create(&amp;tid,&amp;attr,runner,argv[1]); /* wait for the thread to exit */ pthread_join(tid,NULL); printf(&quot;sum = %d\\n&quot;,sum);&#125;/* The thread will begin control in this function */void *runner(void *param)&#123; int i, upper = atoi(param); sum = 0; for (i = 1; i &lt;= upper; i++) sum += i; pthread_exit(0);&#125; 随着多核系统的普及，编写包含多个线程的程序变得越来越普遍。使用pthread_join()函数等待多个线程的简单方法是将操作放在一个简单的for循环中。 12345#define NUM_THREADS 10/* an array of threads to be joined upon */pthread_t workers[NUM_THREADS];for (int i = 0; i &lt; NUM_THREADS; i++) pthread_join(workers[i], NULL); Java Threads 线程是Java程序中程序执行的基本模型，Java语言及其API提供了丰富的功能集，用于创建和管理线程。所有Java程序至少包含一个控制线程，即使是一个只包含main()方法的简单Java程序也会作为一个单线程在JVM中运行。Java线程可用于任何提供JVM的系统，包括Windows、Linux和Mac OS X。Java线程API也适用于Android应用程序。 在Java程序中，有两种创建线程的技术。一种方法是创建一个新类，该类派生自Thread类，并覆盖其run()方法。另一种——更常用的——技术是定义一个实现了Runnable接口的类。Runnable接口定义如下： 123public interface Runnable &#123; public abstract void run();&#125; 当一个类实现了Runnable接口时，它必须定义一个run()方法。实现run()方法的代码将作为一个单独的线程运行。 下面展示了一个在Java中确定非负整数求和的多线程程序的Java版本。Summation类实现了Runnable接口。线程的创建通过创建Thread类的对象实例并将构造函数传递给一个Runnable对象来完成。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Sum private int sum; public int getSum() &#123; return sum; &#125; public void setSum(int sum) &#123; this.sum = sum; &#125;&#125;class Summation implements Runnable&#123; private int upper; private Sum sumValue; public Summation(int upper, Sum sumValue) &#123; this.upper = upper; this.sumValue = sumValue; &#125; public void run() &#123; int sum = 0; for (int i = 0; i &lt;= upper; i++) sum += i; sumValue.setSum(sum); &#125;&#125;public class Driver&#123; public static void main(String[] args) &#123; if (args.length &gt; 0) &#123; if (Integer.parseInt(args[0]) &lt; 0) System.err.println(args[0] + &quot; must be &gt;= 0.&quot;); else &#123; Sum sumObject = new Sum(); int upper = Integer.parseInt(args[0]); Thread thrd = new Thread(new Summation(upper, sumObject)); thrd.start(); try &#123; thrd.join(); System.out.println(&quot;The sum of &quot;+upper+&quot; is &quot;+sumObject.getSum()); &#125; catch (InterruptedException ie) &#123;&#125; &#125; else System.err.println(&quot;Usage: Summation &lt;integer value&gt;&quot;); &#125;&#125; 创建Thread对象并不会直接创建新线程；相反，start()方法创建新线程。对新对象调用start()方法会执行两件事情： 在JVM中分配内存并初始化一个新线程。 调用run()方法，使线程有资格在JVM中运行。 当求和程序运行时，JVM会创建两个线程。第一个是父线程，在main()方法中开始执行。第二个线程在调用Thread对象上的start()方法时创建。这个子线程在Summation类的run()方法中开始执行。在输出求和值后，此线程在退出其run()方法时终止。 在线程之间共享数据在Windows和Pthreads中很容易，因为共享数据只需在全局声明即可。作为一种纯面向对象的语言，Java没有全局数据的概念。如果Java程序中的两个或多个线程需要共享数据，共享是通过将对共享对象的引用传递给适当的线程来进行的。通过适当的getSum()和setSum()方法引用此共享对象。Pthreads库中的父线程使用pthread_join()来等待求和线程完成后继续的方式。Java中的join()方法提供了类似的功能。 JVM和主机操作系统 JVM通常是在主机操作系统之上实现的。这种设置使得JVM能够隐藏底层操作系统的实现细节，并提供一个一致的、抽象的环境，使Java程序能够在支持JVM的任何平台上运行。JVM的规范并未说明如何将Java线程映射到底层操作系统，而是将该决策留给JVM的具体实现。例如，Windows XP操作系统使用一对一模型；因此，在这样的系统上运行的JVM的每个Java线程都映射到一个内核线程。在使用多对多模型的操作系统（例如Tru64 UNIX）上，Java线程根据多对多模型进行映射。Solaris最初使用了多对一模型来实现JVM（前面提到的绿色线程库）。JVM的后续版本则使用了多对多模型。从Solaris 9开始，Java线程使用了一对一模型进行映射。此外，Java线程库和主机操作系统上的线程库之间可能存在关系。例如，针对Windows家族操作系统的JVM实现可能在创建Java线程时使用Windows API；而Linux、Solaris和Mac OS X系统可能使用Pthreads API。 隐式线程 随着多核处理的不断增长，包含数百甚至数千个线程的应用程序即将出现。为了更好地支持多线程应用程序的设计，一种方法是将线程的创建和管理从应用程序开发人员转移到编译器和运行时库中。这种策略被称为隐式线程implicit threading，是当今的一个流行趋势。 Thread Pools 我们描述了一个多线程的Web服务器。在这种情况下，每当服务器收到一个请求，它就会创建一个单独的线程来处理该请求。虽然创建单独的线程肯定比创建单独的进程更好，但是多线程服务器仍然存在潜在的问题。第一个问题涉及创建线程所需的时间，以及一旦线程完成工作就会被丢弃。第二个问题更为棘手。如果我们允许所有并发请求在新线程中得到服务，那么系统中同时活动的线程数量就没有限制。无限制的线程可能会耗尽系统资源，如CPU时间或内存。解决这个问题的一种方法是使用线程池。 线程池背后的一般思想是在进程启动时创建一些线程，并将它们放入一个池中，等待工作。当服务器收到一个请求时，它唤醒池中的一个线程（如果有的话），并将请求传递给它进行处理。一旦线程完成服务，它就返回到池中等待更多的工作。如果池中没有可用的线程，服务器将等待直到有一个线程空闲。 线程池提供以下优势： 使用现有线程来处理请求比等待创建线程更快。 线程池限制了任一时刻存在的线程数量。这在不能支持大量并发线程的系统中尤为重要。 将要执行的任务与创建任务的机制分离，使我们能够使用不同的策略来运行任务。例如，可以安排任务在一段时间后执行或定期执行。 线程池的线程数量可以根据启发式算法设置，考虑因素包括系统中的CPU数量、物理内存量和预期的并发客户端请求数。更复杂的线程池架构可以根据使用模式动态调整池中的线程数量。这样的架构在系统负载较低时提供更小的池，从而消耗更少的内存。 OpenMP OpenMP是一组编译器指令以及用于在C、C++或FORTRAN中编写的程序的API，为共享内存环境中的并行编程提供支持。OpenMP将并行区域标识为可能并行运行的代码块。应用程序开发人员在其代码中在并行区域插入编译器指令，这些指令指示OpenMP运行时库在并行中执行该区域。以下是一个包含printf()语句的并行区域上方的编译器指令的C程序示例： 12345678910#include &lt;omp.h&gt;#include &lt;stdio.h&gt;int main(int argc, char *argv[]) &#123; /* 顺序代码 */ #pragma omp parallel printf(&quot;I am a parallel region.\\n&quot;); /* 顺序代码 */ return 0;&#125; 当OpenMP遇到指令#pragma omp parallel时，它会创建与系统中的处理核心数量相同的线程。因此，对于双核系统，将创建两个线程；对于四核系统，将创建四个线程；依此类推。然后，所有线程同时执行并行区域。每个线程退出并行区域时，它将被终止。 OpenMP提供了一些额外的指令来并行运行代码区域，包括并行化循环。例如，假设我们有两个大小为N的数组a和b。我们希望将它们的内容相加并将结果放入数组c。我们可以通过使用以下包含并行化for循环指令的代码段来并行运行此任务： 1234#pragma omp parallel forfor (i = 0; i &lt; N; i++) &#123; c[i] = a[i] + b[i];&#125; OpenMP将for循环中的工作分配给它根据指令#pragma omp parallel for创建的线程。 除了提供并行化指令外，OpenMP还允许开发人员在多个并行性级别之间进行选择。例如，他们可以手动设置线程数。它还允许开发人员确定数据是否在线程之间共享或对线程私有。OpenMP在Linux、Windows和Mac OS X系统的多个开源和商业编译器上都可用。 Grand Central Dispatch Grand Central Dispatch (GCD) 是苹果的 Mac OS X 和 iOS 操作系统的一项技术，它是 C 语言的扩展、一个 API 和一个运行时库的组合，允许应用程序开发人员标识要并行运行的代码段。与 OpenMP 类似，GCD 管理大部分线程细节。 GCD 标识了称为 blocks 的 C 和 C++ 语言扩展。一个 block 简单地是一个自包含的工作单元，由插入在一对大括号 { } 前面的插入符 ^ 指定。下面是一个 block 的简单示例： 1^&#123; printf(&quot;I am a block&quot;); &#125; GCD 通过将 block 放置在调度队列上来为运行时执行调度 block。当它从队列中移除一个 block 时，它将该 block 分配给其管理的线程池中的一个可用线程。GCD 标识了两种类型的调度队列：串行队列和并发队列。 放置在串行队列上的 block 按照先进先出（FIFO）的顺序被移除。一旦一个 block 从队列中移除，它必须在移除另一个 block 之前完成执行。每个进程都有自己的串行队列（称为主队列）。开发人员可以创建局部于特定进程的额外串行队列。串行队列对于确保多个任务的顺序执行非常有用。 放置在并发队列上的 block 也按照先进先出的顺序被移除，但是可以同时移除多个 block，从而允许多个 block 并行执行。系统有三个全局并发调度队列，并且根据优先级进行区分：低、默认和高。优先级表示 block 相对重要性的近似值。简单来说，具有更高优先级的 block 应该放置在高优先级调度队列上。 以下代码段演示了如何获取默认优先级的并发队列并使用 dispatch_async() 函数将一个 block 提交到队列中： 12dispatch_queue_t queue = dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0);dispatch_async(queue, ^&#123; printf(&quot;I am a block.&quot;); &#125;); 在内部，GCD 的线程池由 POSIX 线程组成。GCD 主动管理该池，允许线程的数量根据应用程序需求和系统容量而增长和收缩。 线程问题 The fork() and exec() System Calls fork() 系统调用用于创建一个独立的、重复的进程。在多线程程序中，fork() 和 exec() 系统调用的语义发生了变化。如果程序中的一个线程调用了 fork()，那么新进程是复制所有线程，还是新进程是单线程的呢？一些 UNIX 系统选择使用两个版本的 fork()，一个版本复制所有线程，另一个版本只复制调用了 fork() 系统调用的线程。 如果一个线程调用了 exec() 系统调用，那么参数中指定的程序将替换整个进程，包括所有线程。 选择使用 fork() 的哪个版本取决于应用程序。如果在 fork() 后立即调用 exec()，则复制所有线程是不必要的，因为参数中指定的程序将替换进程。在这种情况下，只复制调用线程是合适的。然而，如果在 fork() 后独立的进程没有调用 exec()，那么独立的进程应该复制所有线程。 信号处理 信号在UNIX系统中用于通知一个进程特定事件发生。信号可以是同步接收的，也可以是异步接收的，这取决于事件发生的源头和原因。所有信号，无论是同步还是异步，都遵循相同的模式： 通过发生特定事件生成信号。 将信号传递给一个进程。 一旦传递，必须处理信号。 同步信号的示例包括非法内存访问和除零。如果运行中的程序执行这些操作之一，就会生成一个信号。同步信号被传递给执行引起信号的操作的同一进程（这就是它们被认为是同步的原因）。 当由运行进程外部事件生成信号时，该进程异步接收信号。此类信号的示例包括使用特定按键（例如）终止进程以及计时器超时。通常，异步信号被发送到另一个进程。 一个信号可以由两种可能的处理程序之一处理： 默认信号处理程序 用户定义的信号处理程序 每个信号都有一个默认信号处理程序，内核在处理该信号时运行该处理程序。这个默认操作可以被用户定义的信号处理程序覆盖，后者被调用以处理信号。信号的处理方式各不相同。有些信号（例如更改窗口大小）被简单地忽略；其他信号（例如非法内存访问）通过终止程序来处理。 在单线程程序中处理信号是直截了当的：信号总是传递给一个进程。然而，在多线程程序中，一个进程可能有多个线程，因此传递信号更加复杂。那么，信号应该传递到哪里呢？ 将信号传递给引起信号的线程。 将信号传递给进程中的每个线程。 将信号传递给进程中的某些线程。 为接收进程的所有信号分配一个特定的线程。 传递信号的方法取决于生成的信号类型。例如，同步信号需要传递给引起信号的线程，而异步信号的情况则不那么明确。一些异步信号（例如终止进程的信号，如）应该发送到所有线程。 传递信号的标准UNIX函数是： 1int kill(pid_t pid, int signal); 这个函数指定了要向其发送特定信号的进程（pid）。大多数多线程版本的UNIX允许线程指定它们将接受哪些信号和哪些信号将被阻塞。因此，在某些情况下，异步信号可能只被传递到不阻塞它的那些线程。但是，由于信号只需要处理一次，因此通常只将信号传递到找到的第一个不阻塞它的线程。POSIX Pthreads提供了以下函数，允许将信号传递给指定的线程（tid）： 1int pthread_kill(pthread_t tid, int signal); 线程取消 线程取消涉及在其完成之前终止线程。例如，如果多个线程同时在数据库中搜索，其中一个线程返回结果，那么可能会取消其余的线程。另一种情况可能发生在用户按下 Web 浏览器上的按钮停止网页进一步加载时。通常，一个网页使用多个线程加载 ——每个图像都在单独的线程中加载。当用户按下浏览器上的停止按钮时，加载页面的所有线程都将被取消。 即将取消的线程通常称为目标线程。取消目标线程可能发生在两种不同的场景下： 异步取消。一个线程立即终止目标线程。 延迟取消。目标线程周期性地检查是否应该终止，允许其有机会以有序的方式自行终止。 如果资源已分配给要取消的线程或要取消的线程正在更新与其他线程所共享的数据，那么取消就会有困难。这在异步取消的情况下尤其麻烦。通常，操作系统将从已取消的线程中回收系统资源，但不会回收所有资源。因此，异步取消可能无法释放必要的系统范围资源。相反，延迟取消中，一个线程指示目标线程应该被取消，但取消只在目标线程检查标志以确定是否应该取消时发生。Pthread称这些点为取消点 (cancellation point)。 在Pthreads中，使用pthread_cancel()函数启动线程取消。目标线程的标识符作为参数传递给该函数。 123456pthread_t tid;/* 创建线程 */pthread_create(&amp;tid, 0, worker, NULL);. . ./* 取消线程 */pthread_cancel(tid); 然而，调用pthread_cancel()只是表示请求取消目标线程；实际的取消取决于目标线程如何设置以处理请求。Pthreads支持三种取消模式。每种模式都定义为一种状态和一种类型，如下表所示。线程可以使用API设置其取消状态和类型。 正如表格所示，Pthreads允许线程禁用或启用取消。显然，如果禁用取消，线程将无法被取消。但是，取消请求仍然挂起，因此线程稍后可以启用取消并响应请求。默认的取消类型是延迟取消。在这里，取消仅在线程到达取消点cancellation point时发生。建立取消点的一种技术是调用pthread_testcancel()函数。如果发现有挂起的取消请求，则将调用一个称为清理处理程序cleanup handler 的函数。此函数允许线程释放可能已经获取的任何资源，然后线程终止。 以下代码演示了线程如何使用延迟取消响应取消请求： 123456while (1) &#123; /* 进行一段时间的工作 */ /* . . . */ /* 检查是否有取消请求 */ pthread_testcancel();&#125; 由于前述问题，Pthreads文档不推荐使用异步取消。因此，我们在这里不进行介绍。有趣的是，在Linux系统上，使用Pthreads API进行线程取消是通过信号处理的。 Thread-Local Storage 属于一个进程的线程共享该进程的数据。事实上，这种数据共享是多线程编程的一个好处之一。然而，在某些情况下，每个线程可能需要拥有某些数据的自己的副本。我们将这样的数据称为线程本地存储（TLS）。例如，在事务处理系统中，我们可能在单独的线程中处理每个事务。此外，每个事务可能被分配一个唯一的标识符。为了将每个线程与其唯一标识符关联起来，我们可以使用线程本地存储。 很容易将 TLS 与局部变量混淆。然而，局部变量仅在单个函数调用期间可见，而 TLS 数据在函数调用之间是可见的。在某些方面，TLS 与静态数据类似。区别在于 TLS 数据对于每个线程是唯一的。大多数线程库，包括 Windows 和 Pthreads，都提供对线程本地存储的某种形式的支持；Java 也提供支持。 调度程序激活 多线程程序需要考虑的最后一个问题涉及内核与线程库之间的通信，这可能是由多对多和双层模型所要求的。这种协调允许动态调整内核线程的数量，以确保最佳性能。 许多实现多对多或双层模型的系统在用户线程和内核线程之间放置了一个中间数据结构。这个数据结构通常称为轻量级进程（Lightweight Process，LWP）。对于用户线程库来说，LWP 看起来像是可以调度用户线程运行的虚拟处理器。每个 LWP 都附加到一个内核线程上，而操作系统调度的是内核线程在物理处理器上运行。如果一个内核线程阻塞（例如，等待 I/O 操作完成），LWP 也会阻塞。在上层，附加到 LWP 的用户级线程也会阻塞。 一个应用程序可能需要任意数量的 LWPs 来有效运行。考虑在单个处理器上运行的 CPU 密集型应用程序。在这种情况下，一次只能运行一个线程，因此一个 LWP 就足够了。然而，I/O 密集型的应用程序可能需要多个 LWPs 来执行。通常，每个并发的阻塞系统调用都需要一个 LWP。例如，假设同时发生了五个不同的文件读取请求。需要五个 LWPs，因为它们都可能在内核中等待 I/O 完成。如果一个进程只有四个 LWPs，那么第五个请求必须等待其中一个 LWP 从内核返回。 用户线程库和内核之间的通信方案之一被称为调度程序激活（scheduler activation）。其工作原理如下：内核提供应用程序一组虚拟处理器（LWPs），应用程序可以将用户线程调度到可用的虚拟处理器上。此外，内核必须通知应用程序某些事件。这个过程被称为 upcall。Upcalls 由线程库处理，使用 upcall 处理程序，并且 upcall 处理程序必须在虚拟处理器上运行。 触发 upcall 的一个事件是应用程序线程即将阻塞。在这种情况下，内核通过 upcall 通知应用程序线程即将阻塞，并标识特定的线程。然后，内核为应用程序分配一个新的虚拟处理器。应用程序在这个新的虚拟处理器上运行一个 upcall 处理程序，该处理程序保存阻塞线程的状态并放弃正在运行阻塞线程的虚拟处理器。然后，upcall 处理程序调度另一个有资格在新虚拟处理器上运行的线程。当阻塞线程等待的事件发生时，内核再次发起 upcall 给线程库，通知它先前被阻塞的线程现在有资格运行。这个事件的 upcall 处理程序也需要一个虚拟处理器，内核可能分配一个新的虚拟处理器或抢占其中一个用户线程并在其虚拟处理器上运行 upcall 处理程序。在标记未阻塞的线程有资格运行之后，应用程序调度一个有资格在可用虚拟处理器上运行的线程。 Linux 线程 Linux提供了fork()系统调用，具有复制进程的传统功能。Linux还提供使用clone()系统调用创建线程的能力。然而，Linux不区分进程和线程。事实上，Linux在引用程序中的控制流时使用术语task而不是process或thread。 当调用clone()时，它会传递一组标志，这些标志确定父任务和子任务之间要共享多少。假设clone()传递了标志CLONE_FS、CLONE_VM、CLONE_SIGHAND和CLONE_FILES。那么父任务和子任务将共享相同的文件系统信息（例如当前工作目录）、相同的内存空间、相同的信号处理程序和相同的打开文件集。以这种方式使用clone()相当于创建线程，因为父任务与其子任务共享大多数资源。但是，如果在调用clone()时没有设置这些标志中的任何一个，将不会发生共享，结果类似于fork()系统调用提供的功能。 共享级别的差异是可能的，因为任务在Linux内核中的表示方式。对于系统中的每个任务，都存在一个唯一的内核数据结构（具体而言，是struct task_struct）。该数据结构不存储任务的数据，而是包含指向其他数据结构的指针，其中存储了这些数据，例如表示打开文件列表、信号处理信息和虚拟内存的数据结构。当调用fork()时，将创建一个新任务，以及父进程的所有相关数据结构的副本。当调用clone()系统调用时，也会创建一个新任务。但是，与其复制所有数据结构不同，新任务根据传递给clone()的标志集指向父任务的数据结构。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"进程","slug":"进程","date":"2023-11-28T06:01:38.000Z","updated":"2024-01-09T05:17:03.533Z","comments":true,"path":"2023/11/28/进程/","link":"","permalink":"https://blackforest1990.github.io/2023/11/28/%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"进程 在所有操作系统中，一个重要的概念是进程（process）。进程本质上是正在执行的一个程序。进程不止是程序代码，程序代码有时被称为文本段（text section）。进程还包括当前活动，通过程序计数器（program counter）的值和处理器寄存器的内容来表示。另外，进程还包括堆栈段stack（函数参数，返回地址和局部变量）和数据段data section（全局变量）。进程还包括堆（heap），是在进程运行期间动态分配内存。 这里强调:程序本身不是进程:程序是被动实体，如存储在磁盘上包含一系列指令的文件内容(常被称为可执行文件) ，而进程是活动实体，它有一个程序计数器用来表示下一个要执行的命令和相关资源集合。当一个可执行文件被装入内存时，一个程序才能成为进程。装载可执行文件通常有两种方法，即双击一个代表此可执行文件的图标或在命令行中输入该文件的文件名(如prog.exe 或 a.out)。 进程状态 ● new: 进程被创建 ● running: 指令正被执行 ● waiting: 该进程正在等待/阻止某些事件的发生 ● ready: 该进程正在等待被分配给一个处理器 ● terminated: 该进程已完成执行 进程控制块 每个进程在操作系统内用进程控制块 (process control block. PCB)来表示。 进程状态:状态可包括new/running/waiting/ready/terminated。 程序计数器:计数器表示进程要执行的下个指令的地址。 CPU 寄存器:根据计算机体系结构的不同，寄存器的数量和类型也不同。它们包括累加器、索引寄存器、堆栈指针、通用寄存器和其他条件码信息寄存器。与程序计数器一起，这些状态信息在出现中断时也需要保存，以便进程以后能正确地继续执行。 CPU 调度信息:这类信息包括进程优先级、调度队列的指针和其他调度参数 内存管理信息:根据操作系统所使用的内存系统，这类信息包括基址和界限寄存器的值、页表或段表。 记账信息:这类信息包括 CPU 时间、实际使用时间、时间界限、记账数据、作业或进程数量等。 I/O 状态信息:这类信息包括分配给进程的 I/O 设备列表、打开的文件列表等。 Linux中的进程表示 Linux 操作系统中的进程控制块是通过 &lt;linux/sched.h&gt; 中的 task_struct 来表示的。这个结构包含了表示一个进程所需要的所有信息，包括进程的状态、调度和内存管理信息、打开文件列表和指向父进程和所有子进程的指针(创建进程的进程是父进程，被进程创建的进程为子进程) 123456long state; /* state of the process */struct sched_entity se; /* scheduling information */struct task_struct *parent; /* this process’s parent */struct list_head children; /* this process’s children */struct files_struct *files; /* list of open files */struct mm_struct *mm; /* address space of this process */ 例如，进程的状态是通过这个结构中的long state 字段来表示的。在Linux 内核里，所有活动的进程是通过一个名为task_struct 的双向链表来表示的，内核为当前正在运行的进程保存了一个指针(current)。 解释一下内核如何操作一个指定进程的 task_struct 字段。假定操作系统想把当前运行进程的状态值修改成 new state。如果 current是指向当前进程的指针，那么要改变状态可以如下进行: 1current-&gt;state = new state; 进程调度 多道程序设计的目的是无论何时都有进程在运行，从而使 CPU 利用率达到最大化。分时系统在进程之间快速切换 CPU 以便用户在程序运行时能与其进行交互。为了达到此目的，进程调度选择一个可用的进程(可能从多个可用进程集合中选择)到 CPU 上执行。单处理器系统从不会有超过一个进程在运行。如果有多个进程，那么余下的则需要等待 CPU空闲并重新调度。 调度队列 进程进入系统时，会被加入到作业队列(job queue)中,该队列包括系统中所有进程。驻留在内存中就绪的、等待运行的进程保存在就绪队列(ready queue)中, 该队列通常用链表实现，其头结点指向链表的第一个和最后一个PCB块的指针。每个PCB包括一个指向就绪队列的下一个PCB的指针域。 操作系统也有其他队列。当给进程分配了CPU 后，它开始执行并最终完成，或被中断，或等待特定事件发生(如完成I/O 请求)。假设进程向一个共享设备(如磁盘)发送I/O 请求，由于系统有许多进程，磁盘可能会忙于其他进程的I/O 请求，因此该进程可能需要等待磁盘。等待特定 I/O 设备的进程列表称为设备队列(device queue)。 新进程开始处于就绪队列。它在就绪队列中等待直到被选中执行或被派遣。当进程分配到 CPU 并执行时，可能发生下面事件中的一种: 进程可能发出一个 I/O 请求，并被放到 I/O 队列中。 进程可能创建一个新的子进程，并等待其结束。 进程可能会由于中断而强制释放 CPU ，并被放回到就绪队列中。 对于前两种情况，进程最终从等待状态切换到就绪态，并放回到就绪队列中。进程继续这一循环直到终止，到时它将从所有队列中删除，其PCB 和资源将得以释放。 调度程序 进程选择是由相应的调度程序(scheduler) 来执行的。通常对于批处理系统，进程更多地是被提交，而不是马上执行。这些进程被放到大容量存储设备(通常为磁盘)的缓冲池中，保存在那里以便以后执行。长期调度程序(long-term scheduler) 或作业调度程序 (job scheduler) 从该池中选择进程，并装入内存准备执行(秒/分钟级别调度)。短期调度程序 (short-term scheduler) 或 CPU 调度程序从准备执行的进程中选择进程(毫秒级别调度)，并为之分配 CPU 。达到最好性能，长期调度程序应该选择一个合理的包含I/O 为主的和 CPU 为主的组合进程。 对于有些系统，可能没有长期调度程序。例如，UNIX 或Windows 的分时系统通常没有长期调度程序，只是简单地将所有新进程放在内存中以供短期调度程序使用。这些系统的稳定性依赖于物理限制(如可用的终端数)或用户的自我调整。如果多用户系统性能下降到令人难以接受，那么将有用户退出。 有的操作系统如分时系统，可能引入另外的中期调度程序( medium-term scheduler) 。中期调度程序的核心思想是能将进程从内存(或从 CPU 竞争)中移出，从而降低多道程序设计的程度。之后，进程能被重新调入内存，并从中断处继续执行。这种方案称为交换 (swapping) 。通过中期调度程序，进程可换出，并在后来可被换入。为了改善进程组合，或者因内存要求的改变引起了可用内存的过度使用而需要释放内存，就有必要使用交换。 上下文切换 将 CPU 切换到另一个进程需要保存当前进程的状态并恢复另一个进程的状态，这一任务称为上下文切换 (context switch) 。当发生上下文切换时，内核会将旧进程的状态保存在其 PCB 中，然后装入经调度要执行的并己保存的新进程的上下文。上下文切换时间是额外开销，因为切换时系统并不能做什么工作。上下文切换速度因机器而不同，它依赖于内存速度、必须复制的寄存器的数量、是否有特殊指令(如装入或保存所有寄存器的单个指令)，一般需几毫秒。 上下文切换时间与硬件支持密切相关。例如，有的处理器(如 Sun UltraSPARC) 提供了多组寄存器集合，上下文切换只需要简单地改变当前寄存器组的指针。当然，如果活动进程数超过了寄存器集合数量，那么系统需要像以前一样在寄存器与内存之间进行数据复制。而且，操作系统越复杂，上下文切换所要做的工作就越多。 进程操作 绝大多数系统内的进程能并发执行，它们可以动态创建和删除，因此操作系统必须提供某种机制(或工具)以创建和终止进程。 进程创建 在UNIX中，每个进程都由其进程标识符（PID）唯一标识。通过fork()系统调用创建一个新进程。新进程由原始进程的地址空间的副本组成。这种机制允许父进程与其子进程轻松通信。两个进程都在fork()之后的指令继续执行，唯一的区别是fork()的返回码对于新（子）进程是零，而子进程的（非零）进程标识符被返回给父进程。 在fork()系统调用之后，通常有两个进程中的一个使用exec()系统调用来用一个新程序替换进程的内存空间。exec()系统调用将一个二进制文件加载到内存中（销毁包含exec()系统调用的程序的内存映像）并开始执行。通过这种方式，两个进程能够通信然后各自进行。父进程然后可以创建更多的子进程；或者，如果它在子进程运行时没有其他事情可做，它可以发出wait()系统调用，将自己移出就绪队列直到子进程终止。由于对exec()的调用用新程序覆盖进程的地址空间，所以exec()的调用除非发生错误否则不会返回控制。 下面显示的C程序演示了先前描述的UNIX系统调用。现在我们有两个运行相同程序的不同进程。唯一的区别是子进程的pid（进程标识符）的值为零，而父进程的值为大于零的整数（实际上，它是子进程的实际pid）。子进程从父进程继承特权和调度属性，以及某些资源，如打开的文件。然后，子进程使用execlp()系统调用（execlp()是exec()系统调用的一种版本）命令/bin/ls覆盖其地址空间。父进程等待子进程完成，使用wait()系统调用。当子进程完成（通过隐式或显式调用exit()）时，父进程从wait()调用处恢复，然后使用exit()系统调用完成。 12345678910111213141516171819202122#include &lt;sys/types.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main()&#123; pid t pid; /* fork a child process */ pid = fork(); if (pid &lt; 0) &#123; /* error occurred */ fprintf(stderr, &quot;Fork Failed&quot;); return 1; &#125; else if (pid == 0) &#123; /* child process */ execlp(&quot;/bin/ls&quot;,&quot;ls&quot;,NULL); &#125; else &#123; /* parent process */ /* parent will wait for the child to complete */ wait(NULL); printf(&quot;Child Complete&quot;); &#125; return 0;&#125; 某些系统中(UNIX)，当进程创建了另一个进程后，父进程和子进程就以某种形式继续保持关联。子进程自身可以创建更多的进程，组成一个进程的树形结构。进程和它的所有子女以及后裔共同组成一个进程组。当用户从键盘发出一个信号时，该信号被送给当前与键盘相关的进程组中的所有成员（它们通常是在当前窗口创建的所有活动进程）。每个进程可以分别捕获该信号、忽略该信号或采取默认的动作，即被该信号杀死。 下图展示了Linux操作系统的典型进程树，显示了每个进程及其PID（进程标识符）的名称。在这里我们使用术语“进程”比较宽泛，因为Linux更倾向于使用task。init进程（始终具有PID 1）充当所有用户进程的根父进程。一旦系统启动，init进程还可以创建各种用户进程，如Web服务器、打印服务器、SSH服务器等。我们看到init的两个子进程是kthreadd和sshd。kthreadd进程负责创建代表内核执行任务的其他进程（在这种情况下是khelper和pdflush）。sshd进程负责管理通过SSH（安全外壳）连接到系统的客户端。登录进程负责管理直接登录到系统的客户端。在这个示例中，一个客户端已经登录并正在使用bash shell，其分配的PID为8416。使用bash命令行接口，该用户创建了进程ps和emacs编辑器。 进程的终止 一个进程在执行完最后一条语句并通过使用 exit() 系统调用请求操作系统删除它时终止。在这一刻，进程可以通过 wait() 系统调用向其父进程返回一个状态值（通常是一个整数）。进程的所有资源，包括物理和虚拟内存、打开的文件以及I/O缓冲区，都会被操作系统释放。 终止也可能发生在其他情况下。一个进程可以通过适当的系统调用（例如在Windows中的TerminateProcess()）引发另一个进程的终止。通常，这样的系统调用只能由要终止的进程的父进程调用。否则，用户可以任意终止彼此的任务。请注意，如果父进程要终止子进程，父进程需要知道子进程的标识。因此，当一个进程创建一个新的进程时，新创建的进程的标识会传递给父进程。 父进程可能出于多种原因终止其子进程的执行： 子进程已经超出了其被分配的一些资源的使用限制。（要确定是否发生了这种情况，父进程必须有一种机制来检查其子进程的状态。） 分配给子进程的任务不再需要执行。 父进程正在退出，而如果其父进程终止，操作系统不允许子进程继续执行。 在一些系统中，如果一个进程的父进程终止，那么该进程就不能存在。在这种系统中，如果一个进程终止（无论是正常还是异常终止），那么它的所有子进程也必须被终止。这种现象被称为级联终止，通常由操作系统启动。 为了说明进程的执行和终止，在Linux和UNIX系统中，我们可以使用 exit() 系统调用来终止一个进程，并提供一个退出状态作为参数： 12/* exit with status 1 */exit(1); 实际上，在正常终止情况下，exit() 可以直接调用（如上所示）或间接调用（通过 main() 函数中的返回语句）。 父进程可以通过使用 wait() 系统调用等待子进程的终止。wait() 系统调用接受一个参数，允许父进程获取子进程的退出状态。此系统调用还返回终止的子进程的进程标识符，以便父进程可以知道其哪个子进程已经终止。 123pid t pid;int status;pid = wait(&amp;status); 当一个进程终止时，其资源由操作系统释放。然而，它在进程表中的条目必须一直保留，直到父进程调用 wait()，因为进程表包含了进程的退出状态。一个已经终止但其父进程尚未调用 wait() 的进程被称为僵尸进程。所有进程在终止时都会过渡到这个状态，但通常它们只会短暂存在。一旦父进程调用了 wait()，僵尸进程的进程标识符和其在进程表中的条目就会被释放。 现在考虑一下，如果父进程没有调用 wait() 而是终止了，从而使其子进程成为孤儿进程，会发生什么情况。Linux 和 UNIX 通过将 init 进程指定为孤儿进程的新父进程来解决这种情况。init 进程定期调用 wait()，从而允许收集任何孤儿进程的退出状态，并释放孤儿进程的进程标识符和进程表条目。 进程间通信 提供允许进程合作的环境有几个原因： 信息共享。由于可能有多个用户对同一信息感兴趣（例如共享文件），我们必须提供一种环境，允许对这样的信息进行并发访问。 计算加速。如果我们希望某个任务运行得更快，我们必须将其分解为子任务，每个子任务将与其他子任务并行执行。请注意，只有计算机具有多个处理核心时，才能实现这样的加速。 模块化。我们可能希望以模块化的方式构建系统，将系统功能划分为独立的进程或线程。 方便。即使是单个用户也可能同时处理许多任务。例如，用户可能同时进行编辑、听音乐和编译。 合作进程需要一种进程间通信（IPC）机制，使它们能够交换数据和信息。有两种基本的进程间通信模型：共享内存和消息传递。在共享内存模型中，建立了由合作进程共享的内存区域。进程可以通过读写数据到共享区域来交换信息。在消息传递模型中，通信通过合作进程之间交换的消息进行。 这两种模型在操作系统中都很常见，许多系统都同时实现了它们。消息传递对于交换较小量的数据很有用，因为不需要避免冲突。在分布式系统中，消息传递也比共享内存更容易实现。共享内存可能比消息传递更快，因为消息传递系统通常是通过系统调用实现的，因此需要更耗时的内核干预。在共享内存系统中，只有在建立共享内存区域时才需要系统调用。一旦共享内存建立，所有访问都被视为常规内存访问，不需要内核的帮助。 最近对具有多个处理核心的系统进行的研究表明，在这些系统上，消息传递提供了比共享内存更好的性能。共享内存存在缓存一致性问题，因为共享数据在多个缓存之间迁移。随着系统上处理核心数量的增加，我们可能会看到消息传递作为IPC的首选机制。 多进程架构 — Chrome 浏览器 许多网站包含诸如 JavaScript、Flash 和 HTML5 等主动内容，以提供丰富而动态的网页浏览体验。不幸的是，这些 Web 应用程序也可能包含软件缺陷，这可能导致响应时间变慢，甚至导致 Web 浏览器崩溃。在仅显示来自一个网站的内容的 Web 浏览器中，这并不是一个大问题。但是，大多数现代 Web 浏览器提供标签式浏览，允许单个 Web 浏览器应用程序同时打开多个网站，每个站点在单独的标签中。用户只需点击相应的标签即可在不同的站点之间切换。这种排列如下图所示： 这种方法的一个问题是，如果任何一个标签中的 Web 应用程序崩溃，整个进程，包括显示其他网站的所有其他标签，也会崩溃。谷歌的 Chrome Web 浏览器通过采用多进程架构来解决这个问题。Chrome 将进程分为三种类型：浏览器、渲染器和插件。 浏览器进程 负责管理用户界面以及磁盘和网络 I/O。在启动 Chrome 时会创建一个新的浏览器进程。只会创建一个浏览器进程。 渲染器进程 包含用于呈现网页的逻辑。因此，它们包含处理 HTML、JavaScript、图像等的逻辑。通常情况下，每个在新标签中打开的网站都会创建一个新的渲染器进程，因此可以同时存在多个渲染器进程。 插件进程 为每种正在使用的插件（如 Flash 或 QuickTime）创建一个进程。插件进程包含插件的代码以及额外的代码，使插件能够与相关的渲染器进程和浏览器进程进行通信。 多进程方法的优势在于各个网站之间运行时是隔离的。如果一个网站崩溃，只有它的渲染器进程受到影响，所有其他进程都不受影响。此外，渲染器进程在沙箱中运行，这意味着对磁盘和网络 I/O 的访问受到限制，从而最小化了任何安全漏洞的影响。 共享内存系统 使用共享内存进行进程间通信需要通信的进程建立一个共享内存区域。通常，共享内存区域存在于创建该共享内存段的进程的地址空间中。希望使用这个共享内存段进行通信的其他进程必须将其连接到它们的地址空间。通常情况下，操作系统会阻止一个进程访问另一个进程的内存。共享内存要求两个或多个进程同意解除这个限制。然后，它们可以通过在共享区域中读写数据来交换信息。数据的形式和位置由这些进程决定，并不受操作系统的控制。这些进程还负责确保它们不会同时写入相同的位置。 为了阐述合作进程的概念，让我们考虑一下生产者-消费者问题，这是一种常见的合作进程范例。生产者进程生成由消费者进程消耗的信息。例如，编译器可能生成汇编代码，由汇编器消耗。然后，汇编器可能生成目标模块，由加载器消耗。生产者-消费者问题还为客户端-服务器范式提供了一个有用的隐喻。通常我们将服务器视为生产者，客户端视为消费者。例如，Web 服务器生成（即提供）HTML 文件和图像，而客户端 Web 浏览器请求这些资源并消耗（即读取）它们。 生产者-消费者问题的一个解决方案使用了共享内存。为了允许生产者和消费者进程同时运行，我们必须有一个可以由生产者填充并由消费者清空的项目缓冲区。这个缓冲区将位于由生产者和消费者进程共享的内存区域。生产者可以在消费者正在消耗另一项时生成一项。生产者和消费者必须同步，以确保消费者不会尝试消耗尚未生成的项目。 可以使用两种类型的缓冲区。无界缓冲区对缓冲区的大小没有实际限制。消费者可能必须等待新项目，但生产者始终可以生成新项目。有界缓冲区假定一个固定的缓冲区大小。在这种情况下，如果缓冲区为空，消费者必须等待；如果缓冲区已满，生产者必须等待。 让我们更仔细地看一下有界缓冲区是如何通过共享内存进行进程间通信的。以下变量存在于生产者和消费者进程共享的内存区域中： 1234567#define BUFFER SIZE 10typedef struct &#123;. . .&#125;item;item buffer[BUFFER SIZE];int in = 0;int out = 0; 共享缓冲区被实现为一个带有两个逻辑指针（in和out）的循环数组：in指向缓冲区中的下一个空位置；out指向缓冲区中的第一个满位置。当in == out时，缓冲区为空；当((in + 1) % BUFFER SIZE) == out时，缓冲区为满。生产者进程有一个本地变量nextProduced，用于存储要生成的新项。消费者进程有一个本地变量nextConsumed，用于存储要消耗的项。 1234567891011121314151617item next_produced;while (true) &#123; /* produce an item in next produced */ while (((in + 1) % BUFFER SIZE) == out) ; /* do nothing */ buffer[in] = next_produced; in = (in + 1) % BUFFER SIZE;&#125;item next consumed;while (true) &#123; while (in == out) ; /* do nothing */ next_consumed = buffer[out]; out = (out + 1) % BUFFER SIZE; /* consume the item in next consumed */&#125; 这种方案允许在同一时间最多有BUFFER SIZE − 1个项目在缓冲区中。这个例子未解决的问题是生产者进程和消费者进程同时尝试访问共享缓冲区的情况。 An Example: POSIX Shared Memory 在POSIX系统中有多种IPC机制，包括共享内存和消息传递。在这里，我们探讨一下用于共享内存的POSIX API。POSIX共享内存使用内存映射文件进行组织，将共享内存区域与文件关联起来。一个进程首先必须使用shm_open()系统调用创建一个共享内存对象，如下所示： 1shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666); 第一个参数指定了共享内存对象的名称。希望访问这个共享内存的进程必须使用这个名称引用对象。后续的参数指定了如果对象不存在就创建它（O_CREAT），并且对象是可读写的（O_RDWR）。最后一个参数设置了共享内存对象的目录权限。对shm_open()的成功调用会返回一个整数文件描述符，用于表示共享内存对象。一旦对象建立，ftruncate()函数被用于配置对象的大小，单位是字节。调用ftruncate(shm_fd, 4096)将对象的大小设置为4096字节。最后，mmap()函数建立了一个包含共享内存对象的内存映射文件，并返回一个指向用于访问共享内存对象的内存映射文件的指针。 生产者创建了一个名为&quot;OS&quot;的共享内存对象，并向共享内存写入了&quot;Hello World!&quot;这个著名的字符串。该程序内存映射了一个指定大小的共享内存对象，并允许对该对象进行写入（显然，对于生产者来说只有写入是必要的）。标志MAP_SHARED指定对共享内存对象的更改将对所有共享该对象的进程可见。注意，我们通过调用sprintf()函数并将格式化的字符串写入指针ptr来写入共享内存对象。在每次写入之后，我们必须将指针按照写入的字节数递增。 1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;stlib.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/shm.h&gt;#include &lt;sys/stat.h&gt;int main()&#123; /* the size (in bytes) of shared memory object */ const int SIZE 4096; /* name of the shared memory object */ const char *name = &quot;OS&quot;; /* strings written to shared memory */ const char *message 0 = &quot;Hello&quot;; const char *message 1 = &quot;World!&quot;; /* shared memory file descriptor */ int shm_fd; /* pointer to shared memory obect */ void *ptr; /* create the shared memory object */ shm_fd = shm_open(name, O_CREAT | O_RDRW, 0666); /* configure the size of the shared memory object */ ftruncate(shm_fd, SIZE); /* memory map the shared memory object */ ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0); /* write to the shared memory object */ sprintf(ptr,&quot;%s&quot;,message 0); ptr += strlen(message 0); sprintf(ptr,&quot;%s&quot;,message 1); ptr += strlen(message 1); return 0;&#125; 消费者进程读取并输出共享内存的内容。消费者还调用shm_unlink()函数，在消费者访问完共享内存后删除共享内存段。 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;#include &lt;stlib.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/shm.h&gt;#include &lt;sys/stat.h&gt;int main()&#123; /* the size (in bytes) of shared memory object */ const int SIZE 4096; /* name of the shared memory object */ const char *name = &quot;OS&quot;; /* shared memory file descriptor */ int shm_fd; /* pointer to shared memory obect */ void *ptr; /* open the shared memory object */ shm_fd = shm_open(name, O_RDONLY, 0666); /* memory map the shared memory object */ ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0); /* read from the shared memory object */ printf(&quot;%s&quot;,(char *)ptr); /* remove the shared memory object */ shm_unlink(name); return 0;&#125; 消息传递系统 消息传递提供了一种机制，允许进程在不共享相同地址空间的情况下进行通信和同步它们的操作。在分布式环境中特别有用，其中通信的进程可能驻留在由网络连接的不同计算机上。例如，一个互联网聊天程序可以设计成参与聊天的用户通过交换消息进行通信。 消息传递设施至少提供两个操作： send(message)（发送消息） receive(message)（接收消息） 由进程发送的消息可以是固定大小或可变大小的。如果只能发送固定大小的消息，则系统级实现比较直接。然而，这种限制使编程任务变得更加困难。相反，可变大小的消息需要更复杂的系统级实现，但编程任务变得更简单。这是操作系统设计中经常遇到的一种权衡。 如果进程P和Q想要通信，它们必须相互发送消息并接收消息：它们之间必须存在一种通信链路。这个链接可以以多种方式实现。我们关心的不是链接的物理实现（例如共享内存、硬件总线或网络），而是它的逻辑实现。以下是逻辑实现链接和send()/receive()操作的几种方法： 直接或间接通信 同步或异步通信 自动或显式缓冲 命名 进程之间要进行通信，它们必须有一种方式来引用彼此。可以使用直接通信或间接通信。 在直接通信中，每个想要通信的进程必须明确命名通信的接收方或发送方。原语被定义如下： send(P, message) — 向进程 P 发送消息。 receive(Q, message) — 从进程 Q 接收消息。 在这个方案中，通信链具有以下属性： 每对想要通信的进程之间会自动建立一个连接。进程只需知道对方的身份就可以进行通信。 一个链接与两个进程关联。 每对进程之间存在且仅存在一个链接。 这个方案在寻址上表现出对称性，即发送方和接收方都必须命名对方才能进行通信。这个方案的变体采用非对称的寻址方式。在这种情况下，只有发送方命名接收方，而不要求接收方命名发送方。原语被定义如下： send(P, message) — 向进程 P 发送消息。 receive(id, message) — 从任何进程接收消息。变量 id 被设置为发生通信的进程的名称。 这两种方案（对称和非对称）的缺点是由于生成的进程定义的模块性有限。更改进程标识符可能需要检查所有其他进程定义。必须找到对旧标识符的所有引用，以便可以将它们修改为新标识符。总的来说，任何这种硬编码技术，在其中标识符必须明确说明的地方，都不如涉及间接寻址的技术更为可取。 在间接通信中，消息被发送到和从邮箱或端口中接收。**邮箱可以抽象地看作是一个对象，进程可以将消息放入其中，也可以从中移除消息。每个邮箱都有一个唯一的标识。**例如，POSIX 消息队列使用整数值来标识邮箱。一个进程可以通过许多不同的邮箱与另一个进程通信，但只有在它们有一个共享的邮箱时，两个进程才能通信。原语被定义如下： send(A, message) — 向邮箱 A 发送消息。 receive(A, message) — 从邮箱 A 接收消息。 在这个方案中，通信链具有以下属性： 仅当一对进程都有一个共享的邮箱时，才会在它们之间建立连接。 一个链接可以与多于两个进程关联。 对于每一对通信的进程，可能存在多个不同的链接，每个链接对应一个邮箱。 **邮箱可以由进程或操作系统拥有。**如果邮箱由进程拥有（即邮箱是进程的地址空间的一部分），那么我们区分所有者（只能通过这个邮箱接收消息）和用户（只能向邮箱发送消息）。由于每个邮箱都有一个唯一的所有者，所以不会混淆应该接收发送到该邮箱的消息的进程。当拥有邮箱的进程终止时，邮箱消失。随后任何尝试向这个邮箱发送消息的进程都必须得到通知，告诉它该邮箱已经不存在。 相反，由操作系统拥有的邮箱是独立的，不附属于任何特定的进程。操作系统必须提供一种机制，允许进程执行以下操作： 创建一个新的邮箱。 通过邮箱发送和接收消息。 删除一个邮箱。 创建新邮箱的进程默认是那个邮箱的所有者。最初，只有所有者可以通过该邮箱接收消息。但是，通过适当的系统调用，所有权和接收权限可以传递给其他进程。当然，这种规定可能导致每个邮箱有多个接收者。 同步 进程之间的通信通过对send()和receive()原语的调用来实现。实现每个原语的方式有不同的设计选项。消息传递可以是阻塞的或非阻塞的，也称为同步和异步。 阻塞发送。发送进程被阻塞，直到消息被接收进程或邮箱接收。 非阻塞发送。发送进程发送消息并继续操作。 阻塞接收。接收者被阻塞，直到有消息可用。 非阻塞接收。接收者检索到一个有效的消息或一个空消息。 send()和receive()的不同组合是可能的。当send()和receive()都是阻塞的时候，我们有了发送者和接收者之间的汇合。当我们使用阻塞的send()和receive()语句时，生产者-消费者问题的解决方案变得非常简单。生产者只需调用阻塞的send()调用，等待消息被传递给接收者或邮箱。同样，当消费者调用receive()时，它会阻塞，直到有消息可用。 1234567891011message next_produced;while (true) &#123; /* produce an item in next_produced */ send(next_produced);&#125;message next_consumed;while (true) &#123; receive(next_consumed); /* consume the item in next_consumed */&#125; 缓冲 无论通信是直接还是间接的，由通信进程交换的消息都存在于一个临时队列中。基本上，这样的队列可以通过三种方式实现： 零容量（Zero capacity）：队列的最大长度为零；因此，链接不能有任何等待的消息。在这种情况下，发送方必须阻塞，直到接收方接收到消息。 有界容量（Bounded capacity）：队列的长度有限，为n；最多可以容纳n条消息。如果在发送新消息时队列没有满，消息将被放入队列中（可以是消息的副本或消息的指针），发送方可以继续执行而无需等待。然而，链接的容量是有限的。如果链接已满，发送方必须阻塞，直到队列中有空间。 无界容量（Unbounded capacity）：队列的长度是潜在无限的；因此，任意数量的消息都可以在其中等待。发送方永远不会阻塞。 零容量的情况有时被称为没有缓冲的消息系统。其他情况被称为具有自动缓冲的系统。 客户端-服务器系统中的通信 在本节中，我们将探讨客户端-服务器系统中通信的另外三种策略：套接字（sockets）、远程过程调用（RPCs）和管道（pipes）。 Sockets 套接字被定义为通信的端点。在网络上通信的一对进程使用一对套接字，每个进程一个。套接字由IP地址和端口号拼接而成。一般来说，套接字采用客户端-服务器体系结构。服务器通过监听指定端口等待传入的客户端请求。一旦接收到请求，服务器接受来自客户端套接字的连接以完成连接。实现特定服务的服务器（例如telnet、FTP和HTTP）监听知名端口（telnet服务器监听端口23；FTP服务器监听端口21；Web或HTTP服务器监听端口80）。所有小于1024的端口都被视为知名端口；我们可以使用它们来实现标准服务。 当客户端进程发起连接请求时，它会被分配一个由其主机计算机指定的端口。此端口具有大于1024的某个任意数字。例如，如果主机X上的具有IP地址146.86.5.20的客户端希望与在地址161.25.19.8上监听端口80的Web服务器建立连接，主机X可能会被分配端口1625。连接将由一对套接字组成：主机X上的(146.86.5.20:1625)和Web服务器上的(161.25.19.8:80)。在主机之间传输的数据包将根据目标端口号传递到适当的进程。 所有连接必须是唯一的。因此，如果主机X上的另一个进程也希望与相同的Web服务器建立另一个连接，它将被分配一个大于1024且不等于1625的端口号。这确保所有连接都由唯一的套接字对组成。 Java提供了三种不同类型的套接字。基于连接的（TCP）套接字使用Socket类实现。无连接的（UDP）套接字使用DatagramSocket类。最后，MulticastSocket类是DatagramSocket类的子类。多播套接字允许将数据发送给多个接收者。 我们的示例描述了一个使用基于连接的TCP套接字的日期服务器。该操作允许客户端从服务器请求当前日期和时间。服务器监听端口6013，尽管端口可以是大于1024的任意任意数字。当接收到连接时，服务器将日期和时间返回给客户端。服务器创建一个指定将监听端口6013的ServerSocket。然后，服务器开始使用accept()方法监听该端口。服务器在accept()方法上阻塞，等待客户端请求连接。当接收到连接请求时，accept()返回一个套接字，服务器可以用来与客户端通信。 123456789101112131415161718192021222324import java.net.*;import java.io.*;public class DateServer&#123; public static void main(String[] args) &#123; try &#123; ServerSocket sock = new ServerSocket(6013); /* now listen for connections */ while (true) &#123; Socket client = sock.accept(); PrintWriter pout = new PrintWriter(client.getOutputStream(), true); /* write the Date to the socket */ pout.println(new java.util.Date().toString()); /* close the socket and resume */ /* listening for connections */ client.close(); &#125; &#125; catch (IOException ioe) &#123; System.err.println(ioe); &#125; &#125;&#125; 服务器与套接字通信的详细步骤如下。服务器首先创建一个PrintWriter对象，用于与客户端通信。PrintWriter对象允许服务器使用print()和println()方法向套接字写入输出。服务器进程通过调用println()方法向客户端发送日期。一旦将日期写入套接字，服务器关闭与客户端的套接字，并继续等待更多请求。 客户端通过创建一个套接字并连接到服务器正在监听的端口来与服务器通信。客户端创建一个Socket，并请求与IP地址为127.0.0.1、端口为6013的服务器建立连接。一旦建立连接，客户端可以使用正常的流I/O语句从套接字读取数据。在从服务器接收到日期后，客户端关闭套接字并退出。IP地址127.0.0.1是一个特殊的IP地址，被称为回环地址。当计算机引用IP地址127.0.0.1时，它指的是自己。这种机制允许同一主机上的客户端和服务器使用TCP/IP协议进行通信。IP地址127.0.0.1可以替换为运行日期服务器的另一台主机的IP地址。除了IP地址外，还可以使用实际主机名。 1234567891011121314151617181920212223import java.net.*;import java.io.*;public class DateClient&#123; public static void main(String[] args) &#123; try &#123; /* make connection to server socket */ Socket sock = new Socket(&quot;127.0.0.1&quot;,6013); InputStream in = sock.getInputStream(); BufferedReader bin = new BufferedReader(new InputStreamReader(in)); /* read the date from the socket */ String line; while ( (line = bin.readLine()) != null) System.out.println(line); /* close the socket connection*/ sock.close(); &#125; catch (IOException ioe) &#123; System.err.println(ioe); &#125; &#125;&#125; 使用套接字进行通信，尽管普遍而高效，被认为是在分布式进程之间进行低级通信的一种形式。其中一个原因是套接字仅允许在通信线程之间交换无结构的字节流。客户端或服务器应用程序有责任对数据施加结构。 远程过程调用 远程服务的最常见形式之一是RPC范例，RPC被设计为一种抽象的过程调用机制，用于在具有网络连接的系统之间使用。在许多方面，它类似于IPC机制，并且通常构建在这样的系统之上。然而，在这里，因为我们处理的是进程在不同系统上执行的环境，我们必须使用基于消息的通信方案来提供远程服务。 与IPC消息不同，RPC通信中交换的消息是结构良好的，因此不再只是数据包。每个消息都寻址到监听远程系统上某个端口的RPC守护程序，并且每个消息都包含一个标识符，指定要执行的函数以及传递给该函数的参数。然后按照请求执行函数，并将任何输出发送回请求者，以独立的消息形式。 端口只是包含在消息数据包开头的一个数字。虽然系统通常只有一个网络地址，但它可以在该地址内有多个端口，以区分它支持的许多网络服务。如果远程进程需要一个服务，它会将消息寻址到适当的端口。例如，如果一个系统希望允许其他系统能够列出其当前用户，它将具有支持这样一个RPC的守护程序，附加到一个端口上，比如端口3027。任何远程系统都可以通过向服务器的端口3027发送RPC消息来获取所需的信息（即当前用户列表）。数据将在回复消息中接收到。 RPC的语义允许客户端调用远程主机上的过程，就像在本地调用过程一样。RPC系统通过在客户端侧提供一个存根来隐藏允许通信发生的细节。通常，每个单独的远程过程都有一个单独的存根。当客户端调用远程过程时，RPC系统调用适当的存根，将提供给远程过程的参数传递给它。该存根定位服务器上的端口并对参数进行编组。参数编组涉及将参数封装为可以通过网络传输的形式。然后，存根使用消息传递向服务器发送消息。服务器端的类似存根接收此消息并在服务器上调用该过程。如果需要，返回值将使用相同的技术传递回客户端。在Windows系统上，存根代码是从使用Microsoft Interface Definition Language（MIDL）编写的规范中编译出来的，该语言用于定义客户端和服务器程序之间的接口。 必须处理的一个问题涉及客户端和服务器机器上数据表示的差异。考虑32位整数的表示。一些系统（称为big-endian）首先存储最高有效字节，而其他系统（称为little-endian）首先存储最低有效字节。在计算机体系结构内，两种顺序都没有本质的“更好”之分；相反，选择在计算机体系结构内是任意的。为了解决这样的差异，许多RPC系统定义了数据的机器无关表示。其中一种表示被称为外部数据表示（XDR）。在客户端侧，参数编组涉及在将数据发送到服务器之前将机器相关的数据转换为XDR。在服务器端，XDR数据进行解组，并转换为服务器的机器相关表示。 另一个重要问题涉及调用的语义。而本地过程调用仅在极端情况下失败，RPC可能会因为常见网络错误而失败，或者被重复执行多次。解决这个问题的一种方法是让操作系统确保消息仅被执行一次，而不是至多一次。大多数本地过程调用具有“仅一次”功能，但实现起来更为困难。 首先，考虑“至多一次”。这种语义可以通过将时间戳附加到每个消息上来实现。服务器必须保留其已处理的所有消息的时间戳历史记录，或者历史记录足够大，以确保检测到重复的消息。具有已在历史记录中的时间戳的传入消息将被忽略。然后，客户端可以发送一条或多条消息，并确保它只执行一次。 对于“仅一次”，我们需要消除服务器永远不会接收到请求的风险。为了实现这一点，服务器必须实现上述“至多一次”协议，但还必须向客户端确认已接收和执行了RPC调用。这些ACK消息在整个网络中都很常见。客户端必须定期重新发送每个RPC调用，直到收到该调用的ACK为止。 另一个重要问题涉及服务器和客户端之间的通信。**在标准过程调用中，通常在链接、加载或执行时间发生某种绑定，以便过程调用的名称被过程调用的内存地址替换。**RPC方案要求对客户端和服务器端口进行类似的绑定，但是客户端如何知道服务器上的端口号呢？由于它们不共享内存，因此两个系统都没有关于对方的完整信息。 有两种常见的方法。首先，绑定信息可以是预先确定的，以固定的端口地址的形式存在。在编译时，RPC调用与其关联的固定端口号。一旦程序被编译，服务器就不能更改所请求服务的端口号。其次，可以通过会合机制动态地进行绑定。通常，在固定的RPC端口上提供一个会合守护程序。然后，客户端发送一个包含所需执行的RPC的名称的消息给会合守护程序，请求该RPC的端口地址。返回端口号后，就可以将RPC调用发送到该端口，直到该进程终止（或服务器崩溃）。这种方法需要初始请求的额外开销，但比第一种方法更灵活。 RPC方案在实现分布式文件系统时非常有用。这样的系统可以作为一组RPC守护程序和客户端来实现。消息寻址到服务器上将执行文件操作的分布式文件系统端口。消息包含要执行的磁盘操作。磁盘操作可能是读取、写入、重命名、删除或状态，对应于常见的文件相关系统调用。返回消息包含由客户端的DFS守护程序代表客户端执行的该调用产生的任何数据。例如，一条消息可能包含一个请求将整个文件传输给客户端，也可能仅限于简单的块请求。在后一种情况下，如果要传输整个文件，可能需要多次请求。 管道 管道允许两个进程进行通信。在早期的UNIX系统中，管道是最早的IPC机制之一。它们通常为进程相互通信提供了较为简单的方式，尽管它们也有一些局限性。在实现管道时，需要考虑四个问题： 管道是否允许双向通信，还是通信是单向的？ 如果允许双向通信，它是半双工的（数据只能单向传输）还是全双工的（数据可以同时在两个方向传输）？ 通信进程之间是否必须存在某种关系（如父子关系）？ 管道是否可以在网络上通信，还是通信进程必须位于同一台机器上？ 普通管道 普通管道允许两个进程以标准的生产者-消费者方式进行通信：生产者向管道的一端写入（写端），而消费者从另一端读取（读端）。**因此，普通管道是单向的，只允许单向通信。**如果需要双向通信，必须使用两个管道，每个管道在不同的方向发送数据。接下来，我们将演示如何在UNIX上构建普通管道。在这两个程序示例中，一个进程向管道写入消息“Greetings”，而另一个进程从管道中读取这条消息。 在UNIX系统上，使用函数pipe(int fd[])构建普通管道。这个函数创建一个通过int fd[]文件描述符访问的管道：fd[0]是管道的读端fd[1]是写端。UNIX将管道视为一种特殊类型的文件，因此可以使用普通的read()和write()系统调用访问管道。 普通管道无法从创建它的进程外部访问。通常，父进程创建一个管道，并使用它与通过fork()创建的子进程进行通信。下图说明了文件描述符fd与父进程和子进程之间的关系。 在下面的UNIX程序中，父进程创建了一个管道，然后调用fork()创建子进程。在fork()调用之后发生的事情取决于数据如何通过管道流动。在这个示例中，父进程写入管道，而子进程从管道中读取。重要的是要注意，父进程和子进程最初都关闭了他们未使用的管道端。这是一个重要的步骤，以确保从管道读取的进程能够检测到文件末尾（read()返回0），当写入端关闭其端口时。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;sys/types.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define BUFFER_SIZE 25#define READ_END 0#define WRITE_END 1int main(void)&#123; char write msg[BUFFER_SIZE] = &quot;Greetings&quot;; char read msg[BUFFER_SIZE]; int fd[2]; pid_t pid; /* create the pipe */ if (pipe(fd) == -1) &#123; fprintf(stderr,&quot;Pipe failed&quot;); return 1; &#125; /* fork a child process */ pid = fork(); if (pid &lt; 0) &#123; /* error occurred */ fprintf(stderr, &quot;Fork Failed&quot;); return 1; &#125; if (pid &gt; 0) &#123; /* parent process */ /* close the unused end of the pipe */ close(fd[READ_END]); /* write to the pipe */ write(fd[WRITE_END], write msg, strlen(write msg)+1); /* close the write end of the pipe */ close(fd[WRITE_END]); &#125; else &#123; /* child process */ /* close the unused end of the pipe */ close(fd[WRITE_END]); /* read from the pipe */ read(fd[READ_END], read msg, BUFFER SIZE); printf(&quot;read %s&quot;,read msg); /* close the write end of the pipe */ close(fd[READ_END]); &#125; return 0;&#125; 请注意，普通管道在UNIX系统上都要求通信进程之间存在父-子关系。这意味着这些管道只能用于在同一台机器上的进程之间进行通信。 命名管道 普通管道提供了一个简单的机制，允许一对进程进行通信。然而，普通管道只存在于进程在彼此通信时。在UNIX系统上，一旦进程完成通信并终止，普通管道就会停止存在。命名管道提供了一种更强大的通信工具。通信可以是双向的，并且不需要父子关系。一旦建立了命名管道，多个进程可以使用它进行通信。事实上，在典型情况下，一个命名管道可能有多个写入者。此外，命名管道在通信进程完成后仍然存在。 在UNIX系统中，命名管道被称为FIFO（先进先出）。一旦创建，它们会在文件系统中显示为典型的文件。可以使用mkfifo()系统调用创建FIFO，并使用普通的open()、read()、write()和close()系统调用对其进行操作。它将继续存在，直到在文件系统中明确删除为止。虽然FIFO允许双向通信，但通常只允许半双工传输。如果数据必须在两个方向上传输，通常会使用两个FIFO。此外，通信进程必须驻留在同一台机器上。如果需要跨机器通信，则必须使用套接字。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"操作系统原理导论","slug":"操作系统原理导论","date":"2023-11-28T03:09:28.000Z","updated":"2023-12-29T06:34:04.164Z","comments":true,"path":"2023/11/28/操作系统原理导论/","link":"","permalink":"https://blackforest1990.github.io/2023/11/28/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/","excerpt":"","text":"根据维基百科对于操作系统的总结：操作系统（英语：Operating System，缩写：OS）是一组主管并控制计算机操作、运用和运行硬件、软件资源和提供公共服务来组织用户交互的相互关联的系统软件程序，同时也是计算机系统的内核与基石。 操作系统的历史 第一代（1945~1955）：真空管和穿孔卡带 同一个小组的人设计、建造、编程、操作并维护一台机器。所有的程序设计是用纯粹的机器语言编写的，甚至更糟糕，需要通过将上千根电缆接到插件板上连接成电路，以便控制机器的基本功能。没有程序设计语言（甚至汇编语言也没有），操作系统则从来没有听说过。使用机器的一般方式是，程序员在墙上的机时表上预约一段时间，然后到机房中将他的插件板接到计算机里，在接下来的几小时里，期盼正在运行中的两万多个真空管不会烧坏。那时，所有的计算问题实际都只是简单的数字运算，如制作正弦、余弦以及对数表等。到了20世纪50年代早期有了改进，出现了穿孔卡片，这时就可以将程序写在卡片上，然后读入计算机而不用插件板，但其他过程则依然如旧。 第二代（1955～1965）：晶体管和批处理系统 20世纪50年代晶体管的发明极大地改变了整个状况。计算机已经很可靠，厂商可以成批地生产并销售计算机给用户，用户可以指望计算机长时间运行，完成一些有用的工作。此时，设计人员、生产人员、操作人员、程序人员和维护人员之间第一次有了明确的分工。 这些机器，现在被称作大型机（mainframe），锁在有专用空调的房间中，由专业操作人员运行。只有少数大公司、重要的政府部门或大学才接受数百万美元的标价。要运行一个作业（job，即一个或一组程序），程序员首先将程序写在纸上（用FORTRAN语言或汇编语言），然后穿孔成卡片，再将卡片盒带到输入室，交给操作员，接着就喝咖啡直到输出完成。 由于当时的计算机非常昂贵，人们很自然地要想办法减少机时的浪费。通常采用的解决方法就是批处理系统（batch system）。在输入室收集全部的作业，然后用一台相对便宜的计算机，如IBM 1401计算机，将它们读到磁带上。IBM 1401计算机适用于读卡片、复制磁带和输出打印，但不适用于数值运算。另外用较昂贵的计算机，如IBM 7094来完成真正的计算。 第三代（1965～1980）：集成电路芯片和多道程序设计 IBM 360是一个软件兼容的计算机系列，其低档机与1401相当，高档机则比7094功能强很多。由于所有的计算机都有相同的体系结构和指令集，因此，在理论上，为一种型号机器编写的程序可以在其他所有型号的机器上运行。而且360被设计成既可用于科学计算，又可用于商业计算，这样，一个系列的计算机便可以满足所有用户的要求。360是第一个采用（小规模）芯片（集成电路）的主流机型，与采用分立晶体管制造的第二代计算机相比，其性能/价格比有很大提高。“单一家族”思想的最大优点同时也是其最大的缺点。IBM（或其他公司）无法写出同时满足这些相互冲突需要的软件，其结果是一个庞大的又极其复杂的操作系统。 它也使第二代操作系统所缺乏的几项关键技术得到了广泛应用。 其中最重要的应该是多道程序设计（multiprogramming）。当一个作业等待I/O操作完成时，另一个作业可以使用CPU。如果内存中可以同时存放足够多的作业，则CPU利用率可以接近100%。在内存中同时驻留多个作业需要特殊的硬件来对其进行保护，以避免作业的信息被窃取或受到攻击。 同时的外部设备联机操作（Simultaneous Peripheral Operation On Line，SPOOLing），任何时刻当一个作业运行结束时，操作系统就能将一个新作业从磁盘读出，装进空出来的内存区域运行。 分时系统（timesharing）的出现 【Unix诞生】 一位曾参加过MULTICS研制的贝尔实验室计算机科学家Ken Thompson，后来找到一台无人使用的PDP-7机器，并开始开发一个简化的、单用户版MULTICS。他的工作后来导致了UNIX操作系统的诞生。接着，UNIX在学术界，政府部门以及许多公司中流行。为了使编写的程序能够在任何版本的UNIX上运行，IEEE提出了一个UNIX的标准，称作POSIX，目前大多数UNIX版本都支持它。对UNIX版本免费产品（不同于教育目的）的愿望，导致芬兰学生Linus Torvalds编写了Linux。 第四代（1980年至今）：个人计算机 随着LSI（大规模集成电路）的发展，在每平方厘米的硅片芯片上可以集成数千个晶体管，个人计算机时代到来了。 1974年，当Intel 8080，第一代通用8位CPU出现时，Intel希望有一个用于8080的操作系统，部分是为了测试目的。Intel请求其顾问Gary Kildall编写。Kildall和一位朋友首先为新推出的Shugart Associates 8英寸软盘构造了一个控制器，并把这个软磁盘同8080相连，从而制造了第一个配有磁盘的微型计算机。 1977年，Digital Research重写了CP/M，使其可以在使用8080、Zilog Z80以及其他CPU芯片的多种微型计算机上运行，从而使得CP/M完全控制了微型计算机世界达5年之久。 在20世纪80年代的早期，IBM设计了IBM PC并寻找可在上面运行的软件。来自IBM的人员同Bill Gates联系有关他的BASIC解释器的许可证事宜，他们也询问是否他知道可在PC机上运行的操作系统。Gates建议IBM同Digital Research联系，即当时世界上主宰操作系统的公司。在做出毫无疑问是近代历史上最糟的商业决策后，Kildall拒绝与IBM会见。 在IBM返回时，Gates了解到一家本地计算机制造商，Seattle Computer Products，有合适的操作系统DOS（Disk Operating System）。他联系对方并提出购买（宣称75 000美元），对方接受了。然后Gates提供给IBM成套的DOS/BASIC，IBM也接受了。IBM希望做某些修改，于是Gates雇佣了那个写DOS的作者，Tim Paterson，作为Gates的微软公司早期的一个雇员，并开展工作。修改版称为MS-DOS（MicroSoft Disk Operating System），并且很快主导了IBM PC市场。同Kildall试图将CP/M每次卖给用户一个产品相比（至少 开始是这样），这里一个关键因素是Gates（回顾起来，极其聪明）的决策，将MS-DOS与计算机公司的硬件捆绑在一起出售。 1983年，IBM PC后续机型IBM PC/AT推出，配有Intel 80286 CPU。此时，MS-DOS已经确立了地位。 Steve Jobs访问PARC,Jobs一看到GUI，立即意识到它的潜在价值，而Xerox管理层恰好没有认识到。Jobs随后着手设计了带有GUI的苹果计算机。Jobs的第二次尝试，即苹果Macintosh，取得了巨大的成功，它是为那些不仅没有计算机知识，而且也根本不打算学习计算机的用户们准备的。在图像设计、专业数码摄影，以及专业数字视频生产的创意世界里，Macintosh得到广泛的应用。 在微软决定构建MS-DOS的后继产品时，受到了Macintosh成功的巨大影响。微软开发了名为Windows的基于GUI的系统，早期它运行在MS-DOS上层（它更像shell而不像真正的操作系统）。在从1985年至1995年的10年之间，Windows只是在MS-DOS上层的一个图形环境。然而，到了1995年，一个独立的Windows版本，具有许多操作系统功能的Windows 95发布了。Windows 95仅仅把底层的MS-DOS作为启动和运行老的MS-DOS程序之用。1998年，一个稍做修改的系统，Windows 98发布。不过Windows 95和Windows 98仍然使用了大量16位Intel汇编语言。 在个人计算机世界中，另一个主要竞争者是UNIX（和它的各种变体）。UNIX在网络和企业服务器等领域强大，在台式计算机上，特别是在诸如印度和中国这些发展中国家里，UNIX的使用也在增加。 操作系统功能 操作系统位于底层硬件与用户之间，是两者沟通的桥梁。用户可以通过操作系统的用户界面，输入命令。操作系统则对命令进行解释，驱动硬件设备，实现用户要求。操作系统的主要标准功能：进程管理，内存管理，文件系统管理，输入输出管理。 对于操作系统功能总结如下： 同样理论需要向实践服务，知行需要合一，本博客同样针对一种一种多应用与服务端的操作系统进行研究，Linux。 参考资料 维基百科 操作系统概念(第9版) 英文版 操作系统设计与实现 第3版 现代操作系统（原书第3版） (计算机科学丛书) 后续阅读链接","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"导论","slug":"导论","permalink":"https://blackforest1990.github.io/tags/%E5%AF%BC%E8%AE%BA/"}]},{"title":"how to read a book","slug":"how-to-read-a-book","date":"2023-11-21T08:10:05.000Z","updated":"2023-12-04T03:11:19.897Z","comments":true,"path":"2023/11/21/how-to-read-a-book/","link":"","permalink":"https://blackforest1990.github.io/2023/11/21/how-to-read-a-book/","excerpt":"","text":"如何阅读一本书 读书多而无所得，遇到经典书籍很难坚持下去，读不到自己脑子里去，找到一本【如何阅读一本书】，从几个层次来讨论书籍如何来阅读。 读书分为四个层次，基础阅读，检视阅读，分析阅读，主题阅读，层层递进，一层比一层深入。 基础阅读 基础阅读为基本能力，逐句阅读，为信息检索。 检视阅读 检视阅读为了解作者写作的框架，粗粗略略的快速读过，也是读者对于书的挑选，如果是煌煌巨著，再深入之。 (1)先看书名页，然后如果有序就先看序 (2)研究目录页 (3)如果书中附有索引，也要检阅一下 (4)如果那是本包着书衣的新书，不妨读一下出版社的介绍 (5)从你对一本书的目录很概略，甚至有点模糊的印象中，开始挑几个看来跟主题息息相关的篇章来看。 (6)最后一步，把书打开来，东翻翻西翻翻，念个一两段，有时候连续读几页，但不要太多 分析阅读 分析阅读有时候也可以称为“精读”。显然，只有对有价值的书，才值得花力气做“分析阅读”。“烂书或平庸的书”是没有这种待遇的。 这本书花了大量的篇幅来介绍“分析阅读”。这部分是此书【重点中的重点】。 第一阶段 找出一本书在谈些什么： (1)依照书本的种类与主题作分类。 (2)用最简短的句子说出整本书在谈些什么。 (3)按照顺序与关系，列出全书的重要部分。将全书的纲要拟出来之后，再将各个部分的纲要也一一列出。 (4)找出作者在问的问题，或作者想要解决的问题。 第二阶段 诠释一本书的内容： (5)诠释作者使用的关键字，与作者达成共识。 (6)从最重要的句子中抓出作者的重要主旨。 (7)找出作者的论述，重新架构这些论述的前因后果，以明白作者的主张。 (8)确定作者已经解决了哪些问题，还有哪些是未解决的。在未解决的问题中，确定哪些是作者认为自己无法解决的问题。 第三阶段 像是沟通知识一样地评论一本书： A．智慧礼节的一般规则 (9)除非你已经完成大纲架构，也能诠释整本书了，否则不要轻易批评。（在你说出：“我读懂了!”之前，不要说你同意、不同意或暂缓评论。） (10)不要争强好胜，非辩到底不可。 (11)在说出评论之前，你要能证明自己区别得出真正的知识与个人观点的不同。 B．批评观点的特别标准 (12)证明作者的知识不足。 (13)证明作者的知识错误。 (14)证明作者不合逻辑。 (15)证明作者的分析与理由是不完整的。 注意：关于最后这四点，前三点是表示不同意见的准则，如果你无法提出相关的佐证，就必须同意作者的说法，或至少一部分说法。你只能因为最后一点理由，对这本书暂缓评论。 主题阅读 所谓的“主题阅读”，通俗而言就是：为了研究某个主题，阅读跟该主题相关的多本书籍。这种阅读主要包括5个步骤： 步骤一：找到相关的章节。在主题阅读中，你及你关心的主题才是基本的重点，而不是你阅读的书。总之，要记得你最主要的工作不是理解整本书的内容，而是找出这本书对你的主题有什么帮助，而这可能与作者本身的写作目的相去甚远。 步骤二：带引作者与你达成共识。真正的困难在于要强迫作者使用你的语言，而不是使用他的语言。 步骤三：厘清问题。把我们的问题说得比较明白的问题，然后让那些作者来回答这些问题。 步骤四：界定议题。设定了一个不偏不倚的共识，适用于所有被检视过的作者，再设定出一整套的问题，其中大部分都能在作者的说明中找到答案。然后就不同的答案界定并安排出议题。 步骤五：分析讨论。找到有价值的问题之后，就需要通过自己的综合分析，思考一下：为什么这几本书的作者，对同一个问题会有不同的答案。如果你能想明白，那么你对该主题所处的领域，就有了更深刻的理解。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://blackforest1990.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"实用","slug":"实用","permalink":"https://blackforest1990.github.io/tags/%E5%AE%9E%E7%94%A8/"},{"name":"工具书","slug":"工具书","permalink":"https://blackforest1990.github.io/tags/%E5%B7%A5%E5%85%B7%E4%B9%A6/"}]},{"title":"邓小平时代","slug":"邓小平时代","date":"2023-03-21T08:25:20.000Z","updated":"2024-01-09T05:14:10.942Z","comments":true,"path":"2023/03/21/邓小平时代/","link":"","permalink":"https://blackforest1990.github.io/2023/03/21/%E9%82%93%E5%B0%8F%E5%B9%B3%E6%97%B6%E4%BB%A3/","excerpt":"","text":"傅高义这本书，基本上站在客观角度观察邓小平，说明了邓在各个阶段的人生经历和施政要领。 邓是一名忠诚的共产党员和民族主义者，贯穿整个人生他都做到了这一点，他在法国留学，但是由于时局突变，没有机会学习，进行了革命运动，但终其一生他都对于知识和科学技术保留了尊重，明白科技为第一生产力。在毛主政时期，虽然三上三下，屡受打压，但是他没有让打压阻碍了工作，仍然宠辱不惊的完成了外交任务，同时恪守原则，在路线问题上不退后一步，但同时因为与毛的私交没有被开除党籍，仍然保留了希望；邓在施政上以稳定为第一原则，对于文革没有清算，保存了党和政权的威严，在经济发展上，摸着石头过河，有问题先去实践，看看效果，再决定全国是否推广。坚持改革开放不动摇，由地方去影响北京。在学潮问题中，以强悍的手腕维持了稳定，同时积极跟美国沟通，后面虽然遭受了制裁但事情仍有余地，邓的治国，让中国迅速拜托了阶级斗争，封印了文革的伤痛，发展壮大了起来，他始终保持客观与强悍，信奉精英治国，维持了中国的稳定。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://blackforest1990.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"人物传记","slug":"人物传记","permalink":"https://blackforest1990.github.io/tags/%E4%BA%BA%E7%89%A9%E4%BC%A0%E8%AE%B0/"}]},{"title":"读超新星纪元有感","slug":"读超新星纪元有感","date":"2023-03-14T14:12:11.000Z","updated":"2023-03-14T14:21:11.715Z","comments":true,"path":"2023/03/14/读超新星纪元有感/","link":"","permalink":"https://blackforest1990.github.io/2023/03/14/%E8%AF%BB%E8%B6%85%E6%96%B0%E6%98%9F%E7%BA%AA%E5%85%83%E6%9C%89%E6%84%9F/","excerpt":"","text":"超新星纪元 读书笔记 作者：刘慈欣 残酷的幻想，人类被星际灾变拦腰截断。 人类在有一线希望的时候总能迸发强大的生机。 孩子和大人是两种不同的物种，双方对于对方都有误判。 当失去了领土，一种骨子里流淌的东西也失去了。 需要有智囊，同样也要有决断。 国际政治的均衡实际上很脆弱。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://blackforest1990.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"科幻","slug":"科幻","permalink":"https://blackforest1990.github.io/tags/%E7%A7%91%E5%B9%BB/"}]}],"categories":[{"name":"历史","slug":"历史","permalink":"https://blackforest1990.github.io/categories/%E5%8E%86%E5%8F%B2/"},{"name":"编程语言","slug":"编程语言","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Linux","slug":"Linux","permalink":"https://blackforest1990.github.io/categories/Linux/"},{"name":"编程","slug":"编程","permalink":"https://blackforest1990.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"策划","slug":"策划","permalink":"https://blackforest1990.github.io/categories/%E7%AD%96%E5%88%92/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blackforest1990.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"经验总结","slug":"经验总结","permalink":"https://blackforest1990.github.io/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://blackforest1990.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://blackforest1990.github.io/tags/%E6%9D%82%E8%B0%88/"},{"name":"汇编语言","slug":"汇编语言","permalink":"https://blackforest1990.github.io/tags/%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/"},{"name":"Kernel","slug":"Kernel","permalink":"https://blackforest1990.github.io/tags/Kernel/"},{"name":"bit","slug":"bit","permalink":"https://blackforest1990.github.io/tags/bit/"},{"name":"算法","slug":"算法","permalink":"https://blackforest1990.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"小说提纲","slug":"小说提纲","permalink":"https://blackforest1990.github.io/tags/%E5%B0%8F%E8%AF%B4%E6%8F%90%E7%BA%B2/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blackforest1990.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"内存管理","slug":"内存管理","permalink":"https://blackforest1990.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"进程管理","slug":"进程管理","permalink":"https://blackforest1990.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"name":"工作语言","slug":"工作语言","permalink":"https://blackforest1990.github.io/tags/%E5%B7%A5%E4%BD%9C%E8%AF%AD%E8%A8%80/"},{"name":"销售管理","slug":"销售管理","permalink":"https://blackforest1990.github.io/tags/%E9%94%80%E5%94%AE%E7%AE%A1%E7%90%86/"},{"name":"电信运营商","slug":"电信运营商","permalink":"https://blackforest1990.github.io/tags/%E7%94%B5%E4%BF%A1%E8%BF%90%E8%90%A5%E5%95%86/"},{"name":"与虎谋皮","slug":"与虎谋皮","permalink":"https://blackforest1990.github.io/tags/%E4%B8%8E%E8%99%8E%E8%B0%8B%E7%9A%AE/"},{"name":"云计算","slug":"云计算","permalink":"https://blackforest1990.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"how to make","slug":"how-to-make","permalink":"https://blackforest1990.github.io/tags/how-to-make/"},{"name":"黄仁宇","slug":"黄仁宇","permalink":"https://blackforest1990.github.io/tags/%E9%BB%84%E4%BB%81%E5%AE%87/"},{"name":"导论","slug":"导论","permalink":"https://blackforest1990.github.io/tags/%E5%AF%BC%E8%AE%BA/"},{"name":"实用","slug":"实用","permalink":"https://blackforest1990.github.io/tags/%E5%AE%9E%E7%94%A8/"},{"name":"工具书","slug":"工具书","permalink":"https://blackforest1990.github.io/tags/%E5%B7%A5%E5%85%B7%E4%B9%A6/"},{"name":"人物传记","slug":"人物传记","permalink":"https://blackforest1990.github.io/tags/%E4%BA%BA%E7%89%A9%E4%BC%A0%E8%AE%B0/"},{"name":"科幻","slug":"科幻","permalink":"https://blackforest1990.github.io/tags/%E7%A7%91%E5%B9%BB/"}]}